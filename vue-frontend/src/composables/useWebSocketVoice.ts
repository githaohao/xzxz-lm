import { ref, computed, onUnmounted, readonly } from 'vue'
import type { VoiceMessage, CallState } from '@/types'
import { generateId } from '@/utils/voice-utils'

export interface WebSocketVoiceConfig {
  url: string
  sessionId?: string
  language?: string
  onMessage?: (message: VoiceMessage) => void
  onStatusChange?: (status: CallState) => void
  onError?: (error: Error) => void
}

export interface StreamAudioChunk {
  text: string
  chunkId: number
  audioSize: number
  isFinal?: boolean
  timestamp: number
}

export function useWebSocketVoice(config: WebSocketVoiceConfig) {
  // çŠ¶æ€ç®¡ç†
  const websocket = ref<WebSocket | null>(null)
  const connectionState = ref<CallState>('idle')
  const isConnected = ref(false)
  const isRecording = ref(false)
  const isAIPlaying = ref(false)
  const currentPlayingText = ref('')
  const recognizedText = ref('')
  const aiResponse = ref('')
  
  // éŸ³é¢‘ç›¸å…³
  const mediaRecorder = ref<MediaRecorder | null>(null)
  const audioStream = ref<MediaStream | null>(null)
  const audioChunks = ref<Blob[]>([])
  const audioQueue = ref<HTMLAudioElement[]>([])
  const currentAudio = ref<HTMLAudioElement | null>(null)
  
  // è®¡ç®—å±æ€§
  const canRecord = computed(() => isConnected.value && !isRecording.value && !isAIPlaying.value)
  const canConnect = computed(() => !isConnected.value)

  // è¿æ¥WebSocket
  async function connect(): Promise<void> {
    try {
      if (websocket.value?.readyState === WebSocket.OPEN) {
        console.log('WebSocketå·²è¿æ¥')
        return
      }

      connectionState.value = 'connecting'
      
      websocket.value = new WebSocket(config.url)
      
      websocket.value.onopen = () => {
        console.log('ğŸ”Œ WebSocketè¯­éŸ³è¿æ¥å·²å»ºç«‹')
        isConnected.value = true
        connectionState.value = 'connected'
        config.onStatusChange?.('connected')
        
        // å‘é€é…ç½®
        sendConfig()
      }
      
      websocket.value.onmessage = async (event) => {
        try {
          // å¤„ç†JSONæ¶ˆæ¯
          if (typeof event.data === 'string') {
            const message = JSON.parse(event.data)
            await handleWebSocketMessage(message)
          }
          // å¤„ç†äºŒè¿›åˆ¶éŸ³é¢‘æ•°æ®
          else if (event.data instanceof Blob) {
            await handleAudioData(event.data)
          }
          // å¤„ç†ArrayBufferéŸ³é¢‘æ•°æ®
          else if (event.data instanceof ArrayBuffer) {
            const audioBlob = new Blob([event.data], { type: 'audio/mpeg' })
            await handleAudioData(audioBlob)
          }
        } catch (error) {
          console.error('å¤„ç†WebSocketæ¶ˆæ¯å¤±è´¥:', error)
          config.onError?.(new Error(`æ¶ˆæ¯å¤„ç†å¤±è´¥: ${error}`))
        }
      }
      
      websocket.value.onerror = (error) => {
        console.error('âŒ WebSocketè¿æ¥é”™è¯¯:', error)
        config.onError?.(new Error('WebSocketè¿æ¥é”™è¯¯'))
      }
      
      websocket.value.onclose = () => {
        console.log('ğŸ”Œ WebSocketè¿æ¥å·²å…³é—­')
        isConnected.value = false
        connectionState.value = 'idle'
        config.onStatusChange?.('idle')
        cleanup()
      }
      
    } catch (error) {
      console.error('WebSocketè¿æ¥å¤±è´¥:', error)
      connectionState.value = 'idle'
      config.onError?.(new Error(`è¿æ¥å¤±è´¥: ${error}`))
      throw error
    }
  }

  // æ–­å¼€è¿æ¥
  function disconnect(): void {
    if (websocket.value) {
      websocket.value.close()
      websocket.value = null
    }
    cleanup()
  }

  // å‘é€é…ç½®
  function sendConfig(): void {
    if (!websocket.value || websocket.value.readyState !== WebSocket.OPEN) return
    
    const configMessage = {
      type: 'config',
      session_id: config.sessionId || `ws-voice-${Date.now()}`,
      language: config.language || 'auto'
    }
    
    websocket.value.send(JSON.stringify(configMessage))
    console.log('ğŸ“¤ å‘é€WebSocketé…ç½®:', configMessage)
  }

  // å¼€å§‹å½•éŸ³
  async function startRecording(): Promise<void> {
    try {
      if (isRecording.value) return
      
      console.log('ğŸ¤ å¼€å§‹å½•éŸ³...')
      
      // è·å–éº¦å…‹é£æƒé™
      audioStream.value = await navigator.mediaDevices.getUserMedia({
        audio: {
          sampleRate: 16000,
          channelCount: 1,
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true
        }
      })
      
      // åˆ›å»ºMediaRecorder
      const options = {
        mimeType: 'audio/webm;codecs=opus',
        audioBitsPerSecond: 16000
      }
      
      mediaRecorder.value = new MediaRecorder(audioStream.value, options)
      audioChunks.value = []
      
      mediaRecorder.value.ondataavailable = (event) => {
        if (event.data.size > 0) {
          audioChunks.value.push(event.data)
        }
      }
      
      mediaRecorder.value.onstop = async () => {
        console.log('ğŸ¤ å½•éŸ³ç»“æŸï¼Œå¤„ç†éŸ³é¢‘æ•°æ®...')
        await processRecordedAudio()
      }
      
      mediaRecorder.value.start(1000) // æ¯ç§’æ”¶é›†ä¸€æ¬¡æ•°æ®
      isRecording.value = true
      connectionState.value = 'listening'
      config.onStatusChange?.('listening')
      
    } catch (error) {
      console.error('å¼€å§‹å½•éŸ³å¤±è´¥:', error)
      config.onError?.(new Error(`å½•éŸ³å¤±è´¥: ${error}`))
    }
  }

  // åœæ­¢å½•éŸ³
  function stopRecording(): void {
    if (mediaRecorder.value && isRecording.value) {
      mediaRecorder.value.stop()
      isRecording.value = false
      
      // åœæ­¢éŸ³é¢‘æµ
      if (audioStream.value) {
        audioStream.value.getTracks().forEach(track => track.stop())
        audioStream.value = null
      }
    }
  }

  // å¤„ç†å½•éŸ³æ•°æ®
  async function processRecordedAudio(): Promise<void> {
    try {
      if (audioChunks.value.length === 0) {
        console.warn('æ²¡æœ‰å½•éŸ³æ•°æ®')
        return
      }
      
      // åˆå¹¶éŸ³é¢‘æ•°æ®
      const audioBlob = new Blob(audioChunks.value, { type: 'audio/webm' })
      console.log(`ğŸµ å½•éŸ³æ•°æ®å¤§å°: ${audioBlob.size} å­—èŠ‚`)
      
      // è½¬æ¢ä¸ºArrayBufferå¹¶ç›´æ¥å‘é€äºŒè¿›åˆ¶æ•°æ®
      const arrayBuffer = await audioBlob.arrayBuffer()
      
      if (websocket.value?.readyState === WebSocket.OPEN) {
        connectionState.value = 'processing'
        config.onStatusChange?.('processing')
        
        // ç›´æ¥å‘é€äºŒè¿›åˆ¶éŸ³é¢‘æ•°æ®ï¼Œæ— éœ€base64ç¼–ç 
        websocket.value.send(arrayBuffer)
        console.log('ğŸ“¤ å‘é€æµå¼éŸ³é¢‘æ•°æ®:', arrayBuffer.byteLength, 'å­—èŠ‚')
      } else {
        throw new Error('WebSocketè¿æ¥ä¸å¯ç”¨')
      }
      
    } catch (error) {
      console.error('å¤„ç†å½•éŸ³æ•°æ®å¤±è´¥:', error)
      config.onError?.(new Error(`éŸ³é¢‘å¤„ç†å¤±è´¥: ${error}`))
      connectionState.value = 'connected'
      config.onStatusChange?.('connected')
    }
  }

  // å¤„ç†WebSocketæ¶ˆæ¯
  async function handleWebSocketMessage(message: any): Promise<void> {
    console.log('ğŸ“¥ æ”¶åˆ°WebSocketæ¶ˆæ¯:', message.type, message)
    
    switch (message.type) {
      case 'status':
        if (message.status === 'listening') {
          connectionState.value = 'connected'
          config.onStatusChange?.('connected')
        }
        break
        
      case 'recognition_result':
        if (message.success && message.recognized_text) {
          recognizedText.value = message.recognized_text
          
          // æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
          const userMessage: VoiceMessage = {
            id: generateId(),
            content: message.recognized_text,
            isUser: true,
            timestamp: new Date(),
            recognizedText: message.recognized_text
          }
          config.onMessage?.(userMessage)
        }
        break
        
      case 'ai_thinking':
        connectionState.value = 'processing'
        config.onStatusChange?.('processing')
        aiResponse.value = ''
        break
        
      case 'ai_text_chunk':
        aiResponse.value += message.content
        break
        
      case 'audio_chunk_info':
        // éŸ³é¢‘å—ä¿¡æ¯ï¼Œå‡†å¤‡æ¥æ”¶äºŒè¿›åˆ¶æ•°æ®
        currentPlayingText.value = message.text
        console.log(`ğŸµ å‡†å¤‡æ’­æ”¾éŸ³é¢‘å— ${message.chunk_id}: ${message.text.substring(0, 30)}...`)
        break
        
      case 'stream_complete':
        // æµå¼å¤„ç†å®Œæˆ
        if (aiResponse.value.trim()) {
          const aiMessage: VoiceMessage = {
            id: generateId(),
            content: aiResponse.value.trim(),
            isUser: false,
            timestamp: new Date()
          }
          config.onMessage?.(aiMessage)
        }
        
        connectionState.value = 'connected'
        config.onStatusChange?.('connected')
        currentPlayingText.value = ''
        break
        
      case 'error':
        console.error('WebSocketé”™è¯¯:', message.error)
        config.onError?.(new Error(message.error))
        connectionState.value = 'connected'
        config.onStatusChange?.('connected')
        break
        
      case 'tts_error':
        console.error('TTSé”™è¯¯:', message.message)
        break
        
      default:
        console.log('æœªå¤„ç†çš„æ¶ˆæ¯ç±»å‹:', message.type)
    }
  }

  // å¤„ç†éŸ³é¢‘æ•°æ®
  async function handleAudioData(audioBlob: Blob): Promise<void> {
    try {
      console.log(`ğŸ”Š æ”¶åˆ°éŸ³é¢‘æ•°æ®: ${audioBlob.size} å­—èŠ‚`)
      
      // åˆ›å»ºéŸ³é¢‘URL
      const audioUrl = URL.createObjectURL(audioBlob)
      const audio = new Audio(audioUrl)
      audio.volume = 0.8
      audio.preload = 'auto'
      
      // æ·»åŠ åˆ°æ’­æ”¾é˜Ÿåˆ—
      audioQueue.value.push(audio)
      
      // å¦‚æœå½“å‰æ²¡æœ‰æ’­æ”¾ï¼Œå¼€å§‹æ’­æ”¾
      if (!isAIPlaying.value) {
        await playNextAudio()
      }
      
    } catch (error) {
      console.error('å¤„ç†éŸ³é¢‘æ•°æ®å¤±è´¥:', error)
      config.onError?.(new Error(`éŸ³é¢‘æ’­æ”¾å¤±è´¥: ${error}`))
    }
  }

  // æ’­æ”¾ä¸‹ä¸€ä¸ªéŸ³é¢‘
  async function playNextAudio(): Promise<void> {
    if (audioQueue.value.length === 0) {
      isAIPlaying.value = false
      currentPlayingText.value = ''
      connectionState.value = 'connected'
      config.onStatusChange?.('connected')
      return
    }
    
    const audio = audioQueue.value.shift()!
    currentAudio.value = audio
    isAIPlaying.value = true
    connectionState.value = 'speaking'
    config.onStatusChange?.('speaking')
    
    return new Promise((resolve) => {
      audio.onended = async () => {
        // æ¸…ç†éŸ³é¢‘URL
        URL.revokeObjectURL(audio.src)
        
        // æ’­æ”¾ä¸‹ä¸€ä¸ªéŸ³é¢‘
        await playNextAudio()
        resolve()
      }
      
      audio.onerror = (error) => {
        console.error('éŸ³é¢‘æ’­æ”¾é”™è¯¯:', error)
        URL.revokeObjectURL(audio.src)
        playNextAudio()
        resolve()
      }
      
      audio.play().catch(error => {
        console.error('éŸ³é¢‘æ’­æ”¾å¤±è´¥:', error)
        URL.revokeObjectURL(audio.src)
        playNextAudio()
        resolve()
      })
    })
  }

  // åœæ­¢éŸ³é¢‘æ’­æ”¾
  function stopAudio(): void {
    if (currentAudio.value) {
      currentAudio.value.pause()
      currentAudio.value = null
    }
    
    // æ¸…ç†æ’­æ”¾é˜Ÿåˆ—
    audioQueue.value.forEach(audio => {
      URL.revokeObjectURL(audio.src)
    })
    audioQueue.value = []
    
    isAIPlaying.value = false
    currentPlayingText.value = ''
  }

  // å‘é€å¿ƒè·³
  function sendHeartbeat(): void {
    if (websocket.value?.readyState === WebSocket.OPEN) {
      websocket.value.send(JSON.stringify({ type: 'ping' }))
    }
  }

  // æ¸…ç†èµ„æº
  function cleanup(): void {
    stopRecording()
    stopAudio()
    
    if (mediaRecorder.value) {
      mediaRecorder.value = null
    }
    
    if (audioStream.value) {
      audioStream.value.getTracks().forEach(track => track.stop())
      audioStream.value = null
    }
    
    audioChunks.value = []
    recognizedText.value = ''
    aiResponse.value = ''
    currentPlayingText.value = ''
  }

  // ç»„ä»¶å¸è½½æ—¶æ¸…ç†
  onUnmounted(() => {
    disconnect()
  })

  return {
    // çŠ¶æ€
    connectionState: readonly(connectionState),
    isConnected: readonly(isConnected),
    isRecording: readonly(isRecording),
    isAIPlaying: readonly(isAIPlaying),
    currentPlayingText: readonly(currentPlayingText),
    recognizedText: readonly(recognizedText),
    aiResponse: readonly(aiResponse),
    
    // è®¡ç®—å±æ€§
    canRecord,
    canConnect,
    
    // æ–¹æ³•
    connect,
    disconnect,
    startRecording,
    stopRecording,
    stopAudio,
    sendHeartbeat,
    cleanup
  }
} 