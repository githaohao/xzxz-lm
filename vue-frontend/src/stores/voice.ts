import { defineStore } from 'pinia'
import { ref, computed } from 'vue'
import type { VoiceMessage, CallState, KnowledgeBase } from '@/types'
import { generateId, cleanTextForSpeech, AudioPlayQueue } from '@/utils/voice-utils'
import { 
  sendVoiceMessage, 
  sendVoiceMessageStream,
  synthesizeSpeech, 
  checkFunAudioStatus, 
  clearConversationHistory 
} from '@/utils/api'

export const useVoiceStore = defineStore('voice', () => {
  // çŠ¶æ€
  const messages = ref<VoiceMessage[]>([])
  const callState = ref<CallState>('idle')
  const isRecording = ref(false)
  const isAIPlaying = ref(false)
  const isMuted = ref(false)
  const sessionId = ref(`voice-chat-${Date.now()}`)
  const conversationRounds = ref(0)
  const currentTranscript = ref('')
  const funAudioAvailable = ref(false)
  const speechRecognitionAvailable = ref(false)
  
  // æµå¼å¤„ç†ç›¸å…³çŠ¶æ€
  const isStreamMode = ref(true) // é»˜è®¤å¯ç”¨æµå¼æ¨¡å¼
  const currentAIResponse = ref('')
  const currentPlayingText = ref('')
  const audioQueue = ref<AudioPlayQueue | null>(null)
  
  // çŸ¥è¯†åº“ç›¸å…³çŠ¶æ€
  const selectedKnowledgeBase = ref<KnowledgeBase | null>(null)

  // éŸ³é¢‘ç›¸å…³å¼•ç”¨
  const audioStream = ref<MediaStream | null>(null)
  const audioContext = ref<AudioContext | null>(null)
  const currentAudio = ref<HTMLAudioElement | null>(null)
  const analyserNode = ref<AnalyserNode | null>(null)
  const mediaRecorder = ref<MediaRecorder | null>(null)
  
  // æ™ºèƒ½é™éŸ³æ£€æµ‹é…ç½®
  const silenceThreshold = ref(30) // é™éŸ³é˜ˆå€¼ (0-255)
  const silenceTimeout = ref(2000) // é™éŸ³è¶…æ—¶æ—¶é—´ (æ¯«ç§’)
  const minRecordingTime = ref(1000) // æœ€å°å½•éŸ³æ—¶é—´ (æ¯«ç§’)
  const maxRecordingTime = ref(30000) // æœ€å¤§å½•éŸ³æ—¶é—´ (æ¯«ç§’)
  
  // é™éŸ³æ£€æµ‹çŠ¶æ€
  const lastSoundTime = ref(0)
  const recordingStartTime = ref(0)
  const silenceDetectionActive = ref(false)

  // è®¡ç®—å±æ€§
  const hasMessages = computed(() => messages.value.length > 0)
  const isConnected = computed(() => callState.value !== 'idle')
  const canStartCall = computed(() => funAudioAvailable.value && callState.value === 'idle')

  // åˆå§‹åŒ–éŸ³é¢‘æ’­æ”¾é˜Ÿåˆ—
  function initAudioQueue() {
    audioQueue.value = new AudioPlayQueue({
      onPlayStart: (text, chunkId) => {
        currentPlayingText.value = text
        isAIPlaying.value = true
        console.log(`Starting audio chunk ${chunkId}: ${text.substring(0, 30)}...`)
      },
      onPlayEnd: (text, chunkId) => {
        console.log(`Completed audio chunk ${chunkId}`)
      },
      onQueueEmpty: () => {
        isAIPlaying.value = false
        currentPlayingText.value = ''
        callState.value = 'connected'
        console.log('Audio playback queue empty')
        
        // æ’­æ”¾ç»“æŸåç»§ç»­å½•éŸ³
        setTimeout(() => {
          if (funAudioAvailable.value && callState.value === 'connected') {
            startRecording()
          }
        }, 500)
      },
      onError: (error) => {
        console.error('Audio playback error:', error)
        isAIPlaying.value = false
        currentPlayingText.value = ''
      }
    })
  }

  // æ·»åŠ æ¶ˆæ¯
  function addMessage(message: Omit<VoiceMessage, 'id' | 'timestamp'>) {
    const newMessage: VoiceMessage = {
      ...message,
      id: generateId(),
      timestamp: new Date()
    }
    messages.value.push(newMessage)
    return newMessage
  }

  // éŸ³é¢‘ç›‘æµ‹å‡½æ•°
  function startAudioMonitoring(): void {
    if (!audioContext.value || !analyserNode.value) return

    const bufferLength = analyserNode.value.frequencyBinCount
    const dataArray = new Uint8Array(bufferLength)
    
    silenceDetectionActive.value = true
    lastSoundTime.value = Date.now()
    recordingStartTime.value = Date.now()

    function checkAudioLevel(): void {
      if (!silenceDetectionActive.value || !analyserNode.value) return

      analyserNode.value.getByteFrequencyData(dataArray)
      
      // è®¡ç®—å¹³å‡éŸ³é‡
      let sum = 0
      for (let i = 0; i < bufferLength; i++) {
        sum += dataArray[i]
      }
      const averageVolume = sum / bufferLength

      const currentTime = Date.now()
      const recordingDuration = currentTime - recordingStartTime.value
      const silenceDuration = currentTime - lastSoundTime.value

      // å¦‚æœæ£€æµ‹åˆ°å£°éŸ³ï¼Œæ›´æ–°æœ€åå£°éŸ³æ—¶é—´
      if (averageVolume > silenceThreshold.value) {
        lastSoundTime.value = currentTime
      }

      // æ£€æŸ¥æ˜¯å¦åº”è¯¥åœæ­¢å½•éŸ³
      const shouldStop = (
        // é™éŸ³æ—¶é—´è¶…è¿‡é˜ˆå€¼ä¸”å·²å½•éŸ³æœ€å°æ—¶é—´
        (silenceDuration > silenceTimeout.value && recordingDuration > minRecordingTime.value) ||
        // å½•éŸ³æ—¶é—´è¶…è¿‡æœ€å¤§æ—¶é—´
        recordingDuration > maxRecordingTime.value
      )

      if (shouldStop) {
        console.log(`ğŸ”‡ æ™ºèƒ½é™éŸ³æ£€æµ‹: å¹³å‡éŸ³é‡=${averageVolume.toFixed(1)}, é™éŸ³æ—¶é•¿=${silenceDuration}ms, å½•éŸ³æ—¶é•¿=${recordingDuration}ms`)
        stopRecording()
        return
      }

      // ç»§ç»­ç›‘æµ‹
      requestAnimationFrame(checkAudioLevel)
    }

    checkAudioLevel()
  }

  // åœæ­¢éŸ³é¢‘ç›‘æµ‹
  function stopAudioMonitoring(): void {
    silenceDetectionActive.value = false
  }

  // åœæ­¢å½•éŸ³
  function stopRecording(): void {
    if (mediaRecorder.value && mediaRecorder.value.state === 'recording') {
      mediaRecorder.value.stop()
      isRecording.value = false
      stopAudioMonitoring()
      console.log('ğŸ¤ å½•éŸ³å·²åœæ­¢')
    }
  }

  // æ£€æŸ¥æœåŠ¡çŠ¶æ€
  async function checkServiceStatus() {
    try {
      funAudioAvailable.value = await checkFunAudioStatus()
      
      // æ£€æŸ¥Web Speech APIæ”¯æŒ
      speechRecognitionAvailable.value = !!(
        (window as any).webkitSpeechRecognition || (window as any).SpeechRecognition
      )
      
      console.log('æœåŠ¡çŠ¶æ€æ£€æŸ¥:', {
        funAudio: funAudioAvailable.value,
        speechRecognition: speechRecognitionAvailable.value
      })
    } catch (error) {
      console.error('æ£€æŸ¥æœåŠ¡çŠ¶æ€å¤±è´¥:', error)
      funAudioAvailable.value = false
    }
  }

  // åˆå§‹åŒ–å½•éŸ³
  async function initRecording(): Promise<void> {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ 
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true
        } 
      })
      
      audioStream.value = stream
      audioContext.value = new AudioContext()
      
      // åˆ›å»ºéŸ³é¢‘åˆ†æå™¨ç”¨äºæ™ºèƒ½é™éŸ³æ£€æµ‹
      const source = audioContext.value.createMediaStreamSource(stream)
      analyserNode.value = audioContext.value.createAnalyser()
      analyserNode.value.fftSize = 256
      analyserNode.value.smoothingTimeConstant = 0.8
      source.connect(analyserNode.value)
      
      console.log('âœ… å½•éŸ³åˆå§‹åŒ–æˆåŠŸï¼Œæ™ºèƒ½é™éŸ³æ£€æµ‹å·²å¯ç”¨')
    } catch (error) {
      console.error('âŒ å½•éŸ³åˆå§‹åŒ–å¤±è´¥:', error)
      throw new Error('æ— æ³•è®¿é—®éº¦å…‹é£ï¼Œè¯·æ£€æŸ¥æƒé™è®¾ç½®')
    }
  }

  // å¼€å§‹å½•éŸ³
  async function startRecording(): Promise<void> {
    if (!audioStream.value) {
      await initRecording()
    }

    if (!audioStream.value) {
      throw new Error('éŸ³é¢‘æµæœªåˆå§‹åŒ–')
    }

    try {
      mediaRecorder.value = new MediaRecorder(audioStream.value, {
        mimeType: 'audio/webm;codecs=opus'
      })

      const audioChunks: Blob[] = []
      
      mediaRecorder.value.ondataavailable = (event) => {
        if (event.data.size > 0) {
          audioChunks.push(event.data)
        }
      }

      mediaRecorder.value.onstop = async () => {
        const audioBlob = new Blob(audioChunks, { type: 'audio/webm' })
        await handleUserSpeech('', audioBlob)
      }

      mediaRecorder.value.start()
      isRecording.value = true
      callState.value = 'listening'
      
      // å¯åŠ¨æ™ºèƒ½é™éŸ³æ£€æµ‹
      startAudioMonitoring()

      console.log('ğŸ¤ å¼€å§‹å½•éŸ³ (æ™ºèƒ½é™éŸ³æ£€æµ‹å·²å¯ç”¨)')
    } catch (error) {
      console.error('âŒ å¼€å§‹å½•éŸ³å¤±è´¥:', error)
      throw error
    }
  }

  // å¤„ç†ç”¨æˆ·è¯­éŸ³ - æµå¼ç‰ˆæœ¬
  async function handleUserSpeechStream(transcript: string, audioBlob?: Blob): Promise<void> {
    if (!transcript.trim() && !audioBlob) return

    callState.value = 'processing'
    let userMessage: VoiceMessage | null = null
    currentAIResponse.value = ''

    try {
      if (funAudioAvailable.value && audioBlob) {
        console.log('Using streaming FunAudioLLM workflow')

        // åˆ›å»ºç”¨æˆ·æ¶ˆæ¯
        userMessage = addMessage({
          content: transcript || '[è¯­éŸ³è¾“å…¥]',
          isUser: true,
          recognizedText: transcript
        })

        // åˆå§‹åŒ–éŸ³é¢‘é˜Ÿåˆ—
        initAudioQueue()

        // è°ƒç”¨æµå¼è¯­éŸ³èŠå¤©API
        const response = await sendVoiceMessageStream(
          audioBlob, 
          sessionId.value, 
          'auto',
          selectedKnowledgeBase.value?.id
        )

        if (!response.ok) {
          throw new Error(`HTTPé”™è¯¯ ${response.status}: ${response.statusText}`)
        }

        // å¤„ç†æµå¼å“åº”
        await handleStreamingResponse(response)

      } else {
        throw new Error('æ— æœ‰æ•ˆè¾“å…¥')
      }
    } catch (error: any) {
      console.error('Voice processing failed:', error)

      if (!userMessage) {
        userMessage = addMessage({
          content: transcript || '[è¯­éŸ³è¾“å…¥]',
          isUser: true
        })
      }

      addMessage({
        content: 'æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯­éŸ³æ—¶å‡ºç°äº†é—®é¢˜ã€‚è¯·ç¨åé‡è¯•ã€‚',
        isUser: false
      })

      callState.value = 'connected'
    }
  }

  // å¤„ç†æµå¼å“åº”
  async function handleStreamingResponse(response: Response): Promise<void> {
    const reader = response.body?.getReader()
    if (!reader) {
      throw new Error('æ— æ³•è¯»å–å“åº”æµ')
    }

    const decoder = new TextDecoder()
    let buffer = ''

    try {
      while (true) {
        const { done, value } = await reader.read()
        
        if (done) break

        buffer += decoder.decode(value, { stream: true })
        const lines = buffer.split('\n')
        buffer = lines.pop() || ''

        for (const line of lines) {
          if (line.startsWith('data: ')) {
            const data = line.slice(6).trim()
            
            if (data === '[DONE]') {
              // æµå¼å¤„ç†å®Œæˆï¼Œæ·»åŠ å®Œæ•´çš„AIæ¶ˆæ¯
              if (currentAIResponse.value.trim()) {
                addMessage({
                  content: currentAIResponse.value.trim(),
                  isUser: false
                })
              }
              return
            }

            try {
              const event = JSON.parse(data)
              await handleStreamEvent(event)
            } catch (e) {
              console.warn('è§£ææµå¼æ•°æ®å¤±è´¥:', data, e)
            }
          }
        }
      }
    } catch (error) {
      console.error('å¤„ç†æµå¼å“åº”æ—¶å‡ºé”™:', error)
      throw error
    } finally {
      if (!reader.closed) {
        await reader.cancel()
      }
    }
  }

  // å¤„ç†æµå¼äº‹ä»¶
  async function handleStreamEvent(event: any): Promise<void> {
    switch (event.type) {
      case 'status':
        console.log('ğŸ“Š çŠ¶æ€:', event.message)
        break
        
      case 'recognition':
        console.log('ğŸ¤ è¯­éŸ³è¯†åˆ«:', event.text)
        break
        
      case 'ai_text':
        // AIç”Ÿæˆçš„æ–‡å­—ç‰‡æ®µ
        currentAIResponse.value += event.content
        console.log('ğŸ’¬ AIæ–‡å­—:', event.content)
        break
        
      case 'audio_chunk':
        // æ”¶åˆ°éŸ³é¢‘ç‰‡æ®µï¼ŒåŠ å…¥æ’­æ”¾é˜Ÿåˆ—
        if (audioQueue.value && event.audio) {
          callState.value = 'speaking'
          audioQueue.value.addAudio(event.audio, event.text, event.chunk_id)
          console.log(`ğŸµ æ”¶åˆ°éŸ³é¢‘å— ${event.chunk_id}: ${event.text.substring(0, 30)}...`)
        }
        break
        
      case 'complete':
        console.log('âœ… æµå¼å¤„ç†å®Œæˆ')
        break
        
      case 'error':
        console.error('âŒ æµå¼å¤„ç†é”™è¯¯:', event.message)
        throw new Error(event.message)
        
      case 'tts_error':
        console.error('âŒ TTSåˆæˆé”™è¯¯:', event.message)
        // TTSé”™è¯¯ä¸ä¸­æ–­æ•´ä¸ªæµç¨‹
        break
        
      default:
        console.log('ğŸ” æœªçŸ¥äº‹ä»¶ç±»å‹:', event.type, event)
    }
  }

  // ä¿®æ”¹åŸæœ‰çš„handleUserSpeechï¼Œæ”¯æŒæµå¼æ¨¡å¼åˆ‡æ¢
  async function handleUserSpeech(transcript: string, audioBlob?: Blob): Promise<void> {
    if (isStreamMode.value) {
      return handleUserSpeechStream(transcript, audioBlob)
    } else {
      return handleUserSpeechOriginal(transcript, audioBlob)
    }
  }

  // åŸæœ‰çš„éæµå¼å¤„ç†æ–¹æ³•ï¼ˆé‡å‘½åï¼‰
  async function handleUserSpeechOriginal(transcript: string, audioBlob?: Blob): Promise<void> {
    if (!transcript.trim() && !audioBlob) return

    callState.value = 'processing'
    let userMessage: VoiceMessage | null = null
    let aiResponse = ''

    try {
      if (funAudioAvailable.value && audioBlob) {
        console.log('ğŸ¯ ä½¿ç”¨FunAudioLLMæµç¨‹')

        const result = await sendVoiceMessage(
          audioBlob, 
          sessionId.value, 
          'auto',
          selectedKnowledgeBase.value?.id
        )

        if (result.success) {
          userMessage = addMessage({
            content: result.recognized_text || '[è¯­éŸ³è¾“å…¥]',
            isUser: true,
            recognizedText: result.recognized_text
          })

          aiResponse = result.response
          conversationRounds.value = result.history_length || 0

          console.log('âœ… FunAudioLLMè¯­éŸ³å¯¹è¯æˆåŠŸ')
        } else {
          const errorMsg = result.error || ''
          if (errorMsg.includes('æœªè¯†åˆ«åˆ°æœ‰æ•ˆè¯­éŸ³å†…å®¹')) {
            console.log('ğŸ”‡ æœªæ£€æµ‹åˆ°è¯­éŸ³å†…å®¹ï¼Œè‡ªåŠ¨ç»“æŸé€šè¯')
            endCall()
            return
          } else {
            throw new Error(result.error || 'FunAudioLLMå¯¹è¯å¤±è´¥')
          }
        }
      } else {
        throw new Error('æ— æœ‰æ•ˆè¾“å…¥')
      }

      if (aiResponse) {
        const aiMessage = addMessage({
          content: aiResponse,
          isUser: false
        })

        await speakText(aiResponse)
      }
    } catch (error: any) {
      console.error('Voice processing failed:', error)

      if (!userMessage) {
        userMessage = addMessage({
          content: transcript || '[è¯­éŸ³è¾“å…¥]',
          isUser: true
        })
      }

      addMessage({
        content: 'æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯­éŸ³æ—¶å‡ºç°äº†é—®é¢˜ã€‚è¯·ç¨åé‡è¯•ã€‚',
        isUser: false
      })
    }

    callState.value = 'connected'
  }

  // æ–‡æœ¬è½¬è¯­éŸ³
  async function speakText(text: string): Promise<void> {
    try {
      const cleanedText = cleanTextForSpeech(text)

      console.log('ğŸ”Š å¼€å§‹è¯­éŸ³åˆæˆ:', cleanedText.substring(0, 50) + '...')

      if (!cleanedText.trim()) {
        console.warn('âš ï¸ æ¸…ç†åçš„æ–‡æœ¬ä¸ºç©ºï¼Œè·³è¿‡è¯­éŸ³åˆæˆ')
        isAIPlaying.value = false
        callState.value = 'connected'
        return
      }

      isAIPlaying.value = true
      callState.value = 'speaking'

      try {
        const audioBuffer = await synthesizeSpeech({
          text: cleanedText,
          voice: 'zh-CN-XiaoxiaoNeural',
          rate: 0.9,
          pitch: 1.1
        })

        const audioBlob = new Blob([audioBuffer], { type: 'audio/wav' })
        const audioUrl = URL.createObjectURL(audioBlob)

        const audio = new Audio(audioUrl)
        audio.volume = isMuted.value ? 0 : 0.8
        currentAudio.value = audio

        audio.onplay = () => {
          console.log('âœ… AIè¯­éŸ³æ’­æ”¾å¼€å§‹')
        }

        audio.onended = () => {
          console.log('âœ… AIè¯­éŸ³æ’­æ”¾ç»“æŸ')
          isAIPlaying.value = false
          callState.value = 'connected'
          URL.revokeObjectURL(audioUrl)

          // æ’­æ”¾ç»“æŸåç»§ç»­å½•éŸ³
          setTimeout(() => {
            if (funAudioAvailable.value && callState.value === 'connected') {
              startRecording()
            }
          }, 500)
        }

        audio.onerror = (event) => {
          console.error('âŒ éŸ³é¢‘æ’­æ”¾é”™è¯¯:', event)
          isAIPlaying.value = false
          callState.value = 'connected'
          URL.revokeObjectURL(audioUrl)
        }

        await audio.play()
      } catch (error) {
        console.error('âŒ TTS APIè°ƒç”¨å¤±è´¥:', error)
        isAIPlaying.value = false
        callState.value = 'connected'
      }
    } catch (error) {
      console.error('âŒ è¯­éŸ³åˆæˆå¤±è´¥:', error)
      isAIPlaying.value = false
      callState.value = 'connected'
    }
  }

  // å¼€å§‹é€šè¯
  async function startCall(): Promise<void> {
    if (!funAudioAvailable.value) {
      console.log('âš ï¸ æ£€æµ‹åˆ°FunAudioLLMæœåŠ¡ä¸å¯ç”¨ï¼Œå°è¯•é‡æ–°æ£€æŸ¥çŠ¶æ€...')
      await checkServiceStatus()
      await new Promise(resolve => setTimeout(resolve, 1000))
    }

    try {
      callState.value = 'connecting'
      console.log('ğŸ¤ å¯åŠ¨FunAudioLLMå½•éŸ³æ¨¡å¼')
      await initRecording()
      callState.value = 'connected'

      setTimeout(() => {
        startRecording()
      }, 1000)

      messages.value = []
    } catch (error: any) {
      console.error('âŒ å¼€å§‹é€šè¯å¤±è´¥:', error)
      callState.value = 'idle'
      throw new Error('å¼€å§‹é€šè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥éº¦å…‹é£æƒé™å¹¶é‡è¯•')
    }
  }

  // ç»“æŸé€šè¯ - æ·»åŠ åœæ­¢éŸ³é¢‘é˜Ÿåˆ—
  function endCall(): void {
    callState.value = 'idle'
    isRecording.value = false
    isAIPlaying.value = false
    currentTranscript.value = ''
    currentAIResponse.value = ''
    currentPlayingText.value = ''

    // åœæ­¢éŸ³é¢‘é˜Ÿåˆ—
    if (audioQueue.value) {
      audioQueue.value.stop()
      audioQueue.value = null
    }

    // åœæ­¢æ™ºèƒ½é™éŸ³æ£€æµ‹
    stopAudioMonitoring()

    // åœæ­¢å½•éŸ³
    if (mediaRecorder.value && mediaRecorder.value.state === 'recording') {
      mediaRecorder.value.stop()
    }
    mediaRecorder.value = null

    // åœæ­¢éŸ³é¢‘æµ
    if (audioStream.value) {
      audioStream.value.getTracks().forEach(track => track.stop())
      audioStream.value = null
    }

    // å…³é—­éŸ³é¢‘ä¸Šä¸‹æ–‡
    if (audioContext.value) {
      audioContext.value.close()
      audioContext.value = null
    }
    analyserNode.value = null

    // åœæ­¢å½“å‰æ’­æ”¾çš„éŸ³é¢‘
    if (currentAudio.value) {
      currentAudio.value.pause()
      currentAudio.value = null
    }

    console.log('ğŸ“ é€šè¯å·²ç»“æŸï¼Œæ‰€æœ‰èµ„æºå·²æ¸…ç†')
  }

  // åˆ‡æ¢é™éŸ³ - æ”¯æŒéŸ³é¢‘é˜Ÿåˆ—
  function toggleMute(): void {
    isMuted.value = !isMuted.value
    
    if (isMuted.value) {
      // é™éŸ³ï¼šåœæ­¢éŸ³é¢‘é˜Ÿåˆ—
      if (audioQueue.value) {
        audioQueue.value.stop()
      }
      if (currentAudio.value) {
        currentAudio.value.pause()
      }
      isAIPlaying.value = false
    } else {
      // å–æ¶ˆé™éŸ³ï¼šè®¾ç½®éŸ³é‡
      if (audioQueue.value) {
        audioQueue.value.setVolume(0.8)
      }
    }
  }

  // åˆ‡æ¢æµå¼æ¨¡å¼
  function toggleStreamMode(): void {
    isStreamMode.value = !isStreamMode.value
    console.log(`ğŸ”„ æµå¼æ¨¡å¼å·²${isStreamMode.value ? 'å¯ç”¨' : 'ç¦ç”¨'}`)
  }

  // ä¸­æ–­AIè¯´è¯
  function interruptAI(): void {
    if (isAIPlaying.value) {
      if (currentAudio.value) {
        currentAudio.value.pause()
      }
      isAIPlaying.value = false
      callState.value = 'connected'

      if (funAudioAvailable.value) {
        setTimeout(() => {
          startRecording()
        }, 500)
      }
    }
  }

  // æ¸…é™¤å¯¹è¯å†å²
  async function clearHistory(): Promise<void> {
    try {
      const success = await clearConversationHistory(sessionId.value)
      if (success) {
        messages.value = []
        conversationRounds.value = 0
        console.log('âœ… å¯¹è¯å†å²å·²æ¸…é™¤')
      } else {
        console.error('âŒ æ¸…é™¤å¯¹è¯å†å²å¤±è´¥')
      }
    } catch (error) {
      console.error('âŒ æ¸…é™¤å¯¹è¯å†å²é”™è¯¯:', error)
    }
  }

  // é‡æ–°å¼€å§‹ä¼šè¯
  function restartSession(): void {
    sessionId.value = `voice-chat-${Date.now()}`
    messages.value = []
    conversationRounds.value = 0
    console.log('ğŸ”„ ä¼šè¯å·²é‡æ–°å¼€å§‹')
  }

  // è·å–çŠ¶æ€æ˜¾ç¤ºæ–‡æœ¬
  function getStatusText(): string {
    switch (callState.value) {
      case 'idle':
        return 'æœªè¿æ¥'
      case 'connecting':
        return 'æ­£åœ¨è¿æ¥...'
      case 'connected':
        return 'å·²è¿æ¥'
      case 'listening':
        return 'æ­£åœ¨å¬æ‚¨è¯´è¯...'
      case 'speaking':
        return 'AIæ­£åœ¨å›å¤...'
      case 'processing':
        return 'æ­£åœ¨å¤„ç†...'
      default:
        return 'æœªçŸ¥çŠ¶æ€'
    }
  }

  // è®¾ç½®é€‰ä¸­çš„çŸ¥è¯†åº“
  function setSelectedKnowledgeBase(knowledgeBase: KnowledgeBase | null): void {
    selectedKnowledgeBase.value = knowledgeBase
    console.log('ğŸ“š è¯­éŸ³èŠå¤©çŸ¥è¯†åº“å·²åˆ‡æ¢:', knowledgeBase?.name || 'æ— ')
  }

  // é…ç½®æ™ºèƒ½é™éŸ³æ£€æµ‹å‚æ•°
  function configureSilenceDetection(config: {
    threshold?: number
    timeout?: number
    minRecordingTime?: number
    maxRecordingTime?: number
  }): void {
    if (config.threshold !== undefined) {
      silenceThreshold.value = Math.max(0, Math.min(255, config.threshold))
    }
    if (config.timeout !== undefined) {
      silenceTimeout.value = Math.max(500, config.timeout)
    }
    if (config.minRecordingTime !== undefined) {
      minRecordingTime.value = Math.max(500, config.minRecordingTime)
    }
    if (config.maxRecordingTime !== undefined) {
      maxRecordingTime.value = Math.max(5000, config.maxRecordingTime)
    }
    
    console.log('ğŸ”§ æ™ºèƒ½é™éŸ³æ£€æµ‹é…ç½®å·²æ›´æ–°:', {
      threshold: silenceThreshold.value,
      timeout: silenceTimeout.value,
      minRecordingTime: minRecordingTime.value,
      maxRecordingTime: maxRecordingTime.value
    })
  }

  return {
    // çŠ¶æ€
    messages,
    callState,
    isRecording,
    isAIPlaying,
    isMuted,
    sessionId,
    conversationRounds,
    currentTranscript,
    funAudioAvailable,
    speechRecognitionAvailable,
    silenceDetectionActive,
    
    // æµå¼å¤„ç†ç›¸å…³çŠ¶æ€
    isStreamMode,
    currentAIResponse,
    currentPlayingText,
    
    // æ™ºèƒ½é™éŸ³æ£€æµ‹é…ç½®
    silenceThreshold,
    silenceTimeout,
    minRecordingTime,
    maxRecordingTime,
    
    // çŸ¥è¯†åº“ç›¸å…³çŠ¶æ€
    selectedKnowledgeBase,
    
    // è®¡ç®—å±æ€§
    hasMessages,
    isConnected,
    canStartCall,
    
    // æ–¹æ³•
    addMessage,
    checkServiceStatus,
    startCall,
    endCall,
    startRecording,
    stopRecording,
    toggleMute,
    interruptAI,
    clearHistory,
    restartSession,
    getStatusText,
    configureSilenceDetection,
    setSelectedKnowledgeBase,
    toggleStreamMode
  }
}) 