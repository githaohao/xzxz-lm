import { defineStore } from 'pinia'
import { ref, computed } from 'vue'
import type { VoiceMessage, CallState } from '@/types'
import { generateId, cleanTextForSpeech } from '@/utils'
import { 
  sendVoiceMessage, 
  synthesizeSpeech, 
  checkFunAudioStatus, 
  clearConversationHistory 
} from '@/utils/api'

export const useVoiceStore = defineStore('voice', () => {
  // çŠ¶æ€
  const messages = ref<VoiceMessage[]>([])
  const callState = ref<CallState>('idle')
  const isRecording = ref(false)
  const isAIPlaying = ref(false)
  const isMuted = ref(false)
  const sessionId = ref(`voice-chat-${Date.now()}`)
  const conversationRounds = ref(0)
  const currentTranscript = ref('')
  const funAudioAvailable = ref(false)
  const speechRecognitionAvailable = ref(false)

  // éŸ³é¢‘ç›¸å…³å¼•ç”¨
  const audioStream = ref<MediaStream | null>(null)
  const audioContext = ref<AudioContext | null>(null)
  const currentAudio = ref<HTMLAudioElement | null>(null)
  const analyserNode = ref<AnalyserNode | null>(null)
  const mediaRecorder = ref<MediaRecorder | null>(null)
  
  // æ™ºèƒ½é™éŸ³æ£€æµ‹é…ç½®
  const silenceThreshold = ref(30) // é™éŸ³é˜ˆå€¼ (0-255)
  const silenceTimeout = ref(2000) // é™éŸ³è¶…æ—¶æ—¶é—´ (æ¯«ç§’)
  const minRecordingTime = ref(1000) // æœ€å°å½•éŸ³æ—¶é—´ (æ¯«ç§’)
  const maxRecordingTime = ref(30000) // æœ€å¤§å½•éŸ³æ—¶é—´ (æ¯«ç§’)
  
  // é™éŸ³æ£€æµ‹çŠ¶æ€
  const lastSoundTime = ref(0)
  const recordingStartTime = ref(0)
  const silenceDetectionActive = ref(false)

  // è®¡ç®—å±æ€§
  const hasMessages = computed(() => messages.value.length > 0)
  const isConnected = computed(() => callState.value !== 'idle')
  const canStartCall = computed(() => funAudioAvailable.value && callState.value === 'idle')

  // æ·»åŠ æ¶ˆæ¯
  function addMessage(message: Omit<VoiceMessage, 'id' | 'timestamp'>) {
    const newMessage: VoiceMessage = {
      ...message,
      id: generateId(),
      timestamp: new Date()
    }
    messages.value.push(newMessage)
    return newMessage
  }

  // éŸ³é¢‘ç›‘æµ‹å‡½æ•°
  function startAudioMonitoring(): void {
    if (!audioContext.value || !analyserNode.value) return

    const bufferLength = analyserNode.value.frequencyBinCount
    const dataArray = new Uint8Array(bufferLength)
    
    silenceDetectionActive.value = true
    lastSoundTime.value = Date.now()
    recordingStartTime.value = Date.now()

    function checkAudioLevel(): void {
      if (!silenceDetectionActive.value || !analyserNode.value) return

      analyserNode.value.getByteFrequencyData(dataArray)
      
      // è®¡ç®—å¹³å‡éŸ³é‡
      let sum = 0
      for (let i = 0; i < bufferLength; i++) {
        sum += dataArray[i]
      }
      const averageVolume = sum / bufferLength

      const currentTime = Date.now()
      const recordingDuration = currentTime - recordingStartTime.value
      const silenceDuration = currentTime - lastSoundTime.value

      // å¦‚æœæ£€æµ‹åˆ°å£°éŸ³ï¼Œæ›´æ–°æœ€åå£°éŸ³æ—¶é—´
      if (averageVolume > silenceThreshold.value) {
        lastSoundTime.value = currentTime
      }

      // æ£€æŸ¥æ˜¯å¦åº”è¯¥åœæ­¢å½•éŸ³
      const shouldStop = (
        // é™éŸ³æ—¶é—´è¶…è¿‡é˜ˆå€¼ä¸”å·²å½•éŸ³æœ€å°æ—¶é—´
        (silenceDuration > silenceTimeout.value && recordingDuration > minRecordingTime.value) ||
        // å½•éŸ³æ—¶é—´è¶…è¿‡æœ€å¤§æ—¶é—´
        recordingDuration > maxRecordingTime.value
      )

      if (shouldStop) {
        console.log(`ğŸ”‡ æ™ºèƒ½é™éŸ³æ£€æµ‹: å¹³å‡éŸ³é‡=${averageVolume.toFixed(1)}, é™éŸ³æ—¶é•¿=${silenceDuration}ms, å½•éŸ³æ—¶é•¿=${recordingDuration}ms`)
        stopRecording()
        return
      }

      // ç»§ç»­ç›‘æµ‹
      requestAnimationFrame(checkAudioLevel)
    }

    checkAudioLevel()
  }

  // åœæ­¢éŸ³é¢‘ç›‘æµ‹
  function stopAudioMonitoring(): void {
    silenceDetectionActive.value = false
  }

  // åœæ­¢å½•éŸ³
  function stopRecording(): void {
    if (mediaRecorder.value && mediaRecorder.value.state === 'recording') {
      mediaRecorder.value.stop()
      isRecording.value = false
      stopAudioMonitoring()
      console.log('ğŸ¤ å½•éŸ³å·²åœæ­¢')
    }
  }

  // æ£€æŸ¥æœåŠ¡çŠ¶æ€
  async function checkServiceStatus() {
    try {
      funAudioAvailable.value = await checkFunAudioStatus()
      
      // æ£€æŸ¥Web Speech APIæ”¯æŒ
      speechRecognitionAvailable.value = !!(
        (window as any).webkitSpeechRecognition || (window as any).SpeechRecognition
      )
      
      console.log('æœåŠ¡çŠ¶æ€æ£€æŸ¥:', {
        funAudio: funAudioAvailable.value,
        speechRecognition: speechRecognitionAvailable.value
      })
    } catch (error) {
      console.error('æ£€æŸ¥æœåŠ¡çŠ¶æ€å¤±è´¥:', error)
      funAudioAvailable.value = false
    }
  }

  // åˆå§‹åŒ–å½•éŸ³
  async function initRecording(): Promise<void> {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ 
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true
        } 
      })
      
      audioStream.value = stream
      audioContext.value = new AudioContext()
      
      // åˆ›å»ºéŸ³é¢‘åˆ†æå™¨ç”¨äºæ™ºèƒ½é™éŸ³æ£€æµ‹
      const source = audioContext.value.createMediaStreamSource(stream)
      analyserNode.value = audioContext.value.createAnalyser()
      analyserNode.value.fftSize = 256
      analyserNode.value.smoothingTimeConstant = 0.8
      source.connect(analyserNode.value)
      
      console.log('âœ… å½•éŸ³åˆå§‹åŒ–æˆåŠŸï¼Œæ™ºèƒ½é™éŸ³æ£€æµ‹å·²å¯ç”¨')
    } catch (error) {
      console.error('âŒ å½•éŸ³åˆå§‹åŒ–å¤±è´¥:', error)
      throw new Error('æ— æ³•è®¿é—®éº¦å…‹é£ï¼Œè¯·æ£€æŸ¥æƒé™è®¾ç½®')
    }
  }

  // å¼€å§‹å½•éŸ³
  async function startRecording(): Promise<void> {
    if (!audioStream.value) {
      await initRecording()
    }

    if (!audioStream.value) {
      throw new Error('éŸ³é¢‘æµæœªåˆå§‹åŒ–')
    }

    try {
      mediaRecorder.value = new MediaRecorder(audioStream.value, {
        mimeType: 'audio/webm;codecs=opus'
      })

      const audioChunks: Blob[] = []
      
      mediaRecorder.value.ondataavailable = (event) => {
        if (event.data.size > 0) {
          audioChunks.push(event.data)
        }
      }

      mediaRecorder.value.onstop = async () => {
        const audioBlob = new Blob(audioChunks, { type: 'audio/webm' })
        await handleUserSpeech('', audioBlob)
      }

      mediaRecorder.value.start()
      isRecording.value = true
      callState.value = 'listening'
      
      // å¯åŠ¨æ™ºèƒ½é™éŸ³æ£€æµ‹
      startAudioMonitoring()

      console.log('ğŸ¤ å¼€å§‹å½•éŸ³ (æ™ºèƒ½é™éŸ³æ£€æµ‹å·²å¯ç”¨)')
    } catch (error) {
      console.error('âŒ å¼€å§‹å½•éŸ³å¤±è´¥:', error)
      throw error
    }
  }

  // å¤„ç†ç”¨æˆ·è¯­éŸ³
  async function handleUserSpeech(transcript: string, audioBlob?: Blob): Promise<void> {
    if (!transcript.trim() && !audioBlob) return

    callState.value = 'processing'
    let userMessage: VoiceMessage | null = null
    let aiResponse = ''

    try {
      if (funAudioAvailable.value && audioBlob) {
        console.log('ğŸ¯ ä½¿ç”¨FunAudioLLMæµç¨‹')

        const result = await sendVoiceMessage(audioBlob, sessionId.value, 'auto')

        if (result.success) {
          userMessage = addMessage({
            content: result.recognized_text || '[è¯­éŸ³è¾“å…¥]',
            isUser: true,
            recognizedText: result.recognized_text
          })

          aiResponse = result.response
          conversationRounds.value = result.history_length || 0

          console.log('âœ… FunAudioLLMè¯­éŸ³å¯¹è¯æˆåŠŸ')
        } else {
          const errorMsg = result.error || ''
          if (errorMsg.includes('æœªè¯†åˆ«åˆ°æœ‰æ•ˆè¯­éŸ³å†…å®¹')) {
            console.log('ğŸ”‡ æœªæ£€æµ‹åˆ°è¯­éŸ³å†…å®¹ï¼Œè‡ªåŠ¨ç»“æŸé€šè¯')
            endCall()
            return
          } else {
            throw new Error(result.error || 'FunAudioLLMå¯¹è¯å¤±è´¥')
          }
        }
      } else {
        throw new Error('æ— æœ‰æ•ˆè¾“å…¥')
      }

      if (aiResponse) {
        const aiMessage = addMessage({
          content: aiResponse,
          isUser: false
        })

        await speakText(aiResponse)
      }
    } catch (error: any) {
      console.error('âŒ å¤„ç†è¯­éŸ³å¤±è´¥:', error)

      if (!userMessage) {
        userMessage = addMessage({
          content: transcript || '[è¯­éŸ³è¾“å…¥]',
          isUser: true
        })
      }

      addMessage({
        content: 'æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯­éŸ³æ—¶å‡ºç°äº†é—®é¢˜ã€‚è¯·ç¨åé‡è¯•ã€‚',
        isUser: false
      })
    }

    callState.value = 'connected'
  }

  // æ–‡æœ¬è½¬è¯­éŸ³
  async function speakText(text: string): Promise<void> {
    try {
      const cleanedText = cleanTextForSpeech(text)

      console.log('ğŸ”Š å¼€å§‹è¯­éŸ³åˆæˆ:', cleanedText.substring(0, 50) + '...')

      if (!cleanedText.trim()) {
        console.warn('âš ï¸ æ¸…ç†åçš„æ–‡æœ¬ä¸ºç©ºï¼Œè·³è¿‡è¯­éŸ³åˆæˆ')
        isAIPlaying.value = false
        callState.value = 'connected'
        return
      }

      isAIPlaying.value = true
      callState.value = 'speaking'

      try {
        const audioBuffer = await synthesizeSpeech({
          text: cleanedText,
          voice: 'zh-CN-XiaoxiaoNeural',
          rate: 0.9,
          pitch: 1.1
        })

        const audioBlob = new Blob([audioBuffer], { type: 'audio/wav' })
        const audioUrl = URL.createObjectURL(audioBlob)

        const audio = new Audio(audioUrl)
        audio.volume = isMuted.value ? 0 : 0.8
        currentAudio.value = audio

        audio.onplay = () => {
          console.log('âœ… AIè¯­éŸ³æ’­æ”¾å¼€å§‹')
        }

        audio.onended = () => {
          console.log('âœ… AIè¯­éŸ³æ’­æ”¾ç»“æŸ')
          isAIPlaying.value = false
          callState.value = 'connected'
          URL.revokeObjectURL(audioUrl)

          // æ’­æ”¾ç»“æŸåç»§ç»­å½•éŸ³
          setTimeout(() => {
            if (funAudioAvailable.value && callState.value === 'connected') {
              startRecording()
            }
          }, 500)
        }

        audio.onerror = (event) => {
          console.error('âŒ éŸ³é¢‘æ’­æ”¾é”™è¯¯:', event)
          isAIPlaying.value = false
          callState.value = 'connected'
          URL.revokeObjectURL(audioUrl)
        }

        await audio.play()
      } catch (error) {
        console.error('âŒ TTS APIè°ƒç”¨å¤±è´¥:', error)
        isAIPlaying.value = false
        callState.value = 'connected'
      }
    } catch (error) {
      console.error('âŒ è¯­éŸ³åˆæˆå¤±è´¥:', error)
      isAIPlaying.value = false
      callState.value = 'connected'
    }
  }

  // å¼€å§‹é€šè¯
  async function startCall(): Promise<void> {
    if (!funAudioAvailable.value) {
      console.log('âš ï¸ æ£€æµ‹åˆ°FunAudioLLMæœåŠ¡ä¸å¯ç”¨ï¼Œå°è¯•é‡æ–°æ£€æŸ¥çŠ¶æ€...')
      await checkServiceStatus()
      await new Promise(resolve => setTimeout(resolve, 1000))
    }

    try {
      callState.value = 'connecting'
      console.log('ğŸ¤ å¯åŠ¨FunAudioLLMå½•éŸ³æ¨¡å¼')
      await initRecording()
      callState.value = 'connected'

      setTimeout(() => {
        startRecording()
      }, 1000)

      messages.value = []
    } catch (error: any) {
      console.error('âŒ å¼€å§‹é€šè¯å¤±è´¥:', error)
      callState.value = 'idle'
      throw new Error('å¼€å§‹é€šè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥éº¦å…‹é£æƒé™å¹¶é‡è¯•')
    }
  }

  // ç»“æŸé€šè¯
  function endCall(): void {
    callState.value = 'idle'
    isRecording.value = false
    isAIPlaying.value = false
    currentTranscript.value = ''

    // åœæ­¢æ™ºèƒ½é™éŸ³æ£€æµ‹
    stopAudioMonitoring()

    // åœæ­¢å½•éŸ³
    if (mediaRecorder.value && mediaRecorder.value.state === 'recording') {
      mediaRecorder.value.stop()
    }
    mediaRecorder.value = null

    // åœæ­¢éŸ³é¢‘æµ
    if (audioStream.value) {
      audioStream.value.getTracks().forEach(track => track.stop())
      audioStream.value = null
    }

    // å…³é—­éŸ³é¢‘ä¸Šä¸‹æ–‡
    if (audioContext.value) {
      audioContext.value.close()
      audioContext.value = null
    }
    analyserNode.value = null

    // åœæ­¢å½“å‰æ’­æ”¾çš„éŸ³é¢‘
    if (currentAudio.value) {
      currentAudio.value.pause()
      currentAudio.value = null
    }

    console.log('ğŸ“ é€šè¯å·²ç»“æŸï¼Œæ‰€æœ‰èµ„æºå·²æ¸…ç†')
  }

  // åˆ‡æ¢é™éŸ³
  function toggleMute(): void {
    isMuted.value = !isMuted.value
    if (currentAudio.value) {
      currentAudio.value.pause()
      isAIPlaying.value = false
    }
  }

  // ä¸­æ–­AIè¯´è¯
  function interruptAI(): void {
    if (isAIPlaying.value) {
      if (currentAudio.value) {
        currentAudio.value.pause()
      }
      isAIPlaying.value = false
      callState.value = 'connected'

      if (funAudioAvailable.value) {
        setTimeout(() => {
          startRecording()
        }, 500)
      }
    }
  }

  // æ¸…é™¤å¯¹è¯å†å²
  async function clearHistory(): Promise<void> {
    try {
      const success = await clearConversationHistory(sessionId.value)
      if (success) {
        messages.value = []
        conversationRounds.value = 0
        console.log('âœ… å¯¹è¯å†å²å·²æ¸…é™¤')
      } else {
        console.error('âŒ æ¸…é™¤å¯¹è¯å†å²å¤±è´¥')
      }
    } catch (error) {
      console.error('âŒ æ¸…é™¤å¯¹è¯å†å²é”™è¯¯:', error)
    }
  }

  // é‡æ–°å¼€å§‹ä¼šè¯
  function restartSession(): void {
    sessionId.value = `voice-chat-${Date.now()}`
    messages.value = []
    conversationRounds.value = 0
    console.log('ğŸ”„ ä¼šè¯å·²é‡æ–°å¼€å§‹')
  }

  // è·å–çŠ¶æ€æ˜¾ç¤ºæ–‡æœ¬
  function getStatusText(): string {
    switch (callState.value) {
      case 'idle':
        return 'æœªè¿æ¥'
      case 'connecting':
        return 'æ­£åœ¨è¿æ¥...'
      case 'connected':
        return 'å·²è¿æ¥'
      case 'listening':
        return 'æ­£åœ¨å¬æ‚¨è¯´è¯...'
      case 'speaking':
        return 'AIæ­£åœ¨å›å¤...'
      case 'processing':
        return 'æ­£åœ¨å¤„ç†...'
      default:
        return 'æœªçŸ¥çŠ¶æ€'
    }
  }

  // é…ç½®æ™ºèƒ½é™éŸ³æ£€æµ‹å‚æ•°
  function configureSilenceDetection(config: {
    threshold?: number
    timeout?: number
    minRecordingTime?: number
    maxRecordingTime?: number
  }): void {
    if (config.threshold !== undefined) {
      silenceThreshold.value = Math.max(0, Math.min(255, config.threshold))
    }
    if (config.timeout !== undefined) {
      silenceTimeout.value = Math.max(500, config.timeout)
    }
    if (config.minRecordingTime !== undefined) {
      minRecordingTime.value = Math.max(500, config.minRecordingTime)
    }
    if (config.maxRecordingTime !== undefined) {
      maxRecordingTime.value = Math.max(5000, config.maxRecordingTime)
    }
    
    console.log('ğŸ”§ æ™ºèƒ½é™éŸ³æ£€æµ‹é…ç½®å·²æ›´æ–°:', {
      threshold: silenceThreshold.value,
      timeout: silenceTimeout.value,
      minRecordingTime: minRecordingTime.value,
      maxRecordingTime: maxRecordingTime.value
    })
  }

  return {
    // çŠ¶æ€
    messages,
    callState,
    isRecording,
    isAIPlaying,
    isMuted,
    sessionId,
    conversationRounds,
    currentTranscript,
    funAudioAvailable,
    speechRecognitionAvailable,
    silenceDetectionActive,
    
    // æ™ºèƒ½é™éŸ³æ£€æµ‹é…ç½®
    silenceThreshold,
    silenceTimeout,
    minRecordingTime,
    maxRecordingTime,
    
    // è®¡ç®—å±æ€§
    hasMessages,
    isConnected,
    canStartCall,
    
    // æ–¹æ³•
    addMessage,
    checkServiceStatus,
    startCall,
    endCall,
    startRecording,
    stopRecording,
    toggleMute,
    interruptAI,
    clearHistory,
    restartSession,
    getStatusText,
    configureSilenceDetection
  }
}) 