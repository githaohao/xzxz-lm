"""
è®¾å¤‡æ£€æµ‹å’Œä¼˜åŒ–å·¥å…·æ¨¡å—
æä¾›è®¾å¤‡æ£€æµ‹ã€Apple Silicon MPSä¼˜åŒ–ã€å†…å­˜ç®¡ç†ç­‰åŠŸèƒ½
"""

import os
import torch
import logging
from typing import Dict, Any, Optional
from pathlib import Path

logger = logging.getLogger(__name__)

class DeviceManager:
    """è®¾å¤‡ç®¡ç†å·¥å…·ç±»"""
    
    @staticmethod
    def get_optimal_device() -> str:
        """
        è·å–æœ€ä¼˜è®¡ç®—è®¾å¤‡
        
        Returns:
            str: è®¾å¤‡ç±»å‹ ("cuda", "mps", "cpu")
        """
        try:
            if torch.cuda.is_available():
                device = "cuda"
                logger.info("ğŸš€ æ£€æµ‹åˆ° CUDAï¼Œä½¿ç”¨ GPU åŠ é€Ÿ")
            elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                device = "mps"
                logger.info("ğŸ æ£€æµ‹åˆ° Apple Siliconï¼Œä½¿ç”¨ MPS åŠ é€Ÿ")
            else:
                device = "cpu"
                logger.info("ğŸ”§ ä½¿ç”¨ CPU æ¨¡å¼")
            
            return device
            
        except Exception as e:
            logger.error(f"âŒ è®¾å¤‡æ£€æµ‹å¤±è´¥: {e}")
            return "cpu"
    
    @staticmethod
    def get_cache_dir(env_var: str, default_path: str) -> str:
        """
        è·å–ç¼“å­˜ç›®å½•è·¯å¾„
        
        Args:
            env_var: ç¯å¢ƒå˜é‡å
            default_path: é»˜è®¤è·¯å¾„
            
        Returns:
            str: ç¼“å­˜ç›®å½•è·¯å¾„
        """
        try:
            cache_dir = os.getenv(env_var, default_path)
            
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            Path(cache_dir).mkdir(parents=True, exist_ok=True)
            
            logger.info(f"ğŸ“ ç¼“å­˜ç›®å½•: {cache_dir}")
            return cache_dir
            
        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºç¼“å­˜ç›®å½•å¤±è´¥: {e}")
            # å›é€€åˆ°é»˜è®¤è·¯å¾„
            Path(default_path).mkdir(parents=True, exist_ok=True)
            return default_path
    
    @staticmethod
    def get_model_device_config(device: str, model_type: str = "default") -> Dict[str, Any]:
        """
        è·å–æ¨¡å‹è®¾å¤‡é…ç½®
        
        Args:
            device: è®¾å¤‡ç±»å‹ ("cuda", "mps", "cpu")
            model_type: æ¨¡å‹ç±»å‹ ("funasr", "whisper", "default")
            
        Returns:
            Dict[str, Any]: è®¾å¤‡é…ç½®ä¿¡æ¯
        """
        try:
            config = {
                "device": device,
                "fallback_reason": None,
                "optimizations": []
            }
            
            # éªŒè¯è®¾å¤‡å¯ç”¨æ€§
            if device == "cuda":
                if not torch.cuda.is_available():
                    config["device"] = "cpu"
                    config["fallback_reason"] = "CUDAä¸å¯ç”¨ï¼Œå›é€€åˆ°CPU"
                else:
                    config["optimizations"].append("CUDAåŠ é€Ÿ")
                    
            elif device == "mps":
                if not (hasattr(torch.backends, 'mps') and torch.backends.mps.is_available()):
                    config["device"] = "cpu"
                    config["fallback_reason"] = "MPSä¸å¯ç”¨ï¼Œå›é€€åˆ°CPU"
                else:
                    config["optimizations"].append("Apple Silicon MPSåŠ é€Ÿ")
                    
            else:
                config["optimizations"].append("CPUæ¨¡å¼")
            
            # æ ¹æ®æ¨¡å‹ç±»å‹è¿›è¡Œç‰¹æ®Šé…ç½®
            if model_type == "funasr":
                if config["device"] == "mps":
                    # FunASRåœ¨MPSä¸Šçš„ç‰¹æ®Šé…ç½®
                    config["optimizations"].append("FunASR MPSä¼˜åŒ–")
                elif config["device"] == "cuda":
                    config["optimizations"].append("FunASR CUDAä¼˜åŒ–")
                    
            elif model_type == "whisper":
                if config["device"] == "mps":
                    # Whisperåœ¨MPSä¸Šå¯èƒ½éœ€è¦ç‰¹æ®Šå¤„ç†
                    config["optimizations"].append("Whisper MPSå…¼å®¹")
                    
            logger.info(f"ğŸ”§ æ¨¡å‹è®¾å¤‡é…ç½®: {config['device']} ({model_type})")
            
            return config
            
        except Exception as e:
            logger.error(f"âŒ è·å–æ¨¡å‹è®¾å¤‡é…ç½®å¤±è´¥: {e}")
            return {
                "device": "cpu",
                "fallback_reason": f"é…ç½®å¤±è´¥: {str(e)}",
                "optimizations": ["CPUå›é€€æ¨¡å¼"]
            }
    
    @staticmethod
    def setup_mps_optimization() -> Dict[str, Any]:
        """
        è®¾ç½® Apple Silicon MPS ä¼˜åŒ–
        
        Returns:
            Dict[str, Any]: ä¼˜åŒ–é…ç½®ç»“æœ
        """
        try:
            if not (hasattr(torch.backends, 'mps') and torch.backends.mps.is_available()):
                return {
                    "enabled": False,
                    "reason": "MPS ä¸å¯ç”¨",
                    "device": "cpu"
                }
            
            # å¯ç”¨ MPS å›é€€æœºåˆ¶
            os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"
            
            # è®¾ç½®å†…å­˜ä½¿ç”¨æ¯”ä¾‹
            try:
                torch.mps.set_per_process_memory_fraction(0.8)
            except Exception as e:
                logger.warning(f"âš ï¸ è®¾ç½®MPSå†…å­˜æ¯”ä¾‹å¤±è´¥: {e}")
            
            # å¯ç”¨ TF32 ä¼˜åŒ–ï¼ˆå¦‚æœæ”¯æŒï¼‰
            try:
                torch.backends.mps.allow_tf32 = True
            except Exception as e:
                logger.warning(f"âš ï¸ å¯ç”¨TF32ä¼˜åŒ–å¤±è´¥: {e}")
            
            logger.info("âœ… Apple Silicon MPS ä¼˜åŒ–å·²å¯ç”¨")
            
            return {
                "enabled": True,
                "device": "mps",
                "optimizations": [
                    "MPSå›é€€æœºåˆ¶",
                    "å†…å­˜ä½¿ç”¨æ¯”ä¾‹é™åˆ¶(80%)",
                    "TF32ä¼˜åŒ–"
                ],
                "memory_fraction": 0.8
            }
            
        except Exception as e:
            logger.error(f"âŒ MPSä¼˜åŒ–è®¾ç½®å¤±è´¥: {e}")
            return {
                "enabled": False,
                "error": str(e),
                "device": "cpu"
            }
    
    @staticmethod
    def setup_device_optimization(device: str) -> Dict[str, Any]:
        """
        æ ¹æ®è®¾å¤‡ç±»å‹è®¾ç½®ä¼˜åŒ–é…ç½®
        
        Args:
            device: è®¾å¤‡ç±»å‹
            
        Returns:
            Dict[str, Any]: ä¼˜åŒ–é…ç½®ç»“æœ
        """
        try:
            optimizations = []
            
            if device == "mps":
                # Apple Silicon MPS ä¼˜åŒ–
                mps_config = DeviceManager.setup_mps_optimization()
                optimizations.extend(mps_config.get("optimizations", []))
                
            elif device == "cuda":
                # CUDA ä¼˜åŒ–
                try:
                    torch.backends.cudnn.benchmark = True
                    torch.backends.cudnn.deterministic = False
                    optimizations.append("CUDNNåŸºå‡†æµ‹è¯•ä¼˜åŒ–")
                    optimizations.append("CUDNNéç¡®å®šæ€§ä¼˜åŒ–")
                except Exception as e:
                    logger.warning(f"âš ï¸ CUDAä¼˜åŒ–è®¾ç½®å¤±è´¥: {e}")
                
            elif device == "cpu":
                # CPU ä¼˜åŒ–
                try:
                    torch.set_num_threads(torch.get_num_threads())
                    optimizations.append(f"CPUçº¿ç¨‹æ•°: {torch.get_num_threads()}")
                except Exception as e:
                    logger.warning(f"âš ï¸ CPUä¼˜åŒ–è®¾ç½®å¤±è´¥: {e}")
            
            logger.info(f"âœ… {device.upper()} è®¾å¤‡ä¼˜åŒ–é…ç½®å®Œæˆ")
            
            return {
                "device": device,
                "optimizations": optimizations,
                "success": True
            }
            
        except Exception as e:
            logger.error(f"âŒ è®¾å¤‡ä¼˜åŒ–é…ç½®å¤±è´¥: {e}")
            return {
                "device": device,
                "optimizations": [],
                "success": False,
                "error": str(e)
            }
    
    @staticmethod
    def get_memory_usage(device: str = None) -> str:
        """
        è·å–è®¾å¤‡å†…å­˜ä½¿ç”¨æƒ…å†µ
        
        Args:
            device: è®¾å¤‡ç±»å‹ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨æ£€æµ‹
            
        Returns:
            str: å†…å­˜ä½¿ç”¨æƒ…å†µæè¿°
        """
        try:
            if device is None:
                device = DeviceManager.get_optimal_device()
            
            if device == "mps":
                try:
                    allocated = torch.mps.current_allocated_memory() / 1024 / 1024  # MB
                    return f"{allocated:.1f} MB (MPS)"
                except Exception:
                    return "MPS å†…å­˜ä¿¡æ¯ä¸å¯ç”¨"
                    
            elif device == "cuda":
                try:
                    allocated = torch.cuda.memory_allocated() / 1024 / 1024  # MB
                    cached = torch.cuda.memory_reserved() / 1024 / 1024  # MB
                    return f"å·²åˆ†é…: {allocated:.1f} MB, å·²ç¼“å­˜: {cached:.1f} MB (CUDA)"
                except Exception:
                    return "CUDA å†…å­˜ä¿¡æ¯ä¸å¯ç”¨"
                    
            else:
                return "CPU æ¨¡å¼"
                
        except Exception as e:
            logger.error(f"âŒ è·å–å†…å­˜ä½¿ç”¨æƒ…å†µå¤±è´¥: {e}")
            return "å†…å­˜ä¿¡æ¯ä¸å¯ç”¨"
    
    @staticmethod
    def clear_device_cache(device: str = None) -> bool:
        """
        æ¸…ç†è®¾å¤‡ç¼“å­˜
        
        Args:
            device: è®¾å¤‡ç±»å‹ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨æ£€æµ‹
            
        Returns:
            bool: æ¸…ç†æ˜¯å¦æˆåŠŸ
        """
        try:
            if device is None:
                device = DeviceManager.get_optimal_device()
            
            if device == "mps":
                torch.mps.empty_cache()
                logger.info("ğŸ§¹ MPS ç¼“å­˜å·²æ¸…ç†")
                
            elif device == "cuda":
                torch.cuda.empty_cache()
                logger.info("ğŸ§¹ CUDA ç¼“å­˜å·²æ¸…ç†")
                
            else:
                logger.info("â„¹ï¸ CPU æ¨¡å¼æ— éœ€æ¸…ç†ç¼“å­˜")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ¸…ç†è®¾å¤‡ç¼“å­˜å¤±è´¥: {e}")
            return False
    
    @staticmethod
    def get_device_info(device: str = None) -> Dict[str, Any]:
        """
        è·å–è®¾å¤‡è¯¦ç»†ä¿¡æ¯
        
        Args:
            device: è®¾å¤‡ç±»å‹ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨æ£€æµ‹
            
        Returns:
            Dict[str, Any]: è®¾å¤‡ä¿¡æ¯
        """
        try:
            if device is None:
                device = DeviceManager.get_optimal_device()
            
            info = {
                "device": device,
                "torch_version": torch.__version__,
                "memory_usage": DeviceManager.get_memory_usage(device)
            }
            
            if device == "cuda":
                info.update({
                    "cuda_available": torch.cuda.is_available(),
                    "cuda_device_count": torch.cuda.device_count(),
                    "cuda_device_name": torch.cuda.get_device_name(0) if torch.cuda.is_available() else None,
                    "cudnn_version": torch.backends.cudnn.version() if torch.backends.cudnn.is_available() else None
                })
                
            elif device == "mps":
                info.update({
                    "mps_available": torch.backends.mps.is_available() if hasattr(torch.backends, 'mps') else False,
                    "acceleration": "Apple Silicon MPS"
                })
                
            else:
                info.update({
                    "cpu_count": torch.get_num_threads(),
                    "acceleration": "CPU"
                })
            
            return info
            
        except Exception as e:
            logger.error(f"âŒ è·å–è®¾å¤‡ä¿¡æ¯å¤±è´¥: {e}")
            return {
                "device": "unknown",
                "error": str(e)
            }

# ä¾¿åˆ©å‡½æ•°
def get_optimal_device() -> str:
    """è·å–æœ€ä¼˜è®¾å¤‡ä¾¿åˆ©å‡½æ•°"""
    return DeviceManager.get_optimal_device()

def get_cache_dir(env_var: str, default_path: str) -> str:
    """è·å–ç¼“å­˜ç›®å½•ä¾¿åˆ©å‡½æ•°"""
    return DeviceManager.get_cache_dir(env_var, default_path)

def get_model_device_config(device: str, model_type: str = "default") -> Dict[str, Any]:
    """è·å–æ¨¡å‹è®¾å¤‡é…ç½®ä¾¿åˆ©å‡½æ•°"""
    return DeviceManager.get_model_device_config(device, model_type)

def setup_mps_optimization() -> Dict[str, Any]:
    """è®¾ç½®MPSä¼˜åŒ–ä¾¿åˆ©å‡½æ•°"""
    return DeviceManager.setup_mps_optimization()

def setup_device_optimization(device: str) -> Dict[str, Any]:
    """è®¾ç½®è®¾å¤‡ä¼˜åŒ–ä¾¿åˆ©å‡½æ•°"""
    return DeviceManager.setup_device_optimization(device)

def get_memory_usage(device: str = None) -> str:
    """è·å–å†…å­˜ä½¿ç”¨æƒ…å†µä¾¿åˆ©å‡½æ•°"""
    return DeviceManager.get_memory_usage(device)

def clear_device_cache(device: str = None) -> bool:
    """æ¸…ç†è®¾å¤‡ç¼“å­˜ä¾¿åˆ©å‡½æ•°"""
    return DeviceManager.clear_device_cache(device)

def get_device_info(device: str = None) -> Dict[str, Any]:
    """è·å–è®¾å¤‡ä¿¡æ¯ä¾¿åˆ©å‡½æ•°"""
    return DeviceManager.get_device_info(device) 