from fastapi import APIRouter, HTTPException, UploadFile, File, Form, Depends
from fastapi.responses import StreamingResponse, FileResponse
import uuid
import os
import time
from datetime import datetime
from typing import List, Optional, Dict
import aiofiles
import logging
import traceback
import json

from app.models.schemas import (
    ChatRequest, ChatResponse, FileUploadResponse, 
    OCRRequest, OCRResponse, TTSRequest, TTSResponse,
    MessageType, MultimodalStreamRequest, RAGSearchRequest, 
    RAGSearchResponse, DocumentInfo
)
from app.services.lm_studio_service import lm_studio_service
from app.services.ocr_service import ocr_service
from app.services.tts_service import tts_service
from app.services.rag_service import rag_service
from app.services.chat_history_service import chat_history_service
from app.models.chat_history import CreateMessageDto, MessageRole, MessageType as ChatMessageType
from app.config import settings
from app.middleware.auth import get_current_user_id
from app.database import database
# å¯¼å…¥æ–‡ä»¶æå–æœåŠ¡
from app.services.file_extraction_service import file_extraction_service

logger = logging.getLogger(__name__)
router = APIRouter(prefix="/chat", tags=["chat"])

@router.post("/stream")
async def chat_completion_stream(
    request: ChatRequest,
    user_id: int = Depends(get_current_user_id)
):
    """æµå¼èŠå¤©å“åº” - æ”¯æŒè‡ªåŠ¨ä¿å­˜èŠå¤©å†å²"""
    try:
        async def generate():
            ai_response_content = ""  # æ”¶é›†AIå›å¤å†…å®¹
            
            # å¦‚æœæä¾›äº†session_idï¼Œå…ˆä¿å­˜ç”¨æˆ·æ¶ˆæ¯
            if request.session_id:
                try:
                    user_message_dto = CreateMessageDto(
                        session_id=request.session_id,
                        role=MessageRole.USER,
                        content=request.message,
                        message_type=ChatMessageType.TEXT
                    )
                    await chat_history_service.add_message(user_id, user_message_dto)
                    logger.info(f"âœ… ç”¨æˆ·æ¶ˆæ¯å·²ä¿å­˜åˆ°ä¼šè¯ {request.session_id}")
                except Exception as e:
                    logger.warning(f"âš ï¸ ä¿å­˜ç”¨æˆ·æ¶ˆæ¯å¤±è´¥: {e}")
                    # ä¸ä¸­æ–­æµç¨‹ï¼Œç»§ç»­å¤„ç†
            
            # ç”ŸæˆAIå›å¤
            async for chunk in lm_studio_service.chat_completion_stream(request):
                if chunk.strip():
                    ai_response_content += chunk  # æ”¶é›†å†…å®¹
                    yield f"data: {json.dumps({'type': 'content', 'content': chunk})}\n\n"
            
            # å¦‚æœæä¾›äº†session_idï¼Œä¿å­˜AIå›å¤
            if request.session_id and ai_response_content.strip():
                try:
                    ai_message_dto = CreateMessageDto(
                        session_id=request.session_id,
                        role=MessageRole.ASSISTANT,
                        content=ai_response_content.strip(),
                        message_type=ChatMessageType.TEXT
                    )
                    await chat_history_service.add_message(user_id, ai_message_dto)
                    logger.info(f"âœ… AIå›å¤å·²ä¿å­˜åˆ°ä¼šè¯ {request.session_id}")
                except Exception as e:
                    logger.warning(f"âš ï¸ ä¿å­˜AIå›å¤å¤±è´¥: {e}")
                    # ä¸å½±å“ç”¨æˆ·ä½“éªŒ
            
            yield f"data: {json.dumps({'type': 'complete'})}\n\n"
            yield "data: [DONE]\n\n"
        
        return StreamingResponse(
            generate(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "X-Accel-Buffering": "no"
            }
        )
        
    except Exception as e:
        logger.error(f"æµå¼èŠå¤©è¯·æ±‚å¤„ç†å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"å¤„ç†è¯·æ±‚å¤±è´¥: {str(e)}")

@router.post("/upload", response_model=FileUploadResponse)
async def upload_file(
    file: UploadFile = File(...),
    session_id: Optional[str] = Form(None),  # æ·»åŠ å¯é€‰çš„ä¼šè¯IDå‚æ•°
    user_id: int = Depends(get_current_user_id)
):
    """æ–‡ä»¶ä¸Šä¼ å¤„ç† - æ”¯æŒPDFæ™ºèƒ½æ£€æµ‹å’Œå¤„ç†"""
    try:
        # æ£€æŸ¥æ–‡ä»¶ç±»å‹
        file_ext = os.path.splitext(file.filename)[1].lower()
        if file_ext not in settings.allowed_file_types:
            raise HTTPException(
                status_code=400,
                detail=f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_ext}"
            )
        
        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        content = await file.read()
        if len(content) > settings.max_file_size:
            raise HTTPException(
                status_code=400,
                detail=f"æ–‡ä»¶å¤§å°è¶…è¿‡é™åˆ¶: {settings.max_file_size / 1024 / 1024:.1f}MB"
            )
        
        # ç”Ÿæˆå”¯ä¸€æ–‡ä»¶å
        file_id = str(uuid.uuid4())
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        safe_filename = f"{timestamp}_{file_id}_{file.filename}"
        file_path = os.path.join(settings.upload_dir, safe_filename)
        
        # ä¿å­˜æ–‡ä»¶
        async with aiofiles.open(file_path, 'wb') as f:
            await f.write(content)
        
        logger.info(f"æ–‡ä»¶ä¸Šä¼ æˆåŠŸ: {safe_filename}")
        
        # ä½¿ç”¨ç»Ÿä¸€æ–‡ä»¶æå–æœåŠ¡å¤„ç†æ‰€æœ‰æ”¯æŒçš„æ–‡ä»¶ç±»å‹
        try:
            logger.info(f"ğŸ“„ å¼€å§‹å¤„ç†æ–‡ä»¶: {file.filename}")
            
            # ä½¿ç”¨ç»Ÿä¸€çš„æ–‡ä»¶æå–æœåŠ¡
            extracted_text, extraction_metadata = await file_extraction_service.extract_text_from_file(
                file_content=content,
                filename=file.filename,
                file_type=file.content_type
            )
            
            # æ„å»ºå“åº”ï¼Œé›†æˆæå–ç»“æœ
            response = FileUploadResponse(
                file_id=file_id,
                file_name=file.filename,
                file_path=file_path,
                file_size=len(content),
                file_type=file_ext,
                is_pdf=extraction_metadata.get('is_pdf', False),
                is_text_pdf=extraction_metadata.get('is_text_pdf', False),
                char_count=extraction_metadata.get('char_count', 0),
                text_content=extracted_text if extracted_text and extracted_text.strip() else None,
                processing_status=extraction_metadata.get('processing_status', 'processed')
            )
            
            # å¦‚æœæå–åˆ°äº†æ–‡æœ¬å†…å®¹ï¼Œè¿›è¡ŒRAGå¤„ç†
            if extracted_text and extracted_text.strip():
                try:
                    doc_id = await rag_service.process_document(
                        content=extracted_text,
                        filename=file.filename,
                        file_type=file_ext
                    )
                    response.doc_id = doc_id
                    response.rag_processed = True
                    
                    # æ›´æ–°å¤„ç†çŠ¶æ€
                    extraction_method = extraction_metadata.get('extraction_method', 'unknown')
                    if extraction_method == 'text_pdf':
                        response.processing_status = f"æ–‡æœ¬PDF - RAGå¤„ç†å®Œæˆ"
                    elif extraction_method == 'ocr_pdf':
                        confidence = extraction_metadata.get('confidence', 0)
                        response.processing_status = f"æ‰«æPDF - OCR+RAGå®Œæˆ (ç½®ä¿¡åº¦: {confidence:.2f})"
                    elif extraction_method == 'ocr_image':
                        confidence = extraction_metadata.get('confidence', 0)
                        response.processing_status = f"å›¾ç‰‡OCR+RAGå®Œæˆ (ç½®ä¿¡åº¦: {confidence:.2f})"
                    else:
                        response.processing_status = f"{extraction_method.upper()}æ–‡ä»¶ - RAGå¤„ç†å®Œæˆ"
                    
                    logger.info(f"ğŸš€ æ–‡ä»¶RAGå¤„ç†å®Œæˆ: {file.filename}, doc_id: {doc_id}, æ–‡æœ¬é•¿åº¦: {len(extracted_text)}")
                    
                except Exception as rag_error:
                    logger.warning(f"âš ï¸ RAGå¤„ç†å¤±è´¥: {rag_error}")
                    response.processing_status = f"{extraction_metadata.get('processing_status', 'processed')} - RAGå¤„ç†å¤±è´¥"
                    # RAGå¤±è´¥ä¸å½±å“æ–‡ä»¶ä¸Šä¼ 
            else:
                logger.warning(f"âš ï¸ æ–‡ä»¶æœªæå–åˆ°æœ‰æ•ˆæ–‡æœ¬å†…å®¹: {file.filename}")
                response.processing_status = f"{extraction_metadata.get('processing_status', 'processed')} - æ— æ–‡æœ¬å†…å®¹"
                
        except Exception as extraction_error:
            logger.error(f"âŒ æ–‡ä»¶å¤„ç†å¤±è´¥: {extraction_error}")
            
            # å¤„ç†å¤±è´¥æ—¶è¿”å›åŸºç¡€ä¿¡æ¯
            response = FileUploadResponse(
                file_id=file_id,
                file_name=file.filename,
                file_path=file_path,
                file_size=len(content),
                file_type=file_ext,
                is_pdf=(file_ext == '.pdf'),
                processing_status=f"æ–‡ä»¶å¤„ç†å¤±è´¥: {str(extraction_error)}"
            )
            # æ–‡ä»¶å¤„ç†å¤±è´¥ä¸å½±å“æ–‡ä»¶ä¸Šä¼ ï¼Œæ–‡ä»¶ä»ç„¶å¯ç”¨
        
        # å¦‚æœæä¾›äº†session_idä¸”RAGå¤„ç†æˆåŠŸï¼Œåˆ›å»ºä¼šè¯æ–‡æ¡£å…³è”
        if session_id and response.doc_id and response.rag_processed:
            try:
                await create_session_document_association(
                    session_id=session_id,
                    doc_id=response.doc_id,
                    user_id=user_id,
                    filename=response.file_name,
                    file_type=response.file_type,
                    file_size=response.file_size,
                    chunk_count=await get_document_chunk_count(response.doc_id)
                )
                logger.info(f"âœ… æ–‡æ¡£å·²å…³è”åˆ°ä¼šè¯: session_id={session_id}, doc_id={response.doc_id}")
            except Exception as e:
                logger.warning(f"âš ï¸ åˆ›å»ºä¼šè¯æ–‡æ¡£å…³è”å¤±è´¥: {e}")
                # ä¸å½±å“æ–‡ä»¶ä¸Šä¼ ç»“æœ
        
        return response
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {str(e)}")

@router.get("/sessions/{session_id}/documents")
async def get_session_documents_api(
    session_id: str,
    user_id: int = Depends(get_current_user_id)
):
    """è·å–ä¼šè¯å…³è”çš„æ–‡æ¡£åˆ—è¡¨"""
    try:
        documents = await get_session_documents(session_id, user_id)
        return {
            "code": 200,
            "msg": "è·å–æˆåŠŸ",
            "data": documents
        }
    except Exception as e:
        logger.error(f"è·å–ä¼šè¯æ–‡æ¡£åˆ—è¡¨å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–ä¼šè¯æ–‡æ¡£å¤±è´¥: {str(e)}")

@router.post("/ocr", response_model=OCRResponse)
async def extract_text(file_path: str = Form(...)):
    """OCRæ–‡æœ¬æå–"""
    try:
        if not os.path.exists(file_path):
            raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")
        
        file_ext = os.path.splitext(file_path)[1].lower()
        
        if file_ext == '.pdf':
            text, confidence, processing_time = await ocr_service.extract_text_from_pdf(file_path)
        elif file_ext in ['.png', '.jpg', '.jpeg']:
            text, confidence, processing_time = await ocr_service.extract_text_from_image(file_path)
        else:
            raise HTTPException(status_code=400, detail="ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹è¿›è¡ŒOCR")
        
        return OCRResponse(
            text=text,
            confidence=confidence,
            processing_time=processing_time,
            detected_language="zh-cn" if confidence > 0 else None
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"OCRå¤„ç†å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"OCRå¤„ç†å¤±è´¥: {str(e)}")

@router.post("/tts", response_model=TTSResponse)
async def text_to_speech(request: TTSRequest):
    """æ–‡å­—è½¬è¯­éŸ³"""
    try:
        audio_path, file_size = await tts_service.text_to_speech(
            text=request.text,
            voice=request.voice,
            rate=request.rate,
            volume=request.volume
        )
        
        return TTSResponse(
            audio_file=os.path.basename(audio_path),
            file_size=file_size
        )
        
    except Exception as e:
        logger.error(f"TTSè½¬æ¢å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"TTSè½¬æ¢å¤±è´¥: {str(e)}")

@router.get("/tts/audio/{filename}")
async def get_tts_audio(filename: str):
    """è·å–TTSéŸ³é¢‘æ–‡ä»¶"""
    try:
        audio_path = os.path.join(settings.upload_dir, "tts_audio", filename)
        
        if not os.path.exists(audio_path):
            raise HTTPException(status_code=404, detail="éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨")
        
        return FileResponse(
            audio_path,
            media_type="audio/mpeg",
            filename=filename
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–éŸ³é¢‘æ–‡ä»¶å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–éŸ³é¢‘æ–‡ä»¶å¤±è´¥: {str(e)}")

# æ–°å¢ï¼šå¤„ç†å·²é¢„å¤„ç†æ–‡ä»¶æ•°æ®çš„æµå¼èŠå¤©æ¥å£
@router.post("/multimodal/stream/processed")
async def multimodal_chat_stream_with_processed_data(
    request: MultimodalStreamRequest,
    user_id: int = Depends(get_current_user_id)
):
    """å¤„ç†å·²é¢„å¤„ç†æ–‡ä»¶æ•°æ®çš„æµå¼å¤šæ¨¡æ€èŠå¤©æ¥å£ - æ”¯æŒè‡ªåŠ¨ä¿å­˜èŠå¤©å†å²"""
    async def generate():
        try:
            logger.info(f"å¼€å§‹å¤„ç†æµå¼å¤šæ¨¡æ€èŠå¤©è¯·æ±‚: {request.message[:50]}...")
            
            ai_response_content = ""  # æ”¶é›†AIå›å¤å†…å®¹
            
            # å¦‚æœæä¾›äº†session_idï¼Œå…ˆä¿å­˜ç”¨æˆ·æ¶ˆæ¯
            if request.session_id:
                try:
                    # æ„å»ºæ–‡ä»¶å…ƒæ•°æ®
                    metadata = None
                    message_type = ChatMessageType.TEXT
                    if request.file_data:
                        message_type = ChatMessageType.MULTIMODAL
                        metadata = {
                            "fileName": request.file_data.name,
                            "fileSize": request.file_data.size,
                            "fileType": request.file_data.type,
                            "ragEnabled": request.file_data.rag_enabled,
                            "docId": request.file_data.doc_id,
                            "ocrCompleted": request.file_data.ocr_completed
                        }
                    
                    user_message_dto = CreateMessageDto(
                        session_id=request.session_id,
                        role=MessageRole.USER,
                        content=request.message,
                        message_type=message_type,
                        metadata=metadata
                    )
                    await chat_history_service.add_message(user_id, user_message_dto)
                    logger.info(f"âœ… å¤šæ¨¡æ€ç”¨æˆ·æ¶ˆæ¯å·²ä¿å­˜åˆ°ä¼šè¯ {request.session_id}")
                except Exception as e:
                    logger.warning(f"âš ï¸ ä¿å­˜å¤šæ¨¡æ€ç”¨æˆ·æ¶ˆæ¯å¤±è´¥: {e}")
                    # ä¸ä¸­æ–­æµç¨‹ï¼Œç»§ç»­å¤„ç†
            
            # æ„å»ºå®Œæ•´æ¶ˆæ¯
            full_message = request.message
            
            # å¦‚æœæœ‰æ–‡ä»¶æ•°æ®ï¼Œæ ¹æ®rag_enabledå†³å®šå¤„ç†æ–¹å¼
            if request.file_data:
                logger.info(f"å¤„ç†æ–‡ä»¶æ•°æ®: {request.file_data.name}")
                
                # æ£€æŸ¥æ˜¯å¦å¯ç”¨RAGåŠŸèƒ½
                if request.file_data.rag_enabled:
                    logger.info("å¯ç”¨RAGæ¨¡å¼ï¼Œè¿›è¡Œæ™ºèƒ½æ£€ç´¢...")
                    yield f"data: {json.dumps({'type': 'file_processing', 'message': 'ğŸ§  å¯ç”¨æ™ºèƒ½æ£€ç´¢æ¨¡å¼'})}\n\n"
                    
                    # æ£€æŸ¥æ˜¯å¦ä¸ºçŸ¥è¯†åº“æ£€ç´¢æˆ–å¤šæ–‡æ¡£å¤„ç†
                    doc_ids_to_search = []
                    is_multiple_docs = False
                    is_knowledge_base = False
                    
                    # ä¼˜å…ˆæ£€æŸ¥æ˜¯å¦ä¸ºçŸ¥è¯†åº“æ£€ç´¢
                    if hasattr(request.file_data, 'knowledge_base_id') and request.file_data.knowledge_base_id:
                        logger.info(f"æ£€æµ‹åˆ°çŸ¥è¯†åº“æ£€ç´¢: {request.file_data.knowledge_base_id}")
                        yield f"data: {json.dumps({'type': 'file_processing', 'message': 'ğŸ—‚ï¸ æ£€æµ‹åˆ°çŸ¥è¯†åº“ï¼Œæ­£åœ¨è·å–æ–‡æ¡£åˆ—è¡¨...'})}\n\n"
                        
                        # ä»çŸ¥è¯†åº“è·å–æ‰€æœ‰æ–‡æ¡£ID
                        kb_doc_ids = await rag_service.get_knowledge_base_documents(request.file_data.knowledge_base_id)
                        if kb_doc_ids:
                            doc_ids_to_search = kb_doc_ids
                            is_knowledge_base = True
                            is_multiple_docs = len(kb_doc_ids) > 1
                            doc_count = len(kb_doc_ids)
                            logger.info(f"çŸ¥è¯†åº“åŒ…å« {doc_count} ä¸ªæ–‡æ¡£")
                            yield f"data: {json.dumps({'type': 'file_processing', 'message': f'ğŸ“š çŸ¥è¯†åº“åŒ…å« {doc_count} ä¸ªæ–‡æ¡£ï¼Œå¼€å§‹æ™ºèƒ½æ£€ç´¢'})}\n\n"
                        else:
                            logger.warning(f"çŸ¥è¯†åº“ {request.file_data.knowledge_base_id} ä¸­æ²¡æœ‰æ–‡æ¡£")
                            yield f"data: {json.dumps({'type': 'file_processing', 'message': 'âš ï¸ çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ–‡æ¡£'})}\n\n"
                    else:
                        # æ£€æŸ¥æ˜¯å¦ä¸ºå¤šæ–‡æ¡£å¤„ç†ï¼ˆä¼ ç»Ÿæ–¹å¼ï¼‰
                        try:
                            # å°è¯•è§£ædoc_idæ˜¯å¦ä¸ºå¤šæ–‡æ¡£JSONæ ¼å¼
                            if request.file_data.doc_id and request.file_data.doc_id.startswith('{'):
                                multi_doc_data = json.loads(request.file_data.doc_id)
                                if multi_doc_data.get('type') == 'multiple' and 'doc_ids' in multi_doc_data:
                                    doc_ids_to_search = multi_doc_data['doc_ids']
                                    is_multiple_docs = True
                                    doc_count = len(doc_ids_to_search)
                                    logger.info(f"æ£€æµ‹åˆ°å¤šæ–‡æ¡£å¤„ç†: {doc_count} ä¸ªæ–‡æ¡£")
                                    yield f"data: {json.dumps({'type': 'file_processing', 'message': f'ğŸ“š æ£€æµ‹åˆ° {doc_count} ä¸ªæ–‡æ¡£ï¼Œå¼€å§‹å¤šæ–‡æ¡£æ£€ç´¢'})}\n\n"
                                else:
                                    # å•æ–‡æ¡£å¤„ç†
                                    doc_ids_to_search = [request.file_data.doc_id]
                            else:
                                # ä¼ ç»Ÿå•æ–‡æ¡£å¤„ç†
                                doc_ids_to_search = [request.file_data.doc_id] if request.file_data.doc_id else []
                        except json.JSONDecodeError:
                            # å¦‚æœè§£æå¤±è´¥ï¼ŒæŒ‰å•æ–‡æ¡£å¤„ç†
                            doc_ids_to_search = [request.file_data.doc_id] if request.file_data.doc_id else []
                    
                    # å¦‚æœæ–‡ä»¶è¿˜æ²¡æœ‰è¿›è¡ŒRAGå¤„ç†ï¼Œå…ˆè¿›è¡Œå¤„ç†ï¼ˆä»…é€‚ç”¨äºå•æ–‡æ¡£ï¼‰
                    if not doc_ids_to_search and request.file_data.content and not is_multiple_docs:
                        logger.info("å¼€å§‹RAGæ–‡æ¡£å¤„ç†...")
                        yield f"data: {json.dumps({'type': 'file_processing', 'message': 'æ­£åœ¨å¯¹æ–‡æ¡£è¿›è¡Œæ™ºèƒ½ç´¢å¼•...'})}\n\n"
                        
                        # å¤„ç†æ–‡æ¡£å¹¶ç”Ÿæˆdoc_id
                        doc_id = await rag_service.process_document(
                            content=request.file_data.content,
                            filename=request.file_data.name,
                            file_type=request.file_data.type
                        )
                        doc_ids_to_search = [doc_id]
                        yield f"data: {json.dumps({'type': 'file_processing', 'message': f'æ–‡æ¡£ç´¢å¼•å®Œæˆ: {request.file_data.name}'})}\n\n"
                    
                    # å¦‚æœæœ‰doc_idï¼Œä½¿ç”¨RAGæ£€ç´¢ç›¸å…³å†…å®¹
                    if doc_ids_to_search:
                        if is_multiple_docs:
                            logger.info(f"å¼€å§‹å¤šæ–‡æ¡£RAGæ£€ç´¢: {len(doc_ids_to_search)} ä¸ªæ–‡æ¡£")
                            yield f"data: {json.dumps({'type': 'file_processing', 'message': f'ğŸ” æ­£åœ¨ä» {len(doc_ids_to_search)} ä¸ªæ–‡æ¡£ä¸­æ£€ç´¢ç›¸å…³ç‰‡æ®µ...'})}\n\n"
                        else:
                            logger.info("å¼€å§‹å•æ–‡æ¡£RAGæ£€ç´¢...")
                            yield f"data: {json.dumps({'type': 'file_processing', 'message': 'æ­£åœ¨æ£€ç´¢ç›¸å…³æ–‡æ¡£ç‰‡æ®µ...'})}\n\n"
                        
                        relevant_chunks = await rag_service.search_relevant_chunks(
                            query=request.message,
                            doc_ids=doc_ids_to_search,
                            top_k=settings.rag_default_top_k,
                            min_similarity=settings.rag_default_min_similarity
                        )
                        
                        if relevant_chunks:
                            # æ„å»ºRAGä¸Šä¸‹æ–‡
                            rag_context = "\n\n[ç›¸å…³æ–‡æ¡£å†…å®¹]\n"
                            
                            # æŒ‰æ–‡æ¡£åˆ†ç»„æ˜¾ç¤ºç‰‡æ®µ
                            doc_chunks = {}
                            for chunk in relevant_chunks:
                                doc_id = chunk['metadata'].get('doc_id', 'unknown')
                                if doc_id not in doc_chunks:
                                    doc_chunks[doc_id] = []
                                doc_chunks[doc_id].append(chunk)
                            
                            chunk_index = 1
                            for doc_id, chunks in doc_chunks.items():
                                # è·å–æ–‡æ¡£åç§°
                                doc_name = chunks[0]['metadata'].get('filename', f'æ–‡æ¡£{doc_id[:8]}')
                                rag_context += f"\n--- æ¥è‡ªæ–‡æ¡£: {doc_name} ---\n"
                                
                                for chunk in chunks:
                                    rag_context += f"ç‰‡æ®µ{chunk_index} (ç›¸ä¼¼åº¦: {chunk['similarity']:.2f}):\n{chunk['content']}\n\n"
                                    chunk_index += 1
                            
                            full_message = request.message + rag_context
                            chunk_count = len(relevant_chunks)
                            doc_count = len(doc_chunks)
                            
                            if is_multiple_docs:
                                yield f"data: {json.dumps({'type': 'file_processing', 'message': f'âœ… ä» {doc_count} ä¸ªæ–‡æ¡£ä¸­æ£€ç´¢åˆ° {chunk_count} ä¸ªç›¸å…³ç‰‡æ®µ'})}\n\n"
                            else:
                                yield f"data: {json.dumps({'type': 'file_processing', 'message': f'ğŸ” æ£€ç´¢åˆ° {chunk_count} ä¸ªç›¸å…³ç‰‡æ®µ'})}\n\n"
                        else:
                            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç›¸å…³å†…å®¹ï¼Œæç¤ºæ— ç›¸å…³å†…å®¹
                            if is_multiple_docs:
                                yield f"data: {json.dumps({'type': 'file_processing', 'message': f'âš ï¸ åœ¨ {len(doc_ids_to_search)} ä¸ªæ–‡æ¡£ä¸­æœªæ‰¾åˆ°ç›¸å…³ç‰‡æ®µ'})}\n\n"
                            else:
                                yield f"data: {json.dumps({'type': 'file_processing', 'message': 'âš ï¸ æœªæ‰¾åˆ°ç›¸å…³ç‰‡æ®µ'})}\n\n"
                    else:
                        # æ²¡æœ‰doc_idä¸”æ²¡æœ‰contentï¼Œæ— æ³•å¤„ç†
                        yield f"data: {json.dumps({'type': 'file_processing', 'message': 'âŒ æ–‡æ¡£å¤„ç†å¤±è´¥ï¼šç¼ºå°‘å†…å®¹'})}\n\n"
                else:
                    # å…³é—­RAGæ¨¡å¼ï¼Œä½¿ç”¨å®Œæ•´æ–‡æ¡£å†…å®¹
                    logger.info("å…³é—­RAGæ¨¡å¼ï¼Œä½¿ç”¨å®Œæ•´æ–‡æ¡£å†…å®¹...")
                    yield f"data: {json.dumps({'type': 'file_processing', 'message': 'ğŸ“„ ä½¿ç”¨å®Œæ•´æ–‡æ¡£æ¨¡å¼'})}\n\n"
                    
                    if request.file_data.content:
                        # å¦‚æœæœ‰ç›´æ¥çš„å†…å®¹ï¼Œä½¿ç”¨å®ƒ
                        file_content = f"\n\n[æ–‡ä»¶å†…å®¹: {request.file_data.name}]\n{request.file_data.content}"
                        full_message = request.message + file_content
                        yield f"data: {json.dumps({'type': 'file_processing', 'message': f'å·²åŠ è½½å®Œæ•´æ–‡æ¡£: {request.file_data.name}'})}\n\n"
                    elif request.file_data.doc_id:
                        # å¦‚æœåªæœ‰doc_idï¼Œä»RAGç³»ç»Ÿè·å–æ‰€æœ‰åˆ†å—å†…å®¹
                        logger.info("ä»RAGç³»ç»Ÿè·å–å®Œæ•´æ–‡æ¡£å†…å®¹...")
                        yield f"data: {json.dumps({'type': 'file_processing', 'message': 'æ­£åœ¨è·å–å®Œæ•´æ–‡æ¡£å†…å®¹...'})}\n\n"
                        
                        # è·å–æ‰€æœ‰æ–‡æ¡£åˆ†å—
                        all_chunks = await rag_service.get_document_chunks(request.file_data.doc_id)
                        
                        if all_chunks:
                            # é‡å»ºå®Œæ•´æ–‡æ¡£ï¼ˆåˆ†å—å·²ç»æ’åºï¼‰
                            full_content = "\n".join([chunk['content'] for chunk in all_chunks])
                            
                            file_content = f"\n\n[æ–‡ä»¶å†…å®¹: {request.file_data.name}]\n{full_content}"
                            full_message = request.message + file_content
                            yield f"data: {json.dumps({'type': 'file_processing', 'message': f'å·²é‡å»ºå®Œæ•´æ–‡æ¡£: {request.file_data.name}'})}\n\n"
                        else:
                            yield f"data: {json.dumps({'type': 'file_processing', 'message': 'âŒ æ— æ³•è·å–æ–‡æ¡£å†…å®¹'})}\n\n"
                    else:
                        yield f"data: {json.dumps({'type': 'file_processing', 'message': 'âŒ æ–‡æ¡£å¤„ç†å¤±è´¥ï¼šç¼ºå°‘å†…å®¹å’ŒID'})}\n\n"
            

            
            # æ„å»ºèŠå¤©è¯·æ±‚
            chat_request = ChatRequest(
                message=full_message,
                history=request.history,
                temperature=request.temperature,
                max_tokens=request.max_tokens,
                stream=True
            )
            
            logger.info("å¼€å§‹AIæµå¼å¯¹è¯å¤„ç†...")
            
            # æµå¼è·å–AIå“åº”
            async for chunk in lm_studio_service.chat_completion_stream(chat_request):
                if chunk.strip():
                    ai_response_content += chunk  # æ”¶é›†å†…å®¹
                    yield f"data: {json.dumps({'type': 'content', 'content': chunk})}\n\n"
            
            # å¦‚æœæä¾›äº†session_idï¼Œä¿å­˜AIå›å¤
            if request.session_id and ai_response_content.strip():
                try:
                    ai_message_dto = CreateMessageDto(
                        session_id=request.session_id,
                        role=MessageRole.ASSISTANT,
                        content=ai_response_content.strip(),
                        message_type=ChatMessageType.TEXT
                    )
                    await chat_history_service.add_message(user_id, ai_message_dto)
                    logger.info(f"âœ… å¤šæ¨¡æ€AIå›å¤å·²ä¿å­˜åˆ°ä¼šè¯ {request.session_id}")
                except Exception as e:
                    logger.warning(f"âš ï¸ ä¿å­˜å¤šæ¨¡æ€AIå›å¤å¤±è´¥: {e}")
                    # ä¸å½±å“ç”¨æˆ·ä½“éªŒ
            
            # å‘é€å®Œæˆä¿¡å·
            yield f"data: {json.dumps({'type': 'complete'})}\n\n"
            yield "data: [DONE]\n\n"
            
        except Exception as e:
            logger.error(f"æµå¼å¤šæ¨¡æ€èŠå¤©å¤±è´¥: {e}")
            error_message = f"å¤„ç†è¯·æ±‚æ—¶å‡ºç°é”™è¯¯: {str(e)}"
            yield f"data: {json.dumps({'type': 'error', 'message': error_message})}\n\n"
            yield "data: [DONE]\n\n"

    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no"
        }
    )

# ==================== ä¼šè¯æ–‡æ¡£å…³è”ç®¡ç† ====================

async def create_session_document_association(
    session_id: str,
    doc_id: str,
    user_id: int,
    filename: str,
    file_type: str,
    file_size: int,
    chunk_count: int = 0
):
    """åˆ›å»ºä¼šè¯æ–‡æ¡£å…³è”"""
    try:
        async with database.get_connection() as db:
            await db.execute("""
                INSERT OR IGNORE INTO session_documents 
                (session_id, doc_id, user_id, filename, file_type, file_size, chunk_count)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            """, (session_id, doc_id, user_id, filename, file_type, file_size, chunk_count))
            await db.commit()
    except Exception as e:
        logger.error(f"åˆ›å»ºä¼šè¯æ–‡æ¡£å…³è”å¤±è´¥: {e}")
        raise

async def get_session_documents(session_id: str, user_id: int) -> List[Dict]:
    """è·å–ä¼šè¯å…³è”çš„æ–‡æ¡£åˆ—è¡¨"""
    try:
        async with database.get_connection() as db:
            cursor = await db.execute("""
                SELECT doc_id, filename, file_type, file_size, chunk_count, upload_time
                FROM session_documents 
                WHERE session_id = ? AND user_id = ?
                ORDER BY upload_time DESC
            """, (session_id, user_id))
            rows = await cursor.fetchall()
            
            documents = []
            for row in rows:
                documents.append({
                    "doc_id": row[0],
                    "filename": row[1],
                    "file_type": row[2],
                    "file_size": row[3],
                    "chunk_count": row[4],
                    "upload_time": row[5]
                })
            
            return documents
    except Exception as e:
        logger.error(f"è·å–ä¼šè¯æ–‡æ¡£åˆ—è¡¨å¤±è´¥: {e}")
        return []

async def get_document_chunk_count(doc_id: str) -> int:
    """è·å–æ–‡æ¡£çš„åˆ†å—æ•°é‡"""
    try:
        # ä»RAGæœåŠ¡è·å–æ–‡æ¡£ä¿¡æ¯
        doc_info = await rag_service.get_document_info(doc_id)
        if doc_info:
            return doc_info.get('chunk_count', 0)
        return 0
    except Exception as e:
        logger.warning(f"è·å–æ–‡æ¡£åˆ†å—æ•°é‡å¤±è´¥: {e}")
        return 0

@router.post("/extract-file-text")
async def extract_file_text(
    file: UploadFile = File(...),
    user_id: int = Depends(get_current_user_id)
):
    """
    æ–‡ä»¶æ–‡æœ¬æå–æµ‹è¯•ç«¯ç‚¹
    æ”¯æŒPDFã€DOCXã€TXTã€å›¾ç‰‡ç­‰å¤šç§æ ¼å¼
    """
    try:
        # è¯»å–æ–‡ä»¶å†…å®¹
        file_content = await file.read()
        
        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        if len(file_content) > settings.max_file_size:
            raise HTTPException(
                status_code=413,
                detail=f"æ–‡ä»¶å¤§å°è¶…è¿‡é™åˆ¶ ({settings.max_file_size / 1024 / 1024:.1f}MB)"
            )
        
        # ä½¿ç”¨ç»Ÿä¸€çš„æ–‡ä»¶æå–æœåŠ¡
        extracted_text, metadata = await file_extraction_service.extract_text_from_file(
            file_content, file.filename, file.content_type
        )
        
        return {
            "status": "success",
            "filename": file.filename,
            "file_type": file.content_type,
            "file_size": len(file_content),
            "extracted_text": extracted_text,
            "metadata": metadata,
            "supported_types": file_extraction_service.get_supported_file_types()
        }
        
    except Exception as e:
        logger.error(f"æ–‡ä»¶æå–å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æ–‡ä»¶æå–å¤±è´¥: {str(e)}")

@router.get("/supported-file-types")
async def get_supported_file_types():
    """è·å–æ”¯æŒçš„æ–‡ä»¶ç±»å‹"""
    return {
        "supported_types": file_extraction_service.get_supported_file_types(),
        "allowed_extensions": settings.allowed_file_types_list
    }
