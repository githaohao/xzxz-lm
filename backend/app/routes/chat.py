from fastapi import APIRouter, HTTPException, UploadFile, File, Form, Depends
from fastapi.responses import StreamingResponse, FileResponse
import uuid
import os
import time
from datetime import datetime
from typing import List, Optional
import aiofiles
import logging
import traceback
import json

from app.models.schemas import (
    ChatRequest, ChatResponse, FileUploadResponse, 
    OCRRequest, OCRResponse, TTSRequest, TTSResponse,
    MessageType, MultimodalStreamRequest, RAGSearchRequest, 
    RAGSearchResponse, DocumentInfo
)
from app.services.lm_studio_service import lm_studio_service
from app.services.ocr_service import ocr_service
from app.services.tts_service import tts_service
from app.services.rag_service import rag_service
from app.services.chat_history_service import chat_history_service
from app.models.chat_history import CreateMessageDto, MessageRole, MessageType as ChatMessageType
from app.config import settings
from app.middleware.auth import get_current_user_id

logger = logging.getLogger(__name__)
router = APIRouter(prefix="/chat", tags=["chat"])

@router.post("/stream")
async def chat_completion_stream(
    request: ChatRequest,
    user_id: int = Depends(get_current_user_id)
):
    """æµå¼èŠå¤©å“åº” - æ”¯æŒè‡ªåŠ¨ä¿å­˜èŠå¤©å†å²"""
    try:
        async def generate():
            ai_response_content = ""  # æ”¶é›†AIå›å¤å†…å®¹
            
            # å¦‚æœæä¾›äº†session_idï¼Œå…ˆä¿å­˜ç”¨æˆ·æ¶ˆæ¯
            if request.session_id:
                try:
                    user_message_dto = CreateMessageDto(
                        session_id=request.session_id,
                        role=MessageRole.USER,
                        content=request.message,
                        message_type=ChatMessageType.TEXT
                    )
                    await chat_history_service.add_message(user_id, user_message_dto)
                    logger.info(f"âœ… ç”¨æˆ·æ¶ˆæ¯å·²ä¿å­˜åˆ°ä¼šè¯ {request.session_id}")
                except Exception as e:
                    logger.warning(f"âš ï¸ ä¿å­˜ç”¨æˆ·æ¶ˆæ¯å¤±è´¥: {e}")
                    # ä¸ä¸­æ–­æµç¨‹ï¼Œç»§ç»­å¤„ç†
            
            # ç”ŸæˆAIå›å¤
            async for chunk in lm_studio_service.chat_completion_stream(request):
                if chunk.strip():
                    ai_response_content += chunk  # æ”¶é›†å†…å®¹
                    yield f"data: {json.dumps({'type': 'content', 'content': chunk})}\n\n"
            
            # å¦‚æœæä¾›äº†session_idï¼Œä¿å­˜AIå›å¤
            if request.session_id and ai_response_content.strip():
                try:
                    ai_message_dto = CreateMessageDto(
                        session_id=request.session_id,
                        role=MessageRole.ASSISTANT,
                        content=ai_response_content.strip(),
                        message_type=ChatMessageType.TEXT
                    )
                    await chat_history_service.add_message(user_id, ai_message_dto)
                    logger.info(f"âœ… AIå›å¤å·²ä¿å­˜åˆ°ä¼šè¯ {request.session_id}")
                except Exception as e:
                    logger.warning(f"âš ï¸ ä¿å­˜AIå›å¤å¤±è´¥: {e}")
                    # ä¸å½±å“ç”¨æˆ·ä½“éªŒ
            
            yield f"data: {json.dumps({'type': 'complete'})}\n\n"
            yield "data: [DONE]\n\n"
        
        return StreamingResponse(
            generate(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "X-Accel-Buffering": "no"
            }
        )
        
    except Exception as e:
        logger.error(f"æµå¼èŠå¤©è¯·æ±‚å¤„ç†å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"å¤„ç†è¯·æ±‚å¤±è´¥: {str(e)}")

@router.post("/upload", response_model=FileUploadResponse)
async def upload_file(file: UploadFile = File(...)):
    """æ–‡ä»¶ä¸Šä¼ å¤„ç†"""
    try:
        # æ£€æŸ¥æ–‡ä»¶ç±»å‹
        file_ext = os.path.splitext(file.filename)[1].lower()
        if file_ext not in settings.allowed_file_types:
            raise HTTPException(
                status_code=400,
                detail=f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_ext}"
            )
        
        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        content = await file.read()
        if len(content) > settings.max_file_size:
            raise HTTPException(
                status_code=400,
                detail=f"æ–‡ä»¶å¤§å°è¶…è¿‡é™åˆ¶: {settings.max_file_size / 1024 / 1024:.1f}MB"
            )
        
        # ç”Ÿæˆå”¯ä¸€æ–‡ä»¶å
        file_id = str(uuid.uuid4())
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        safe_filename = f"{timestamp}_{file_id}_{file.filename}"
        file_path = os.path.join(settings.upload_dir, safe_filename)
        
        # ä¿å­˜æ–‡ä»¶
        async with aiofiles.open(file_path, 'wb') as f:
            await f.write(content)
        
        logger.info(f"æ–‡ä»¶ä¸Šä¼ æˆåŠŸ: {safe_filename}")
        
        return FileUploadResponse(
            file_id=file_id,
            file_name=file.filename,
            file_path=file_path,
            file_size=len(content),
            file_type=file_ext
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {str(e)}")

@router.post("/ocr", response_model=OCRResponse)
async def extract_text(file_path: str = Form(...)):
    """OCRæ–‡æœ¬æå–"""
    try:
        if not os.path.exists(file_path):
            raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")
        
        file_ext = os.path.splitext(file_path)[1].lower()
        
        if file_ext == '.pdf':
            text, confidence, processing_time = await ocr_service.extract_text_from_pdf(file_path)
        elif file_ext in ['.png', '.jpg', '.jpeg']:
            text, confidence, processing_time = await ocr_service.extract_text_from_image(file_path)
        else:
            raise HTTPException(status_code=400, detail="ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹è¿›è¡ŒOCR")
        
        return OCRResponse(
            text=text,
            confidence=confidence,
            processing_time=processing_time,
            detected_language="zh-cn" if confidence > 0 else None
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"OCRå¤„ç†å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"OCRå¤„ç†å¤±è´¥: {str(e)}")

@router.post("/tts", response_model=TTSResponse)
async def text_to_speech(request: TTSRequest):
    """æ–‡å­—è½¬è¯­éŸ³"""
    try:
        audio_path, file_size = await tts_service.text_to_speech(
            text=request.text,
            voice=request.voice,
            rate=request.rate,
            volume=request.volume
        )
        
        return TTSResponse(
            audio_file=os.path.basename(audio_path),
            file_size=file_size
        )
        
    except Exception as e:
        logger.error(f"TTSè½¬æ¢å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"TTSè½¬æ¢å¤±è´¥: {str(e)}")

@router.get("/tts/audio/{filename}")
async def get_tts_audio(filename: str):
    """è·å–TTSéŸ³é¢‘æ–‡ä»¶"""
    try:
        audio_path = os.path.join(settings.upload_dir, "tts_audio", filename)
        
        if not os.path.exists(audio_path):
            raise HTTPException(status_code=404, detail="éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨")
        
        return FileResponse(
            audio_path,
            media_type="audio/mpeg",
            filename=filename
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–éŸ³é¢‘æ–‡ä»¶å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–éŸ³é¢‘æ–‡ä»¶å¤±è´¥: {str(e)}")

# æ–°å¢ï¼šå¤„ç†å·²é¢„å¤„ç†æ–‡ä»¶æ•°æ®çš„æµå¼èŠå¤©æ¥å£
@router.post("/multimodal/stream/processed")
async def multimodal_chat_stream_with_processed_data(
    request: MultimodalStreamRequest,
    user_id: int = Depends(get_current_user_id)
):
    """å¤„ç†å·²é¢„å¤„ç†æ–‡ä»¶æ•°æ®çš„æµå¼å¤šæ¨¡æ€èŠå¤©æ¥å£ - æ”¯æŒè‡ªåŠ¨ä¿å­˜èŠå¤©å†å²"""
    async def generate():
        try:
            logger.info(f"å¼€å§‹å¤„ç†æµå¼å¤šæ¨¡æ€èŠå¤©è¯·æ±‚: {request.message[:50]}...")
            
            ai_response_content = ""  # æ”¶é›†AIå›å¤å†…å®¹
            
            # å¦‚æœæä¾›äº†session_idï¼Œå…ˆä¿å­˜ç”¨æˆ·æ¶ˆæ¯
            if request.session_id:
                try:
                    # æ„å»ºæ–‡ä»¶å…ƒæ•°æ®
                    metadata = None
                    message_type = ChatMessageType.TEXT
                    if request.file_data:
                        message_type = ChatMessageType.MULTIMODAL
                        metadata = {
                            "fileName": request.file_data.name,
                            "fileSize": request.file_data.size,
                            "fileType": request.file_data.type,
                            "ragEnabled": request.file_data.rag_enabled,
                            "docId": request.file_data.doc_id,
                            "ocrCompleted": request.file_data.ocr_completed
                        }
                    
                    user_message_dto = CreateMessageDto(
                        session_id=request.session_id,
                        role=MessageRole.USER,
                        content=request.message,
                        message_type=message_type,
                        metadata=metadata
                    )
                    await chat_history_service.add_message(user_id, user_message_dto)
                    logger.info(f"âœ… å¤šæ¨¡æ€ç”¨æˆ·æ¶ˆæ¯å·²ä¿å­˜åˆ°ä¼šè¯ {request.session_id}")
                except Exception as e:
                    logger.warning(f"âš ï¸ ä¿å­˜å¤šæ¨¡æ€ç”¨æˆ·æ¶ˆæ¯å¤±è´¥: {e}")
                    # ä¸ä¸­æ–­æµç¨‹ï¼Œç»§ç»­å¤„ç†
            
            # æ„å»ºå®Œæ•´æ¶ˆæ¯
            full_message = request.message
            
            # å¦‚æœæœ‰æ–‡ä»¶æ•°æ®ï¼Œæ ¹æ®rag_enabledå†³å®šå¤„ç†æ–¹å¼
            if request.file_data:
                logger.info(f"å¤„ç†æ–‡ä»¶æ•°æ®: {request.file_data.name}")
                
                # æ£€æŸ¥æ˜¯å¦å¯ç”¨RAGåŠŸèƒ½
                if request.file_data.rag_enabled:
                    logger.info("å¯ç”¨RAGæ¨¡å¼ï¼Œè¿›è¡Œæ™ºèƒ½æ£€ç´¢...")
                    yield f"data: {json.dumps({'type': 'file_processing', 'message': 'ğŸ§  å¯ç”¨æ™ºèƒ½æ£€ç´¢æ¨¡å¼'})}\n\n"
                    
                    # å¦‚æœæ–‡ä»¶è¿˜æ²¡æœ‰è¿›è¡ŒRAGå¤„ç†ï¼Œå…ˆè¿›è¡Œå¤„ç†
                    if not request.file_data.doc_id and request.file_data.content:
                        logger.info("å¼€å§‹RAGæ–‡æ¡£å¤„ç†...")
                        yield f"data: {json.dumps({'type': 'file_processing', 'message': 'æ­£åœ¨å¯¹æ–‡æ¡£è¿›è¡Œæ™ºèƒ½ç´¢å¼•...'})}\n\n"
                        
                        # å¤„ç†æ–‡æ¡£å¹¶ç”Ÿæˆdoc_id
                        doc_id = await rag_service.process_document(
                            content=request.file_data.content,
                            filename=request.file_data.name,
                            file_type=request.file_data.type
                        )
                        request.file_data.doc_id = doc_id
                        yield f"data: {json.dumps({'type': 'file_processing', 'message': f'æ–‡æ¡£ç´¢å¼•å®Œæˆ: {request.file_data.name}'})}\n\n"
                    
                    # å¦‚æœæœ‰doc_idï¼Œä½¿ç”¨RAGæ£€ç´¢ç›¸å…³å†…å®¹
                    if request.file_data.doc_id:
                        logger.info("å¼€å§‹RAGæ£€ç´¢ç›¸å…³å†…å®¹...")
                        yield f"data: {json.dumps({'type': 'file_processing', 'message': 'æ­£åœ¨æ£€ç´¢ç›¸å…³æ–‡æ¡£ç‰‡æ®µ...'})}\n\n"
                        
                        relevant_chunks = await rag_service.search_relevant_chunks(
                            query=request.message,
                            doc_ids=[request.file_data.doc_id],
                            top_k=settings.rag_default_top_k,
                            min_similarity=settings.rag_default_min_similarity
                        )
                        
                        if relevant_chunks:
                            # æ„å»ºRAGä¸Šä¸‹æ–‡
                            rag_context = "\n\n[ç›¸å…³æ–‡æ¡£å†…å®¹]\n"
                            for i, chunk in enumerate(relevant_chunks, 1):
                                rag_context += f"ç‰‡æ®µ{i} (ç›¸ä¼¼åº¦: {chunk['similarity']:.2f}):\n{chunk['content']}\n\n"
                            
                            full_message = request.message + rag_context
                            chunk_count = len(relevant_chunks)
                            yield f"data: {json.dumps({'type': 'file_processing', 'message': f'ğŸ” æ£€ç´¢åˆ° {chunk_count} ä¸ªç›¸å…³ç‰‡æ®µ'})}\n\n"
                        else:
                            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç›¸å…³å†…å®¹ï¼Œæç¤ºæ— ç›¸å…³å†…å®¹
                            yield f"data: {json.dumps({'type': 'file_processing', 'message': 'âš ï¸ æœªæ‰¾åˆ°ç›¸å…³ç‰‡æ®µ'})}\n\n"
                    else:
                        # æ²¡æœ‰doc_idä¸”æ²¡æœ‰contentï¼Œæ— æ³•å¤„ç†
                        yield f"data: {json.dumps({'type': 'file_processing', 'message': 'âŒ æ–‡æ¡£å¤„ç†å¤±è´¥ï¼šç¼ºå°‘å†…å®¹'})}\n\n"
                else:
                    # å…³é—­RAGæ¨¡å¼ï¼Œä½¿ç”¨å®Œæ•´æ–‡æ¡£å†…å®¹
                    logger.info("å…³é—­RAGæ¨¡å¼ï¼Œä½¿ç”¨å®Œæ•´æ–‡æ¡£å†…å®¹...")
                    yield f"data: {json.dumps({'type': 'file_processing', 'message': 'ğŸ“„ ä½¿ç”¨å®Œæ•´æ–‡æ¡£æ¨¡å¼'})}\n\n"
                    
                    if request.file_data.content:
                        # å¦‚æœæœ‰ç›´æ¥çš„å†…å®¹ï¼Œä½¿ç”¨å®ƒ
                        file_content = f"\n\n[æ–‡ä»¶å†…å®¹: {request.file_data.name}]\n{request.file_data.content}"
                        full_message = request.message + file_content
                        yield f"data: {json.dumps({'type': 'file_processing', 'message': f'å·²åŠ è½½å®Œæ•´æ–‡æ¡£: {request.file_data.name}'})}\n\n"
                    elif request.file_data.doc_id:
                        # å¦‚æœåªæœ‰doc_idï¼Œä»RAGç³»ç»Ÿè·å–æ‰€æœ‰åˆ†å—å†…å®¹
                        logger.info("ä»RAGç³»ç»Ÿè·å–å®Œæ•´æ–‡æ¡£å†…å®¹...")
                        yield f"data: {json.dumps({'type': 'file_processing', 'message': 'æ­£åœ¨è·å–å®Œæ•´æ–‡æ¡£å†…å®¹...'})}\n\n"
                        
                        # è·å–æ‰€æœ‰æ–‡æ¡£åˆ†å—
                        all_chunks = await rag_service.get_document_chunks(request.file_data.doc_id)
                        
                        if all_chunks:
                            # é‡å»ºå®Œæ•´æ–‡æ¡£ï¼ˆåˆ†å—å·²ç»æ’åºï¼‰
                            full_content = "\n".join([chunk['content'] for chunk in all_chunks])
                            
                            file_content = f"\n\n[æ–‡ä»¶å†…å®¹: {request.file_data.name}]\n{full_content}"
                            full_message = request.message + file_content
                            yield f"data: {json.dumps({'type': 'file_processing', 'message': f'å·²é‡å»ºå®Œæ•´æ–‡æ¡£: {request.file_data.name}'})}\n\n"
                        else:
                            yield f"data: {json.dumps({'type': 'file_processing', 'message': 'âŒ æ— æ³•è·å–æ–‡æ¡£å†…å®¹'})}\n\n"
                    else:
                        yield f"data: {json.dumps({'type': 'file_processing', 'message': 'âŒ æ–‡æ¡£å¤„ç†å¤±è´¥ï¼šç¼ºå°‘å†…å®¹å’ŒID'})}\n\n"
            

            
            # æ„å»ºèŠå¤©è¯·æ±‚
            chat_request = ChatRequest(
                message=full_message,
                history=request.history,
                temperature=request.temperature,
                max_tokens=request.max_tokens,
                stream=True
            )
            
            logger.info("å¼€å§‹AIæµå¼å¯¹è¯å¤„ç†...")
            
            # æµå¼è·å–AIå“åº”
            async for chunk in lm_studio_service.chat_completion_stream(chat_request):
                if chunk.strip():
                    ai_response_content += chunk  # æ”¶é›†å†…å®¹
                    yield f"data: {json.dumps({'type': 'content', 'content': chunk})}\n\n"
            
            # å¦‚æœæä¾›äº†session_idï¼Œä¿å­˜AIå›å¤
            if request.session_id and ai_response_content.strip():
                try:
                    ai_message_dto = CreateMessageDto(
                        session_id=request.session_id,
                        role=MessageRole.ASSISTANT,
                        content=ai_response_content.strip(),
                        message_type=ChatMessageType.TEXT
                    )
                    await chat_history_service.add_message(user_id, ai_message_dto)
                    logger.info(f"âœ… å¤šæ¨¡æ€AIå›å¤å·²ä¿å­˜åˆ°ä¼šè¯ {request.session_id}")
                except Exception as e:
                    logger.warning(f"âš ï¸ ä¿å­˜å¤šæ¨¡æ€AIå›å¤å¤±è´¥: {e}")
                    # ä¸å½±å“ç”¨æˆ·ä½“éªŒ
            
            # å‘é€å®Œæˆä¿¡å·
            yield f"data: {json.dumps({'type': 'complete'})}\n\n"
            yield "data: [DONE]\n\n"
            
        except Exception as e:
            logger.error(f"æµå¼å¤šæ¨¡æ€èŠå¤©å¤±è´¥: {e}")
            error_message = f"å¤„ç†è¯·æ±‚æ—¶å‡ºç°é”™è¯¯: {str(e)}"
            yield f"data: {json.dumps({'type': 'error', 'message': error_message})}\n\n"
            yield "data: [DONE]\n\n"
    
    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no"
        }
    )
