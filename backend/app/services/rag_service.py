"""
RAG (Retrieval-Augmented Generation) æœåŠ¡
ç”¨äºæ™ºèƒ½æ–‡æ¡£æ£€ç´¢å’Œå¢å¼ºç”Ÿæˆ
"""

import os
import uuid
import hashlib
import logging
from typing import List, Dict, Optional, Tuple
from datetime import datetime
import asyncio
from concurrent.futures import ThreadPoolExecutor

import chromadb
from chromadb.config import Settings
from sentence_transformers import SentenceTransformer
from langchain.text_splitter import RecursiveCharacterTextSplitter
import numpy as np

from app.config import settings
from app.database import Database
from app.utils import TextProcessor, DocumentAnalyzer, LLMClient, generate_doc_id, get_random_color

logger = logging.getLogger(__name__)

class DocumentChunk:
    """æ–‡æ¡£å—æ•°æ®ç»“æ„"""
    def __init__(self, content: str, metadata: Dict):
        self.content = content
        self.metadata = metadata
        self.chunk_id = str(uuid.uuid4())

class RAGService:
    """RAGæœåŠ¡ç±»"""
    
    def __init__(self):
        self.chroma_client = None
        self.collection = None
        self.embedding_model = None
        self.text_splitter = None
        self.executor = ThreadPoolExecutor(max_workers=4)
        self.db = Database()  # æ•°æ®åº“è¿æ¥
        # æ–‡æ¡£-çŸ¥è¯†åº“å…³è”å…³ç³»ç¼“å­˜ï¼Œä»æ•°æ®åº“åŠ è½½
        self.document_kb_mapping = {}  # doc_id -> [kb_id1, kb_id2, ...]  
        # æ–‡æ¡£åˆ†æç»“æœç¼“å­˜ï¼Œé¿å…é‡å¤åˆ†æ
        self.analysis_cache = {}  # content_hash -> analysis_result
        self._initialize()
    
    def _initialize(self):
        """åˆå§‹åŒ–RAGæœåŠ¡"""
        try:
            # è®¾ç½® tokenizers ç¯å¢ƒå˜é‡ï¼Œé¿å…å¹¶è¡Œå¤„ç†è­¦å‘Š
            os.environ["TOKENIZERS_PARALLELISM"] = "false"
            
            # åˆå§‹åŒ–ChromaDB
            chroma_path = os.path.join(settings.upload_dir, "chroma_db")
            os.makedirs(chroma_path, exist_ok=True)
            
            self.chroma_client = chromadb.PersistentClient(
                path=chroma_path,
                settings=Settings(
                    anonymized_telemetry=False,
                    allow_reset=True
                )
            )
            
            # è·å–æˆ–åˆ›å»ºé›†åˆ
            self.collection = self.chroma_client.get_or_create_collection(
                name="document_chunks",
                metadata={"description": "å­˜å‚¨æ–‡æ¡£åˆ†å—çš„å‘é‡é›†åˆ"}
            )
            
            # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹ (æ”¯æŒä¸­æ–‡)
            model_name = settings.embedding_model
            logger.info(f"æ­£åœ¨åŠ è½½åµŒå…¥æ¨¡å‹: {model_name}")
            
            try:
                self.embedding_model = SentenceTransformer(model_name)
                logger.info(f"åµŒå…¥æ¨¡å‹åŠ è½½æˆåŠŸ: {model_name}")
            except Exception as e:
                logger.error(f"åŠ è½½åµŒå…¥æ¨¡å‹ {model_name} å¤±è´¥: {e}")
                # ä½¿ç”¨ä¸­æ–‡å‹å¥½çš„å¤‡ç”¨æ¨¡å‹
                fallback_model = 'shibing624/text2vec-base-chinese'
                try:
                    self.embedding_model = SentenceTransformer(fallback_model)
                    logger.info(f"å¤‡ç”¨åµŒå…¥æ¨¡å‹åŠ è½½æˆåŠŸ: {fallback_model}")
                except Exception as fallback_error:
                    logger.error(f"å¤‡ç”¨æ¨¡å‹ä¹ŸåŠ è½½å¤±è´¥: {fallback_error}")
                    raise RuntimeError(f"æ— æ³•åŠ è½½ä»»ä½•åµŒå…¥æ¨¡å‹: ä¸»æ¨¡å‹={model_name}, å¤‡ç”¨æ¨¡å‹={fallback_model}")
            
            # åˆå§‹åŒ–æ–‡æœ¬åˆ†å‰²å™¨
            self.text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=settings.rag_chunk_size,
                chunk_overlap=settings.rag_chunk_overlap,
                length_function=len,
                separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", " ", ""]
            )
            
            # å»¶è¿ŸåŠ è½½æ–‡æ¡£-çŸ¥è¯†åº“å…³è”å…³ç³»ï¼ˆåœ¨é¦–æ¬¡éœ€è¦æ—¶åŠ è½½ï¼‰
            self._mapping_loaded = False
            
            logger.info("RAGæœåŠ¡åˆå§‹åŒ–æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"RAGæœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    async def process_document(self, content: str, filename: str, file_type: str) -> str:
        """
        å¤„ç†æ–‡æ¡£ï¼šåˆ†å—ã€å‘é‡åŒ–ã€å­˜å‚¨
        
        Args:
            content: æ–‡æ¡£å†…å®¹
            filename: æ–‡ä»¶å
            file_type: æ–‡ä»¶ç±»å‹
            
        Returns:
            document_id: æ–‡æ¡£å”¯ä¸€æ ‡è¯†
        """
        try:
            # ç”Ÿæˆæ–‡æ¡£ID
            doc_id = generate_doc_id(content, filename)
            
            # æ£€æŸ¥æ–‡æ¡£æ˜¯å¦å·²å­˜åœ¨
            existing_docs = self.collection.get(
                where={"doc_id": doc_id}
            )
            
            if existing_docs['ids']:
                logger.info(f"æ–‡æ¡£å·²å­˜åœ¨ï¼Œè·³è¿‡å¤„ç†: {filename}")
                return doc_id
            
            # æ–‡æ¡£åˆ†å—
            chunks = await self._split_document(content, filename, file_type)
            
            if not chunks:
                logger.warning(f"æ–‡æ¡£åˆ†å—ä¸ºç©º: {filename}")
                return doc_id
            
            # å‘é‡åŒ–å’Œå­˜å‚¨
            await self._vectorize_and_store(chunks, doc_id)
            
            logger.info(f"æ–‡æ¡£å¤„ç†å®Œæˆ: {filename}, åˆ†å—æ•°: {len(chunks)}")
            return doc_id
            
        except Exception as e:
            logger.error(f"æ–‡æ¡£å¤„ç†å¤±è´¥ {filename}: {e}")
            raise
    
    async def search_relevant_chunks(
        self, 
        query: str, 
        doc_ids: Optional[List[str]] = None,
        top_k: int = 5,
        min_similarity: float = 0.355,
        _retry_count: int = 0  # å†…éƒ¨é‡è¯•è®¡æ•°å™¨
    ) -> List[Dict]:
        """
        æ£€ç´¢ç›¸å…³æ–‡æ¡£å—
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            doc_ids: é™å®šæœç´¢çš„æ–‡æ¡£IDåˆ—è¡¨
            top_k: æœ€å¤§è¿”å›æ•°é‡ï¼ˆä¸ä¿è¯è¿”å›è¿™ä¹ˆå¤šï¼Œä¼šæ ¹æ®ç›¸ä¼¼åº¦ç­›é€‰ï¼‰
            min_similarity: æœ€å°ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œåªè¿”å›ç›¸ä¼¼åº¦è¾¾åˆ°æ­¤é˜ˆå€¼çš„ç»“æœ
            _retry_count: å†…éƒ¨é‡è¯•è®¡æ•°å™¨ï¼Œé˜²æ­¢æ— é™é€’å½’
            
        Returns:
            ç›¸å…³æ–‡æ¡£å—åˆ—è¡¨ï¼ˆæŒ‰ç›¸ä¼¼åº¦é™åºæ’åˆ—ï¼Œæ•°é‡å¯èƒ½å°‘äºtop_kï¼‰
        """
        try:
            if not query.strip():
                logger.warning("æŸ¥è¯¢å­—ç¬¦ä¸²ä¸ºç©º")
                return []
            
            # é™åˆ¶æœ€å¤§é‡è¯•æ¬¡æ•°
            max_retries = 2
            if _retry_count > max_retries:
                logger.warning(f"å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° {max_retries}ï¼Œåœæ­¢æœç´¢")
                return []
            
            retry_info = f", é‡è¯•ç¬¬{_retry_count}æ¬¡" if _retry_count > 0 else ""
            logger.info(f"å¼€å§‹RAGæ£€ç´¢{retry_info}: query='{query}', doc_ids={doc_ids}, max_results={top_k}, min_similarity={min_similarity}")
            
            # ç”ŸæˆæŸ¥è¯¢å‘é‡
            query_embedding = await self._generate_embedding(query)
            logger.info(f"æŸ¥è¯¢å‘é‡ç”ŸæˆæˆåŠŸï¼Œç»´åº¦: {query_embedding.shape}")
            
            # æ„å»ºæŸ¥è¯¢æ¡ä»¶
            where_clause = None
            if doc_ids:
                where_clause = {"doc_id": {"$in": doc_ids}}
                logger.info(f"é™å®šæœç´¢æ–‡æ¡£: {doc_ids}")
            
            # æ£€æŸ¥æ•°æ®åº“ä¸­çš„æ–‡æ¡£æ•°é‡
            total_docs = len(self.collection.get()['ids'])
            logger.info(f"æ•°æ®åº“ä¸­å…±æœ‰ {total_docs} ä¸ªæ–‡æ¡£å—")
            
            if total_docs == 0:
                logger.warning("æ•°æ®åº“ä¸­æ²¡æœ‰æ–‡æ¡£å—ï¼Œè¯·å…ˆä¸Šä¼ å¹¶å¤„ç†æ–‡æ¡£")
                return []
            
            # å¢åŠ æ£€ç´¢æ•°é‡ä»¥è·å¾—æ›´å¤šå€™é€‰ç»“æœè¿›è¡Œç­›é€‰
            search_count = min(max(top_k * 2, 20), total_docs)
            
            # å‘é‡æ£€ç´¢
            results = self.collection.query(
                query_embeddings=[query_embedding.tolist()],
                n_results=search_count,
                where=where_clause
            )
            
            logger.info(f"ChromaDBè¿”å› {len(results['ids'][0]) if results['ids'][0] else 0} ä¸ªå€™é€‰ç»“æœ")
            
            # å¤„ç†ç»“æœ
            relevant_chunks = []
            
            if results['ids'][0]:  # å¦‚æœæœ‰ç»“æœ
                logger.info("å¤„ç†æ£€ç´¢ç»“æœ...")
                for i, chunk_id in enumerate(results['ids'][0]):
                    distance = results['distances'][0][i]
                    
                    # ä¿®å¤ç›¸ä¼¼åº¦è®¡ç®—ï¼šæ­£ç¡®å¤„ç†ChromaDBçš„L2è·ç¦»
                    # ChromaDBä½¿ç”¨å¹³æ–¹L2è·ç¦»ï¼Œå¯¹äºå½’ä¸€åŒ–å‘é‡çš„æ­£ç¡®è½¬æ¢å¦‚ä¸‹ï¼š
                    if distance >= 0:
                        similarity = max(0.0, 1.0 - distance)
                    else:
                        similarity = 1.0  # è·ç¦»ä¸ºè´Ÿæ•°æ—¶è®¾ä¸ºæœ€é«˜ç›¸ä¼¼åº¦
                    
                    # ç¡®ä¿ç›¸ä¼¼åº¦åœ¨åˆç†èŒƒå›´å†…
                    similarity = max(0.0, min(1.0, similarity))
                    
                    logger.debug(f"å€™é€‰ç»“æœ {i+1}: L2è·ç¦»={distance:.4f}, è®¡ç®—ç›¸ä¼¼åº¦={similarity:.4f}")
                    
                    # åªä¿ç•™è¾¾åˆ°ç›¸ä¼¼åº¦é˜ˆå€¼çš„ç»“æœ
                    if similarity >= min_similarity:
                        relevant_chunks.append({
                            'chunk_id': chunk_id,
                            'content': results['documents'][0][i],
                            'metadata': results['metadatas'][0][i],
                            'similarity': similarity
                        })
                        logger.debug(f"ç»“æœ {i+1} é€šè¿‡ç›¸ä¼¼åº¦é˜ˆå€¼")
                    else:
                        logger.debug(f"ç»“æœ {i+1} æœªé€šè¿‡ç›¸ä¼¼åº¦é˜ˆå€¼ ({similarity:.4f} < {min_similarity})")
                
                # æŒ‰ç›¸ä¼¼åº¦é™åºæ’åº
                relevant_chunks.sort(key=lambda x: x['similarity'], reverse=True)
                
                # åªè¿”å›top_kä¸ªæœ€ç›¸å…³çš„ç»“æœ
                if len(relevant_chunks) > top_k:
                    logger.info(f"ç­›é€‰å‡º {len(relevant_chunks)} ä¸ªç›¸å…³ç»“æœï¼Œä¿ç•™å‰ {top_k} ä¸ªæœ€ç›¸å…³çš„")
                    relevant_chunks = relevant_chunks[:top_k]
                
            else:
                logger.warning("ChromaDBæ²¡æœ‰è¿”å›ä»»ä½•ç»“æœ")
            
            logger.info(f"æœ€ç»ˆæ£€ç´¢åˆ° {len(relevant_chunks)} ä¸ªç›¸å…³æ–‡æ¡£å—")
            
            # æ™ºèƒ½é‡è¯•é€»è¾‘ï¼šåªåœ¨ç‰¹å®šæ¡ä»¶ä¸‹é‡è¯•ï¼Œä¸”æé«˜é˜ˆå€¼è¦æ±‚
            if not relevant_chunks and _retry_count < max_retries:
                # è·å–æœ€é«˜ç›¸ä¼¼åº¦ä»¥åˆ¤æ–­æ˜¯å¦éœ€è¦é‡è¯•
                if results['ids'][0]:
                    highest_similarity = max([
                        max(0.0, 1.0 - results['distances'][0][i])  # ä¿®æ­£ç›¸ä¼¼åº¦è®¡ç®—
                        for i in range(len(results['distances'][0]))
                    ])
                    
                    # åªæœ‰åœ¨æœ€é«˜ç›¸ä¼¼åº¦æ¥è¿‘é˜ˆå€¼æ—¶æ‰é‡è¯•ï¼Œä¸”ä¸é™å¾—å¤ªä½
                    if highest_similarity >= 0.25 and min_similarity > 0.5:
                        retry_threshold = max(0.5, min_similarity - 0.1)  # è°ƒæ•´é‡è¯•é˜ˆå€¼ä¸‹é™åˆ°0.5
                        logger.info(f"æœ€é«˜ç›¸ä¼¼åº¦ {highest_similarity:.3f} æ¥è¿‘é˜ˆå€¼ï¼Œå°è¯•é™ä½ç›¸ä¼¼åº¦é˜ˆå€¼ä» {min_similarity} åˆ° {retry_threshold}")
                        return await self.search_relevant_chunks(
                            query=query,
                            doc_ids=doc_ids,
                            top_k=top_k,
                            min_similarity=retry_threshold,
                            _retry_count=_retry_count + 1
                        )
                    elif highest_similarity < 0.25:
                        logger.info(f"æœ€é«˜ç›¸ä¼¼åº¦ {highest_similarity:.3f} è¿‡ä½ï¼Œç›´æ¥è¿”å›ç©ºç»“æœ")
                        return []
            
            return relevant_chunks
            
        except Exception as e:
            logger.error(f"æ–‡æ¡£æ£€ç´¢å¤±è´¥: {e}")
            return []
    
    async def get_document_info(self, doc_id: str) -> Optional[Dict]:
        """è·å–æ–‡æ¡£ä¿¡æ¯"""
        try:
            results = self.collection.get(
                where={"doc_id": doc_id},
                limit=1
            )
            
            if results['ids']:
                metadata = results['metadatas'][0]
                return {
                    'doc_id': doc_id,
                    'filename': metadata.get('filename'),
                    'file_type': metadata.get('file_type'),
                    'chunk_count': len(results['ids']),
                    'created_at': metadata.get('created_at')
                }
            
            return None
            
        except Exception as e:
            logger.error(f"è·å–æ–‡æ¡£ä¿¡æ¯å¤±è´¥ {doc_id}: {e}")
            return None
    
    async def get_all_documents(self) -> List[Dict]:
        """è·å–æ‰€æœ‰æ–‡æ¡£åˆ—è¡¨"""
        try:
            # è·å–æ‰€æœ‰æ–‡æ¡£å—
            results = self.collection.get()
            
            # æŒ‰doc_idåˆ†ç»„ç»Ÿè®¡
            doc_stats = {}
            
            if results['ids']:
                for i, chunk_id in enumerate(results['ids']):
                    metadata = results['metadatas'][i]
                    doc_id = metadata.get('doc_id')
                    
                    if doc_id not in doc_stats:
                        doc_stats[doc_id] = {
                            'doc_id': doc_id,
                            'filename': metadata.get('filename', 'Unknown'),
                            'file_type': metadata.get('file_type', 'Unknown'),
                            'created_at': metadata.get('created_at', ''),
                            'chunk_count': 0,
                            'total_length': 0
                        }
                    
                    doc_stats[doc_id]['chunk_count'] += 1
                    doc_stats[doc_id]['total_length'] += metadata.get('chunk_length', 0)
            
            # è½¬æ¢ä¸ºåˆ—è¡¨å¹¶æŒ‰åˆ›å»ºæ—¶é—´æ’åº
            documents = list(doc_stats.values())
            documents.sort(key=lambda x: x['created_at'], reverse=True)
            
            logger.info(f"è·å–åˆ° {len(documents)} ä¸ªæ–‡æ¡£")
            return documents
            
        except Exception as e:
            logger.error(f"è·å–æ–‡æ¡£åˆ—è¡¨å¤±è´¥: {e}")
            return []

    async def get_document_chunks(self, doc_id: str) -> List[Dict]:
        """è·å–æŒ‡å®šæ–‡æ¡£çš„æ‰€æœ‰åˆ†å—"""
        try:
            # è·å–æ‰€æœ‰ç›¸å…³åˆ†å—
            results = self.collection.get(
                where={"doc_id": doc_id}
            )
            
            chunks = []
            if results['ids']:
                for i, chunk_id in enumerate(results['ids']):
                    chunks.append({
                        'chunk_id': chunk_id,
                        'content': results['documents'][i],
                        'metadata': results['metadatas'][i],
                        'similarity': 1.0  # å®Œæ•´æ–‡æ¡£æ—¶ç›¸ä¼¼åº¦è®¾ä¸º1.0
                    })
                
                # æŒ‰chunk_indexæ’åº
                chunks.sort(key=lambda x: x['metadata'].get('chunk_index', 0))
            
            logger.info(f"è·å–æ–‡æ¡£ {doc_id} çš„ {len(chunks)} ä¸ªåˆ†å—")
            return chunks
            
        except Exception as e:
            logger.error(f"è·å–æ–‡æ¡£åˆ†å—å¤±è´¥ {doc_id}: {e}")
            return []

    async def delete_document(self, doc_id: str) -> bool:
        """åˆ é™¤æ–‡æ¡£åŠå…¶æ‰€æœ‰åˆ†å—"""
        try:
            # è·å–æ‰€æœ‰ç›¸å…³åˆ†å—
            results = self.collection.get(
                where={"doc_id": doc_id}
            )
            
            if results['ids']:
                # åˆ é™¤åˆ†å—
                self.collection.delete(ids=results['ids'])
                logger.info(f"åˆ é™¤æ–‡æ¡£æˆåŠŸ: {doc_id}, åˆ é™¤åˆ†å—æ•°: {len(results['ids'])}")
                return True
            
            return False
            
        except Exception as e:
            logger.error(f"åˆ é™¤æ–‡æ¡£å¤±è´¥ {doc_id}: {e}")
            return False
    
    async def _split_document(self, content: str, filename: str, file_type: str) -> List[DocumentChunk]:
        """åˆ†å‰²æ–‡æ¡£"""
        try:
            # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œæ–‡æœ¬åˆ†å‰²ï¼ˆCPUå¯†é›†å‹æ“ä½œï¼‰
            loop = asyncio.get_event_loop()
            chunks_text = await loop.run_in_executor(
                self.executor,
                self.text_splitter.split_text,
                content
            )
            
            chunks = []
            for i, chunk_text in enumerate(chunks_text):
                if chunk_text.strip():  # è·³è¿‡ç©ºåˆ†å—
                    metadata = {
                        'filename': filename,
                        'file_type': file_type,
                        'chunk_index': i,
                        'chunk_length': len(chunk_text),
                        'created_at': datetime.now().isoformat()
                    }
                    chunks.append(DocumentChunk(chunk_text, metadata))
            
            return chunks
            
        except Exception as e:
            logger.error(f"æ–‡æ¡£åˆ†å—å¤±è´¥: {e}")
            return []
    
    async def _vectorize_and_store(self, chunks: List[DocumentChunk], doc_id: str):
        """å‘é‡åŒ–å¹¶å­˜å‚¨æ–‡æ¡£å— - é’ˆå¯¹PDFå’ŒOCRæ–‡æ¡£ä¼˜åŒ–"""
        try:
            if not chunks:
                return
            # å¯¹chunksè¿›è¡Œè´¨é‡ä¼˜åŒ–å¤„ç†
            processed_chunks = await self._optimize_chunks_for_indexing(chunks)
            
            if not processed_chunks:
                logger.warning(f"æ–‡æ¡£å—è´¨é‡ä¼˜åŒ–åä¸ºç©º: {doc_id}")
                return
            
            # å‡†å¤‡æ•°æ®
            chunk_texts = [chunk.content for chunk in processed_chunks]
            chunk_ids = [chunk.chunk_id for chunk in processed_chunks]
            metadatas = []
            
            for chunk in processed_chunks:
                metadata = chunk.metadata.copy()
                metadata['doc_id'] = doc_id
                metadatas.append(metadata)
            
            # æ ¹æ®æ–‡æ¡£ç±»å‹é€‰æ‹©æœ€ä½³åµŒå…¥ç­–ç•¥
            embeddings = await self._generate_optimized_embeddings(chunk_texts, metadatas)
            
            # å­˜å‚¨åˆ°ChromaDB
            self.collection.add(
                ids=chunk_ids,
                embeddings=embeddings.tolist(),
                documents=chunk_texts,
                metadatas=metadatas
            )
            
            logger.info(f"æˆåŠŸå­˜å‚¨ {len(processed_chunks)} ä¸ªä¼˜åŒ–åçš„æ–‡æ¡£å—")
            
        except Exception as e:
            logger.error(f"å‘é‡åŒ–å­˜å‚¨å¤±è´¥: {e}")
            raise

    async def _optimize_chunks_for_indexing(self, chunks: List[DocumentChunk]) -> List[DocumentChunk]:
        """ä¼˜åŒ–æ–‡æ¡£å—ä»¥æé«˜ç´¢å¼•è´¨é‡"""
        optimized_chunks = []
        
        for chunk in chunks:
            try:
                # æ£€æµ‹æ–‡æ¡£ç±»å‹å’Œè´¨é‡
                doc_type = chunk.metadata.get('file_type', '').lower()
                is_pdf = chunk.metadata.get('is_pdf', False)
                is_text_pdf = chunk.metadata.get('is_text_pdf', False)
                is_ocr = chunk.metadata.get('is_ocr_processed', False)
                
                # åŸå§‹å†…å®¹
                original_content = chunk.content
                
                # åº”ç”¨åˆ†å±‚ä¼˜åŒ–ç­–ç•¥
                if is_ocr or (is_pdf and not is_text_pdf):
                    # OCRæ‰«æä»¶ï¼šéœ€è¦æ›´ä¸¥æ ¼çš„å¤„ç†
                    processed_content = await TextProcessor.process_ocr_content(original_content)
                elif is_pdf and is_text_pdf:
                    # æ–‡æœ¬PDFï¼šä¸­ç­‰å¤„ç†
                    processed_content = await TextProcessor.process_text_pdf_content(original_content)
                else:
                    # æ™®é€šæ–‡æ¡£ï¼šåŸºç¡€å¤„ç†
                    processed_content = await TextProcessor.process_standard_content(original_content)
                
                # è´¨é‡æ£€æŸ¥
                quality_score = TextProcessor.calculate_content_quality(processed_content)
                
                # åªä¿ç•™è´¨é‡è¾¾æ ‡çš„åˆ†å—
                if quality_score >= 0.3:  # è´¨é‡é˜ˆå€¼
                    # æ›´æ–°åˆ†å—å†…å®¹å’Œå…ƒæ•°æ®
                    optimized_chunk = DocumentChunk(processed_content, chunk.metadata.copy())
                    optimized_chunk.chunk_id = chunk.chunk_id
                    
                    # å¢å¼ºå…ƒæ•°æ®
                    optimized_chunk.metadata.update({
                        'quality_score': quality_score,
                        'content_length_original': len(original_content),
                        'content_length_processed': len(processed_content),
                        'processing_applied': TextProcessor.get_processing_type(is_ocr, is_pdf, is_text_pdf),
                        'optimization_version': '2.0'
                    })
                    
                    optimized_chunks.append(optimized_chunk)
                else:
                    logger.debug(f"è·³è¿‡ä½è´¨é‡åˆ†å—: quality_score={quality_score:.3f}")
                    
            except Exception as e:
                logger.warning(f"åˆ†å—ä¼˜åŒ–å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹å†…å®¹: {e}")
                optimized_chunks.append(chunk)
        
        logger.info(f"åˆ†å—ä¼˜åŒ–å®Œæˆ: {len(chunks)} -> {len(optimized_chunks)}")
        return optimized_chunks

    async def _generate_optimized_embeddings(self, texts: List[str], metadatas: List[Dict]) -> np.ndarray:
        """æ ¹æ®æ–‡æ¡£ç±»å‹ç”Ÿæˆä¼˜åŒ–çš„åµŒå…¥å‘é‡"""
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰ç‰¹æ®Šç±»å‹çš„æ–‡æ¡£éœ€è¦å¢å¼ºå¤„ç†
            has_ocr_docs = any(meta.get('processing_applied') == 'ocr_enhanced' for meta in metadatas)
            
            if has_ocr_docs:
                # å¯¹OCRæ–‡æ¡£ä½¿ç”¨æ›´robustçš„åµŒå…¥è®¾ç½®
                loop = asyncio.get_event_loop()
                embeddings = await loop.run_in_executor(
                    self.executor,
                    lambda: self.embedding_model.encode(
                        texts, 
                        normalize_embeddings=True,
                        batch_size=16,  # å‡å°æ‰¹æ¬¡å¤§å°æé«˜ç¨³å®šæ€§
                        show_progress_bar=False
                    )
                )
            else:
                # æ ‡å‡†åµŒå…¥å¤„ç†
                embeddings = await self._generate_embeddings_batch(texts)
            
            # ç¡®ä¿å‘é‡è´¨é‡
            norms = np.linalg.norm(embeddings, axis=1, keepdims=True)
            norms[norms == 0] = 1  # é¿å…é™¤é›¶
            embeddings = embeddings / norms
            
            return embeddings
            
        except Exception as e:
            logger.error(f"ä¼˜åŒ–åµŒå…¥ç”Ÿæˆå¤±è´¥ï¼Œå›é€€åˆ°æ ‡å‡†æ–¹æ³•: {e}")
            return await self._generate_embeddings_batch(texts)
    
    async def _generate_embedding(self, text: str) -> np.ndarray:
        """ç”Ÿæˆå•ä¸ªæ–‡æœ¬çš„åµŒå…¥å‘é‡"""
        loop = asyncio.get_event_loop()
        embedding = await loop.run_in_executor(
            self.executor,
            lambda: self.embedding_model.encode(text, normalize_embeddings=True)
        )
        # ç¡®ä¿å‘é‡è¢«å½’ä¸€åŒ–
        norm = np.linalg.norm(embedding)
        if norm > 0:
            embedding = embedding / norm
        return embedding
    
    async def _generate_embeddings_batch(self, texts: List[str]) -> np.ndarray:
        """æ‰¹é‡ç”ŸæˆåµŒå…¥å‘é‡"""
        loop = asyncio.get_event_loop()
        embeddings = await loop.run_in_executor(
            self.executor,
            lambda: self.embedding_model.encode(texts, normalize_embeddings=True)
        )
        # ç¡®ä¿æ‰€æœ‰å‘é‡è¢«å½’ä¸€åŒ–
        norms = np.linalg.norm(embeddings, axis=1, keepdims=True)
        norms[norms == 0] = 1  # é¿å…é™¤é›¶
        embeddings = embeddings / norms
        return embeddings
    
    def _generate_doc_id(self, content: str, filename: str) -> str:
        """ç”Ÿæˆæ–‡æ¡£å”¯ä¸€æ ‡è¯† """
        return generate_doc_id(content, filename)
    
    async def _load_document_kb_mapping_from_db(self):
        """ä»æ•°æ®åº“åŠ è½½æ–‡æ¡£-çŸ¥è¯†åº“å…³è”å…³ç³»"""
        try:
            if self._mapping_loaded:
                return
                
            self.document_kb_mapping.clear()
            
            async with self.db.get_connection() as db:
                cursor = await db.execute("""
                    SELECT doc_id, knowledge_base_id 
                    FROM knowledge_base_documents
                    ORDER BY doc_id, added_at
                """)
                
                rows = await cursor.fetchall()
                
                for row in rows:
                    doc_id, kb_id = row
                    
                    if doc_id not in self.document_kb_mapping:
                        self.document_kb_mapping[doc_id] = []
                    
                    if kb_id not in self.document_kb_mapping[doc_id]:
                        self.document_kb_mapping[doc_id].append(kb_id)
                
                self._mapping_loaded = True
                logger.info(f"ä»æ•°æ®åº“åŠ è½½äº† {len(self.document_kb_mapping)} ä¸ªæ–‡æ¡£çš„çŸ¥è¯†åº“å…³è”å…³ç³»")
                
        except Exception as e:
            logger.error(f"ä»æ•°æ®åº“åŠ è½½æ–‡æ¡£-çŸ¥è¯†åº“å…³è”å…³ç³»å¤±è´¥: {e}")
            # ç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªç©ºçš„æ˜ å°„ï¼Œé¿å…åç»­æ“ä½œå¤±è´¥
            self.document_kb_mapping = {}
            self._mapping_loaded = True
    
    async def _ensure_mapping_loaded(self):
        """ç¡®ä¿æ˜ å°„å…³ç³»å·²åŠ è½½"""
        if not self._mapping_loaded:
            await self._load_document_kb_mapping_from_db()
    
    # çŸ¥è¯†åº“ç®¡ç†æ–¹æ³• - ä½¿ç”¨æ•°æ®åº“å­˜å‚¨ï¼ˆä¸ç»‘å®šç”¨æˆ·ï¼‰
    async def create_knowledge_base(self, name: str, description: str = None, color: str = "#3B82F6") -> Dict:
        """åˆ›å»ºçŸ¥è¯†åº“"""
        kb_id = str(uuid.uuid4())
        now = datetime.now()
        
        try:
            async with self.db.get_connection() as db:
                # ä½¿ç”¨é»˜è®¤ç”¨æˆ·IDï¼ˆ0è¡¨ç¤ºå…¨å±€ï¼‰
                await db.execute("""
                    INSERT INTO knowledge_bases (id, name, description, color, document_count, created_at, updated_at)
                    VALUES (?, ?, ?, ?, ?, ?, ?)
                """, (kb_id, name, description, color, 0, now, now))
                await db.commit()
            
            knowledge_base = {
                "id": kb_id,
                "name": name,
                "description": description,
                "color": color,
                "document_count": 0,
                "created_at": now,
                "updated_at": now
            }
            
            logger.info(f"åˆ›å»ºçŸ¥è¯†åº“æˆåŠŸ: {name} (ID: {kb_id})")
            return knowledge_base
            
        except Exception as e:
            logger.error(f"åˆ›å»ºçŸ¥è¯†åº“å¤±è´¥: {e}")
            raise
        
    async def get_all_knowledge_bases(self) -> List[Dict]:
        """è·å–æ‰€æœ‰çŸ¥è¯†åº“"""
        try:
            async with self.db.get_connection() as db:
                cursor = await db.execute("""
                    SELECT id, name, description, color, document_count, created_at, updated_at
                    FROM knowledge_bases
                    ORDER BY created_at DESC
                """)
                
                rows = await cursor.fetchall()
                
                knowledge_bases = []
                for row in rows:
                    kb = {
                        "id": row[0],
                        "name": row[1],
                        "description": row[2],
                        "color": row[3],
                        "document_count": row[4],
                        "created_at": row[5],
                        "updated_at": row[6]
                    }
                    knowledge_bases.append(kb)
                
                return knowledge_bases
                
        except Exception as e:
            logger.error(f"è·å–çŸ¥è¯†åº“åˆ—è¡¨å¤±è´¥: {e}")
            return []
        
    async def get_knowledge_base(self, kb_id: str) -> Optional[Dict]:
        """è·å–å•ä¸ªçŸ¥è¯†åº“"""
        try:
            async with self.db.get_connection() as db:
                cursor = await db.execute("""
                    SELECT id, name, description, color, document_count, created_at, updated_at
                    FROM knowledge_bases
                    WHERE id = ? 
                """, (kb_id,))
                row = await cursor.fetchone()
                
                if row:
                    return {
                        "id": row[0],
                        "name": row[1],
                        "description": row[2],
                        "color": row[3],
                        "document_count": row[4],
                        "created_at": row[5],
                        "updated_at": row[6]
                    }
                return None
                
        except Exception as e:
            logger.error(f"è·å–çŸ¥è¯†åº“å¤±è´¥: {e}")
            return None
        
    async def update_knowledge_base(self, kb_id: str, name: str = None, description: str = None, color: str = None) -> bool:
        """æ›´æ–°çŸ¥è¯†åº“"""
        try:
            # æ„å»ºæ›´æ–°å­—æ®µ
            update_fields = []
            update_values = []
            
            if name is not None:
                update_fields.append("name = ?")
                update_values.append(name)
            if description is not None:
                update_fields.append("description = ?")
                update_values.append(description)
            if color is not None:
                update_fields.append("color = ?")
                update_values.append(color)
            
            if not update_fields:
                return True  # æ²¡æœ‰éœ€è¦æ›´æ–°çš„å­—æ®µ
            
            update_fields.append("updated_at = ?")
            update_values.append(datetime.now())
            update_values.append(kb_id)
            
            async with self.db.get_connection() as db:
                cursor = await db.execute(f"""
                    UPDATE knowledge_bases 
                    SET {', '.join(update_fields)}
                    WHERE id = ?
                """, update_values)
                
                await db.commit()
                
                if cursor.rowcount > 0:
                    logger.info(f"æ›´æ–°çŸ¥è¯†åº“æˆåŠŸ: ID={kb_id}")
                    return True
                else:
                    logger.warning(f"çŸ¥è¯†åº“ä¸å­˜åœ¨: ID={kb_id}")
                    return False
                    
        except Exception as e:
            logger.error(f"æ›´æ–°çŸ¥è¯†åº“å¤±è´¥: {e}")
            return False
        
    async def delete_knowledge_base(self, kb_id: str) -> bool:
        """åˆ é™¤çŸ¥è¯†åº“"""
        try:
            async with self.db.get_connection() as db:
                # é¦–å…ˆè·å–çŸ¥è¯†åº“åç§°ç”¨äºæ—¥å¿—
                cursor = await db.execute("""
                    SELECT name FROM knowledge_bases 
                    WHERE id = ?
                """, (kb_id,))
                
                row = await cursor.fetchone()
                kb_name = row[0] if row else "æœªçŸ¥"
                
                # åˆ é™¤çŸ¥è¯†åº“
                cursor = await db.execute("""
                    DELETE FROM knowledge_bases 
                    WHERE id = ?
                """, (kb_id,))
                
                await db.commit()
                
                if cursor.rowcount > 0:
                    # ä»æ•°æ®åº“åˆ é™¤å…³è”å…³ç³»
                    await db.execute("""
                        DELETE FROM knowledge_base_documents 
                        WHERE knowledge_base_id = ?
                    """, (kb_id,))
                    
                    # æ¸…ç†å†…å­˜ä¸­çš„æ–‡æ¡£å…³è”å…³ç³»
                    if hasattr(self, '_mapping_loaded') and self._mapping_loaded:
                        for doc_id in list(self.document_kb_mapping.keys()):
                            if kb_id in self.document_kb_mapping[doc_id]:
                                self.document_kb_mapping[doc_id].remove(kb_id)
                                if not self.document_kb_mapping[doc_id]:
                                    del self.document_kb_mapping[doc_id]
                    
                    logger.info(f"åˆ é™¤çŸ¥è¯†åº“æˆåŠŸ: {kb_name} (ID: {kb_id})")
                    return True
                else:
                    logger.warning(f"çŸ¥è¯†åº“ä¸å­˜åœ¨: ID={kb_id}")
                    return False
                    
        except Exception as e:
            logger.error(f"åˆ é™¤çŸ¥è¯†åº“å¤±è´¥: {e}")
            return False
        
    async def add_documents_to_knowledge_base(self, kb_id: str, doc_ids: List[str]) -> bool:
        """å°†æ–‡æ¡£æ·»åŠ åˆ°çŸ¥è¯†åº“"""
        try:
            # ç¡®ä¿æ˜ å°„å…³ç³»å·²åŠ è½½
            await self._ensure_mapping_loaded()
            
            # éªŒè¯çŸ¥è¯†åº“æ˜¯å¦å­˜åœ¨
            kb = await self.get_knowledge_base(kb_id)
            if not kb:
                logger.warning(f"çŸ¥è¯†åº“ä¸å­˜åœ¨: ID={kb_id}")
                return False
            
            # æ£€æŸ¥æ–‡æ¡£æ˜¯å¦å­˜åœ¨
            existing_docs = await self.get_all_documents()
            existing_doc_ids = {doc["doc_id"] for doc in existing_docs}
            
            added_count = 0
            async with self.db.get_connection() as db:
                for doc_id in doc_ids:
                    if doc_id in existing_doc_ids:
                        # æ£€æŸ¥æ˜¯å¦å·²ç»å­˜åœ¨å…³è”å…³ç³»
                        if doc_id not in self.document_kb_mapping:
                            self.document_kb_mapping[doc_id] = []
                        
                        if kb_id not in self.document_kb_mapping[doc_id]:
                            try:
                                # æ’å…¥åˆ°æ•°æ®åº“
                                await db.execute("""
                                    INSERT INTO knowledge_base_documents (knowledge_base_id, doc_id, added_at)
                                    VALUES (?, ?, ?)
                                """, (kb_id, doc_id, datetime.now()))
                                
                                # æ›´æ–°å†…å­˜ç¼“å­˜
                                self.document_kb_mapping[doc_id].append(kb_id)
                                added_count += 1
                                
                            except Exception as e:
                                # å¯èƒ½æ˜¯é‡å¤æ’å…¥ï¼Œå¿½ç•¥
                                if "UNIQUE constraint failed" not in str(e):
                                    logger.warning(f"æ’å…¥å…³è”å…³ç³»å¤±è´¥ {doc_id}->{kb_id}: {e}")
                
                # æ›´æ–°çŸ¥è¯†åº“çš„æ–‡æ¡£è®¡æ•°
                if added_count > 0:
                    await db.execute("""
                        UPDATE knowledge_bases 
                        SET document_count = document_count + ?, updated_at = ?
                        WHERE id = ?
                    """, (added_count, datetime.now(), kb_id))
                
                await db.commit()
            
            kb_name = kb["name"]
            logger.info(f"å‘çŸ¥è¯†åº“ '{kb_name}' æ·»åŠ äº† {added_count} ä¸ªæ–‡æ¡£")
            return True
            
        except Exception as e:
            logger.error(f"æ·»åŠ æ–‡æ¡£åˆ°çŸ¥è¯†åº“å¤±è´¥: {e}")
            return False
        
    async def remove_documents_from_knowledge_base(self, kb_id: str, doc_ids: List[str]) -> bool:
        """ä»çŸ¥è¯†åº“ä¸­ç§»é™¤æ–‡æ¡£"""
        try:
            # ç¡®ä¿æ˜ å°„å…³ç³»å·²åŠ è½½
            await self._ensure_mapping_loaded()
            
            # éªŒè¯çŸ¥è¯†åº“æ˜¯å¦å­˜åœ¨
            kb = await self.get_knowledge_base(kb_id)
            if not kb:
                logger.warning(f"çŸ¥è¯†åº“ä¸å­˜åœ¨: ID={kb_id}")
                return False
            
            removed_count = 0
            async with self.db.get_connection() as db:
                for doc_id in doc_ids:
                    if doc_id in self.document_kb_mapping and kb_id in self.document_kb_mapping[doc_id]:
                        try:
                            # ä»æ•°æ®åº“åˆ é™¤å…³è”å…³ç³»
                            cursor = await db.execute("""
                                DELETE FROM knowledge_base_documents 
                                WHERE knowledge_base_id = ? AND doc_id = ?
                            """, (kb_id, doc_id))
                            
                            if cursor.rowcount > 0:
                                # æ›´æ–°å†…å­˜ç¼“å­˜
                                self.document_kb_mapping[doc_id].remove(kb_id)
                                if not self.document_kb_mapping[doc_id]:
                                    del self.document_kb_mapping[doc_id]
                                removed_count += 1
                                
                        except Exception as e:
                            logger.warning(f"åˆ é™¤å…³è”å…³ç³»å¤±è´¥ {doc_id}->{kb_id}: {e}")
                
                # æ›´æ–°çŸ¥è¯†åº“çš„æ–‡æ¡£è®¡æ•°
                if removed_count > 0:
                    await db.execute("""
                        UPDATE knowledge_bases 
                        SET document_count = GREATEST(0, document_count - ?), updated_at = ?
                        WHERE id = ?
                    """, (removed_count, datetime.now(), kb_id))
                
                await db.commit()
            
            kb_name = kb["name"]
            logger.info(f"ä»çŸ¥è¯†åº“ '{kb_name}' ç§»é™¤äº† {removed_count} ä¸ªæ–‡æ¡£")
            return True
            
        except Exception as e:
            logger.error(f"ä»çŸ¥è¯†åº“ç§»é™¤æ–‡æ¡£å¤±è´¥: {e}")
            return False
        
    async def get_knowledge_base_documents(self, kb_id: str) -> List[str]:
        """è·å–çŸ¥è¯†åº“çš„æ‰€æœ‰æ–‡æ¡£ID"""
        try:
            # ç¡®ä¿æ˜ å°„å…³ç³»å·²åŠ è½½
            await self._ensure_mapping_loaded()
            
            # éªŒè¯çŸ¥è¯†åº“æ˜¯å¦å­˜åœ¨
            kb = await self.get_knowledge_base(kb_id)
            if not kb:
                logger.warning(f"çŸ¥è¯†åº“ä¸å­˜åœ¨: ID={kb_id}")
                return []
            
            # ä»å†…å­˜ç¼“å­˜è·å–ï¼ˆå·²ä»æ•°æ®åº“åŠ è½½ï¼‰
            return [doc_id for doc_id, kb_ids in self.document_kb_mapping.items() if kb_id in kb_ids]
            
        except Exception as e:
            logger.error(f"è·å–çŸ¥è¯†åº“æ–‡æ¡£å¤±è´¥: {e}")
            return []

    async def analyze_document_for_archive(
        self, 
        file_content: bytes, 
        filename: str, 
        file_type: str,
        analysis_prompt: str,
        custom_analysis: bool = False
    ) -> Dict:
        """
        åˆ†ææ–‡æ¡£å†…å®¹å¹¶ä¿å­˜åˆ°å‘é‡æ•°æ®åº“ï¼Œé¢„è§ˆå½’æ¡£å»ºè®®
        
        Args:
            file_content: æ–‡ä»¶å†…å®¹ï¼ˆå­—èŠ‚ï¼‰
            filename: æ–‡ä»¶å
            file_type: æ–‡ä»¶ç±»å‹
            analysis_prompt: åˆ†ææç¤ºè¯
            custom_analysis: æ˜¯å¦ä½¿ç”¨è‡ªå®šä¹‰åˆ†æ
            
        Returns:
            åˆ†æç»“æœä¿¡æ¯ï¼ˆåŒ…å«å®é™…çš„docIdï¼‰
        """
        try:
            # æå–æ–‡æ¡£å†…å®¹
            text_content = await self._extract_text_from_file(file_content, filename, file_type)
            
            # ğŸš€ ä¼˜åŒ–ï¼šåœ¨åˆ†æé˜¶æ®µå°±ä¿å­˜æ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“
            doc_id = await self.process_document(text_content, filename, file_type)
            logger.info(f"æ–‡æ¡£å·²ä¿å­˜åˆ°å‘é‡æ•°æ®åº“: {filename} (doc_id: {doc_id})")
            
            # ä½¿ç”¨AIåˆ†ææ–‡æ¡£å†…å®¹å¹¶åŒ¹é…çŸ¥è¯†åº“
            analysis_result = await self._analyze_document_content(
                content=text_content,
                filename=filename,
                analysis_prompt=analysis_prompt,
                custom_analysis=custom_analysis
            )
            
            knowledge_base_name = analysis_result['knowledge_base_name']
            is_new_kb = analysis_result['is_new_knowledge_base']
            reason = analysis_result.get('reason', '')
            
            # å¦‚æœä¸æ˜¯æ–°å»ºçŸ¥è¯†åº“ï¼Œæ£€æŸ¥ç°æœ‰çŸ¥è¯†åº“æ˜¯å¦å­˜åœ¨
            kb_id = None
            if not is_new_kb:
                all_kbs = await self.get_all_knowledge_bases()
                for kb in all_kbs:
                    if kb['name'] == knowledge_base_name:
                        kb_id = kb['id']
                        break
                
                # å¦‚æœæ²¡æ‰¾åˆ°åŒ¹é…çš„çŸ¥è¯†åº“ï¼Œæ ‡è®°ä¸ºæ–°å»º
                if not kb_id:
                    is_new_kb = True
            
            result = {
                "fileName": filename,
                "knowledgeBaseName": knowledge_base_name,
                "isNewKnowledgeBase": is_new_kb,
                "reason": reason,
                "knowledgeBaseId": kb_id,
                "documentType": analysis_result.get('document_type', 'æœªçŸ¥'),
                "textContent": text_content[:500] + "..." if len(text_content) > 500 else text_content,  # é¢„è§ˆå†…å®¹
                "docId": doc_id,  # âœ¨ æ–°å¢ï¼šè¿”å›æ–‡æ¡£ID
                "analysisTime": datetime.now().timestamp()
            }
            
            logger.info(f"æ–‡æ¡£åˆ†æå’Œä¿å­˜å®Œæˆ: {filename} -> {knowledge_base_name} ({'æ–°å»º' if is_new_kb else 'ç°æœ‰'}), doc_id: {doc_id}")
            return result
            
        except Exception as e:
            logger.error(f"æ–‡æ¡£åˆ†æå¤±è´¥ {filename}: {e}")
            raise

    async def confirm_archive_document(
        self,
        file_content: str,  # å·²å¼ƒç”¨ï¼Œä¿ç•™ç”¨äºå…¼å®¹æ€§
        filename: str,
        file_type: str,
        analysis_result: Dict
    ) -> Dict:
        """
        ç¡®è®¤å½’æ¡£æ–‡æ¡£ï¼Œä»…æ‰§è¡Œæ–‡æ¡£ä¸çŸ¥è¯†åº“çš„å…³è”æ“ä½œ
        ï¼ˆæ–‡æ¡£åœ¨analyzeé˜¶æ®µå·²ä¿å­˜åˆ°å‘é‡æ•°æ®åº“ï¼‰
        
        Args:
            file_content: å·²å¼ƒç”¨ï¼Œæ–‡æ¡£å·²åœ¨åˆ†æé˜¶æ®µä¿å­˜
            filename: æ–‡ä»¶å
            file_type: æ–‡ä»¶ç±»å‹  
            analysis_result: åˆ†æç»“æœï¼ˆåŒ…å«doc_idï¼‰
            
        Returns:
            å½’æ¡£ç»“æœä¿¡æ¯
        """
        try:
            # ğŸš€ ä¼˜åŒ–ï¼šä»åˆ†æç»“æœç›´æ¥è·å–doc_idï¼Œä¸å†é‡å¤å¤„ç†æ–‡æ¡£
            doc_id = analysis_result.get('docId')
            if not doc_id:
                raise ValueError("åˆ†æç»“æœä¸­ç¼ºå°‘æ–‡æ¡£IDï¼Œè¯·é‡æ–°åˆ†ææ–‡æ¡£")
            
            knowledge_base_name = analysis_result['knowledgeBaseName']
            is_new_kb = analysis_result['isNewKnowledgeBase']
            reason = analysis_result.get('reason', '')
            kb_id = analysis_result.get('knowledgeBaseId')
            
            # è·å–æˆ–åˆ›å»ºçŸ¥è¯†åº“
            if is_new_kb or not kb_id:
                kb = await self.create_knowledge_base(
                    name=knowledge_base_name,
                    description=f"ç”±AIæ™ºèƒ½åˆ†æåˆ›å»ºçš„çŸ¥è¯†åº“ï¼Œç”¨äºå­˜å‚¨{analysis_result.get('documentType', 'ç›¸å…³')}ç±»å‹çš„æ–‡æ¡£",
                    color=self._get_random_color()
                )
                kb_id = kb['id']
                logger.info(f"åˆ›å»ºæ–°çŸ¥è¯†åº“: {knowledge_base_name} (ID: {kb_id})")
            else:
                # éªŒè¯çŸ¥è¯†åº“æ˜¯å¦ä»ç„¶å­˜åœ¨
                try:
                    all_kbs = await self.get_all_knowledge_bases()
                    kb_exists = any(kb['id'] == kb_id for kb in all_kbs)
                    if not kb_exists:
                        # çŸ¥è¯†åº“ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°çš„
                        kb = await self.create_knowledge_base(
                            name=knowledge_base_name,
                            description=f"æ™ºèƒ½å½’æ¡£åˆ›å»ºçš„çŸ¥è¯†åº“",
                            color=self._get_random_color()
                        )
                        kb_id = kb['id']
                        is_new_kb = True
                        logger.info(f"åŸçŸ¥è¯†åº“ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°çŸ¥è¯†åº“: {knowledge_base_name}")
                except Exception as e:
                    logger.warning(f"éªŒè¯çŸ¥è¯†åº“æ—¶å‡ºé”™: {e}ï¼Œåˆ›å»ºæ–°çŸ¥è¯†åº“")
                    kb = await self.create_knowledge_base(
                        name=knowledge_base_name,
                        description=f"æ™ºèƒ½å½’æ¡£åˆ›å»ºçš„çŸ¥è¯†åº“",
                        color=self._get_random_color()
                    )
                    kb_id = kb['id']
                    is_new_kb = True
            
            # âš¡ æ ¸å¿ƒï¼šå°†å·²ä¿å­˜çš„æ–‡æ¡£å…³è”åˆ°çŸ¥è¯†åº“
            await self.add_documents_to_knowledge_base(kb_id, [doc_id])
            
            result = {
                "fileName": filename,
                "knowledgeBaseName": knowledge_base_name,
                "isNewKnowledgeBase": is_new_kb,
                "reason": reason,
                "docId": doc_id,
                "knowledgeBaseId": kb_id
            }
            
            logger.info(f"ç¡®è®¤å½’æ¡£å®Œæˆï¼ˆä»…å…³è”ï¼‰: {filename} -> {knowledge_base_name} ({'æ–°å»º' if is_new_kb else 'ç°æœ‰'}), doc_id: {doc_id}")
            return result
            
        except Exception as e:
            logger.error(f"ç¡®è®¤å½’æ¡£å¤±è´¥ {filename}: {e}")
            raise

    
    async def _extract_text_from_file(self, file_content: bytes, filename: str, file_type: str) -> str:
        """ä»æ–‡ä»¶ä¸­æå–æ–‡æœ¬å†…å®¹"""
        try:
            from app.services.file_extraction_service import file_extraction_service
            
            # ä½¿ç”¨ç»Ÿä¸€çš„æ–‡ä»¶æå–æœåŠ¡
            text, metadata = await file_extraction_service.extract_text_from_file(
                file_content, filename, file_type
            )
            
            # è®°å½•æå–å…ƒæ•°æ®åˆ°æ—¥å¿—
            extraction_method = metadata.get('extraction_method', 'unknown')
            processing_time = metadata.get('extraction_time', 0)
            confidence = metadata.get('confidence', None)
            
            log_msg = f"æ–‡ä»¶æå–å®Œæˆ: {filename}, æ–¹æ³•: {extraction_method}, è€—æ—¶: {processing_time:.2f}ç§’"
            if confidence is not None:
                log_msg += f", ç½®ä¿¡åº¦: {confidence:.2f}"
            
            logger.info(log_msg)
            
            return text
                
        except Exception as e:
            logger.error(f"æ–‡æœ¬æå–å¤±è´¥ {filename}: {e}")
            return f"æ–‡æ¡£: {filename}\nå†…å®¹æå–å¤±è´¥: {str(e)}"
    
    async def _analyze_document_content(
        self, 
        content: str, 
        filename: str, 
        analysis_prompt: str,
        custom_analysis: bool = False
    ) -> Dict:
        """
        ä½¿ç”¨çœŸæ­£çš„LLMåˆ†ææ–‡æ¡£å†…å®¹å¹¶å†³å®šå½’æ¡£ä½ç½®
        
        å¯¹äºç‰¹å¤§æ–‡æ¡£é‡‡ç”¨æ™ºèƒ½å¤„ç†ç­–ç•¥ï¼š
        1. å°æ–‡æ¡£(<5000å­—ç¬¦)ï¼šç›´æ¥å…¨æ–‡åˆ†æ
        2. ä¸­ç­‰æ–‡æ¡£(5000-20000å­—ç¬¦)ï¼šæå–å…³é”®æ®µè½åˆ†æ
        3. å¤§æ–‡æ¡£(20000-100000å­—ç¬¦)ï¼šåˆ†æ®µæ‘˜è¦åˆ†æ
        4. ç‰¹å¤§æ–‡æ¡£(>100000å­—ç¬¦)ï¼šæ™ºèƒ½é‡‡æ ·+å…³é”®ä¿¡æ¯æå–
        """
        try:
            # æ£€æŸ¥ç¼“å­˜
            content_hash = hashlib.md5((content + filename + analysis_prompt).encode('utf-8')).hexdigest()
            if content_hash in self.analysis_cache:
                logger.info(f"ä½¿ç”¨ç¼“å­˜çš„åˆ†æç»“æœ: {filename}")
                return self.analysis_cache[content_hash]
            
            content_length = len(content)
            logger.info(f"å¼€å§‹åˆ†ææ–‡æ¡£: {filename}, å¤§å°: {content_length} å­—ç¬¦")
            
            # æ ¹æ®æ–‡æ¡£å¤§å°é€‰æ‹©å¤„ç†ç­–ç•¥
            processing_strategy = DocumentAnalyzer.get_document_processing_strategy(content_length)
            
            if processing_strategy == "direct_analysis":
                # å°æ–‡æ¡£ï¼šç›´æ¥å…¨æ–‡åˆ†æ
                analysis_content = content
            elif processing_strategy == "key_paragraphs":
                # ä¸­ç­‰æ–‡æ¡£ï¼šæå–å…³é”®æ®µè½
                analysis_content = await TextProcessor.extract_key_paragraphs(content, filename)
            elif processing_strategy == "segment_summary":
                # å¤§æ–‡æ¡£ï¼šåˆ†æ®µæ‘˜è¦
                analysis_content = await TextProcessor.create_document_summary(content, filename)
            else:
                # ç‰¹å¤§æ–‡æ¡£ï¼šæ™ºèƒ½é‡‡æ ·
                analysis_content = await TextProcessor.intelligent_sampling(content, filename)
            
            logger.info(f"æ–‡æ¡£å¤„ç†ç­–ç•¥: {processing_strategy}, åˆ†æå†…å®¹é•¿åº¦: {len(analysis_content)}")
            
            # è°ƒç”¨LLMè¿›è¡Œåˆ†æ
            analysis_result = await self._call_llm_for_analysis(
                content=analysis_content,
                filename=filename,
                analysis_prompt=analysis_prompt,
                custom_analysis=custom_analysis,
                processing_strategy=processing_strategy
            )
            
            # ç¼“å­˜ç»“æœ
            self.analysis_cache[content_hash] = analysis_result
            
            # é™åˆ¶ç¼“å­˜å¤§å°
            if len(self.analysis_cache) > 100:
                # ç§»é™¤æœ€æ—§çš„ç¼“å­˜é¡¹
                oldest_key = next(iter(self.analysis_cache))
                del self.analysis_cache[oldest_key]
            
            logger.info(f"æ–‡æ¡£åˆ†æå®Œæˆ: {filename} -> {analysis_result['knowledge_base_name']}")
            return analysis_result
            
        except Exception as e:
            logger.error(f"æ–‡æ¡£åˆ†æå¤±è´¥: {e}")
    
    async def _extract_key_paragraphs(self, content: str, filename: str) -> str:
        """æå–å…³é”®æ®µè½ """
        return await TextProcessor.extract_key_paragraphs(content, filename)
    
    async def _create_document_summary(self, content: str, filename: str) -> str:
        """åˆ›å»ºæ–‡æ¡£æ‘˜è¦"""
        return await TextProcessor.create_document_summary(content, filename)
    
    async def _intelligent_sampling(self, content: str, filename: str) -> str:
        """æ™ºèƒ½é‡‡æ · """
        return await TextProcessor.intelligent_sampling(content, filename)
    
    async def _call_llm_for_analysis(
        self,
        content: str,
        filename: str,
        analysis_prompt: str,
        custom_analysis: bool,
        processing_strategy: str
    ) -> Dict:
        """è°ƒç”¨LLMè¿›è¡Œæ–‡æ¡£å†…å®¹åˆ†æ """
        return await LLMClient.call_llm_for_analysis(content, filename, analysis_prompt, custom_analysis, processing_strategy)

    async def _make_llm_request(self, system_prompt: str, user_prompt: str) -> str:
        """å‘LM Studioå‘é€è¯·æ±‚ """
        return await LLMClient.make_llm_request(system_prompt, user_prompt)

    async def _parse_llm_response(self, response: str) -> Dict:
        """è§£æLLMå“åº” """
        return DocumentAnalyzer.parse_llm_response(response)

    async def _validate_analysis_result(self, result: Dict, filename: str) -> Dict:
        """éªŒè¯å’Œä¿®æ­£åˆ†æç»“æœ """
        return DocumentAnalyzer.validate_analysis_result(result, filename)

    def _extract_document_type(self, filename: str, content: str) -> str:
        """ä»æ–‡ä»¶åå’Œå†…å®¹ä¸­æå–æ–‡æ¡£ç±»å‹ """
        return TextProcessor.extract_document_type(filename, content)

    def _get_random_color(self) -> str:
        """è·å–éšæœºé¢œè‰² """
        return get_random_color()

    async def analyze_existing_document_for_archive(
        self, 
        doc_id: str,
        filename: str, 
        file_type: str,
        text_content: str,
        analysis_prompt: str,
        custom_analysis: bool = False
    ) -> Dict:
        """
        åˆ†æå·²æœ‰æ–‡æ¡£è¿›è¡Œæ™ºèƒ½å½’æ¡£å»ºè®®
        
        Args:
            doc_id: æ–‡æ¡£ID
            filename: æ–‡ä»¶å
            file_type: æ–‡ä»¶ç±»å‹
            text_content: æ–‡æ¡£å†…å®¹
            analysis_prompt: åˆ†ææç¤ºè¯
            custom_analysis: æ˜¯å¦ä½¿ç”¨è‡ªå®šä¹‰åˆ†æ
            
        Returns:
            åˆ†æç»“æœä¿¡æ¯
        """
        try:
            # ä½¿ç”¨AIåˆ†ææ–‡æ¡£å†…å®¹å¹¶åŒ¹é…çŸ¥è¯†åº“
            analysis_result = await self._analyze_document_content(
                content=text_content,
                filename=filename,
                analysis_prompt=analysis_prompt,
                custom_analysis=custom_analysis
            )
            
            knowledge_base_name = analysis_result['knowledge_base_name']
            is_new_kb = analysis_result['is_new_knowledge_base']
            reason = analysis_result.get('reason', '')
            
            # å¦‚æœä¸æ˜¯æ–°å»ºçŸ¥è¯†åº“ï¼Œæ£€æŸ¥ç°æœ‰çŸ¥è¯†åº“æ˜¯å¦å­˜åœ¨
            kb_id = None
            if not is_new_kb:
                all_kbs = await self.get_all_knowledge_bases()
                for kb in all_kbs:
                    if kb['name'] == knowledge_base_name:
                        kb_id = kb['id']
                        break
                
                # å¦‚æœæ²¡æ‰¾åˆ°åŒ¹é…çš„çŸ¥è¯†åº“ï¼Œæ ‡è®°ä¸ºæ–°å»º
                if not kb_id:
                    is_new_kb = True
            
            result = {
                "filename": filename,
                "knowledgeBaseName": knowledge_base_name,
                "isNewKnowledgeBase": is_new_kb,
                "reason": reason,
                "knowledgeBaseId": kb_id,
                "documentType": analysis_result.get('document_type', 'æœªçŸ¥'),
                "textContent": text_content[:500] + "..." if len(text_content) > 500 else text_content,  # é¢„è§ˆå†…å®¹
                "docId": doc_id,  # æ–‡æ¡£IDï¼ˆå·²å­˜åœ¨ï¼‰
                "analysisTime": datetime.now().timestamp()
            }
            
            logger.info(f"å·²æœ‰æ–‡æ¡£åˆ†æå®Œæˆ: {filename} -> {knowledge_base_name} ({'æ–°å»º' if is_new_kb else 'ç°æœ‰'}), doc_id: {doc_id}")
            return result
            
        except Exception as e:
            logger.error(f"å·²æœ‰æ–‡æ¡£åˆ†æå¤±è´¥ {filename}: {e}")
            raise

    async def confirm_existing_document_archive(
        self,
        doc_id: str,
        analysis_result: Dict
    ) -> Dict:
        """
        ç¡®è®¤å·²æœ‰æ–‡æ¡£çš„å½’æ¡£æ“ä½œï¼Œæ‰§è¡Œæ–‡æ¡£ä¸çŸ¥è¯†åº“çš„å…³è”
        
        Args:
            doc_id: æ–‡æ¡£ID
            analysis_result: åˆ†æç»“æœ
            
        Returns:
            å½’æ¡£ç»“æœä¿¡æ¯
        """
        try:
            knowledge_base_name = analysis_result['knowledgeBaseName']
            is_new_kb = analysis_result['isNewKnowledgeBase']
            reason = analysis_result.get('reason', '')
            kb_id = analysis_result.get('knowledgeBaseId')
            filename = analysis_result.get('filename', 'æœªçŸ¥æ–‡æ¡£')
            
            # è·å–æˆ–åˆ›å»ºçŸ¥è¯†åº“
            if is_new_kb or not kb_id:
                kb = await self.create_knowledge_base(
                    name=knowledge_base_name,
                    description=f"ç”±AIæ™ºèƒ½åˆ†æåˆ›å»ºçš„çŸ¥è¯†åº“ï¼Œç”¨äºå­˜å‚¨{analysis_result.get('documentType', 'ç›¸å…³')}ç±»å‹çš„æ–‡æ¡£",
                    color=self._get_random_color()
                )
                kb_id = kb['id']
                logger.info(f"åˆ›å»ºæ–°çŸ¥è¯†åº“: {knowledge_base_name} (ID: {kb_id})")
            else:
                # éªŒè¯çŸ¥è¯†åº“æ˜¯å¦ä»ç„¶å­˜åœ¨
                try:
                    all_kbs = await self.get_all_knowledge_bases()
                    kb_exists = any(kb['id'] == kb_id for kb in all_kbs)
                    if not kb_exists:
                        # çŸ¥è¯†åº“ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°çš„
                        kb = await self.create_knowledge_base(
                            name=knowledge_base_name,
                            description=f"æ™ºèƒ½å½’æ¡£åˆ›å»ºçš„çŸ¥è¯†åº“",
                            color=self._get_random_color()
                        )
                        kb_id = kb['id']
                        is_new_kb = True
                        logger.info(f"åŸçŸ¥è¯†åº“ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°çŸ¥è¯†åº“: {knowledge_base_name}")
                except Exception as e:
                    logger.warning(f"éªŒè¯çŸ¥è¯†åº“æ—¶å‡ºé”™: {e}ï¼Œåˆ›å»ºæ–°çŸ¥è¯†åº“")
                    kb = await self.create_knowledge_base(
                        name=knowledge_base_name,
                        description=f"æ™ºèƒ½å½’æ¡£åˆ›å»ºçš„çŸ¥è¯†åº“",
                        color=self._get_random_color()
                    )
                    kb_id = kb['id']
                    is_new_kb = True
            
            # å°†æ–‡æ¡£å…³è”åˆ°çŸ¥è¯†åº“
            await self.add_documents_to_knowledge_base(kb_id, [doc_id])
            
            result = {
                "filename": filename,
                "knowledgeBaseName": knowledge_base_name,
                "isNewKnowledgeBase": is_new_kb,
                "reason": reason,
                "docId": doc_id,
                "knowledgeBaseId": kb_id
            }
            
            logger.info(f"å·²æœ‰æ–‡æ¡£å½’æ¡£å®Œæˆ: {filename} -> {knowledge_base_name} ({'æ–°å»º' if is_new_kb else 'ç°æœ‰'}), doc_id: {doc_id}")
            return result
            
        except Exception as e:
            logger.error(f"å·²æœ‰æ–‡æ¡£å½’æ¡£å¤±è´¥: {e}")
            raise

    def __del__(self):
        """æ¸…ç†èµ„æº"""
        if hasattr(self, 'executor'):
            self.executor.shutdown(wait=False)

# å…¨å±€RAGæœåŠ¡å®ä¾‹
rag_service = RAGService()
