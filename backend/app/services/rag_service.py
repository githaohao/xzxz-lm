"""
RAG (Retrieval-Augmented Generation) æœåŠ¡
ç”¨äºæ™ºèƒ½æ–‡æ¡£æ£€ç´¢å’Œå¢å¼ºç”Ÿæˆ
"""

import os
import uuid
import hashlib
import logging
from typing import List, Dict, Optional, Tuple
from datetime import datetime
import asyncio
from concurrent.futures import ThreadPoolExecutor
import json
import httpx

import chromadb
from chromadb.config import Settings
from sentence_transformers import SentenceTransformer
from langchain.text_splitter import RecursiveCharacterTextSplitter
import numpy as np

from app.config import settings
from app.database import Database

logger = logging.getLogger(__name__)

class DocumentChunk:
    """æ–‡æ¡£å—æ•°æ®ç»“æ„"""
    def __init__(self, content: str, metadata: Dict):
        self.content = content
        self.metadata = metadata
        self.chunk_id = str(uuid.uuid4())

class RAGService:
    """RAGæœåŠ¡ç±»"""
    
    def __init__(self):
        self.chroma_client = None
        self.collection = None
        self.embedding_model = None
        self.text_splitter = None
        self.executor = ThreadPoolExecutor(max_workers=4)
        self.db = Database()  # æ•°æ®åº“è¿æ¥
        # æ–‡æ¡£-çŸ¥è¯†åº“å…³è”å…³ç³»ç¼“å­˜ï¼Œä»æ•°æ®åº“åŠ è½½
        self.document_kb_mapping = {}  # doc_id -> [kb_id1, kb_id2, ...]  
        # æ–‡æ¡£åˆ†æç»“æœç¼“å­˜ï¼Œé¿å…é‡å¤åˆ†æ
        self.analysis_cache = {}  # content_hash -> analysis_result
        self._initialize()
    
    def _initialize(self):
        """åˆå§‹åŒ–RAGæœåŠ¡"""
        try:
            # è®¾ç½® tokenizers ç¯å¢ƒå˜é‡ï¼Œé¿å…å¹¶è¡Œå¤„ç†è­¦å‘Š
            os.environ["TOKENIZERS_PARALLELISM"] = "false"
            
            # åˆå§‹åŒ–ChromaDB
            chroma_path = os.path.join(settings.upload_dir, "chroma_db")
            os.makedirs(chroma_path, exist_ok=True)
            
            self.chroma_client = chromadb.PersistentClient(
                path=chroma_path,
                settings=Settings(
                    anonymized_telemetry=False,
                    allow_reset=True
                )
            )
            
            # è·å–æˆ–åˆ›å»ºé›†åˆ
            self.collection = self.chroma_client.get_or_create_collection(
                name="document_chunks",
                metadata={"description": "å­˜å‚¨æ–‡æ¡£åˆ†å—çš„å‘é‡é›†åˆ"}
            )
            
            # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹ (æ”¯æŒä¸­æ–‡)
            model_name = settings.embedding_model
            logger.info(f"æ­£åœ¨åŠ è½½åµŒå…¥æ¨¡å‹: {model_name}")
            
            try:
                self.embedding_model = SentenceTransformer(model_name)
                logger.info(f"åµŒå…¥æ¨¡å‹åŠ è½½æˆåŠŸ: {model_name}")
            except Exception as e:
                logger.error(f"åŠ è½½åµŒå…¥æ¨¡å‹ {model_name} å¤±è´¥: {e}")
                # ä½¿ç”¨ä¸­æ–‡å‹å¥½çš„å¤‡ç”¨æ¨¡å‹
                fallback_model = 'shibing624/text2vec-base-chinese'
                try:
                    self.embedding_model = SentenceTransformer(fallback_model)
                    logger.info(f"å¤‡ç”¨åµŒå…¥æ¨¡å‹åŠ è½½æˆåŠŸ: {fallback_model}")
                except Exception as fallback_error:
                    logger.error(f"å¤‡ç”¨æ¨¡å‹ä¹ŸåŠ è½½å¤±è´¥: {fallback_error}")
                    raise RuntimeError(f"æ— æ³•åŠ è½½ä»»ä½•åµŒå…¥æ¨¡å‹: ä¸»æ¨¡å‹={model_name}, å¤‡ç”¨æ¨¡å‹={fallback_model}")
            
            # åˆå§‹åŒ–æ–‡æœ¬åˆ†å‰²å™¨
            self.text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=settings.rag_chunk_size,
                chunk_overlap=settings.rag_chunk_overlap,
                length_function=len,
                separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", " ", ""]
            )
            
            # å»¶è¿ŸåŠ è½½æ–‡æ¡£-çŸ¥è¯†åº“å…³è”å…³ç³»ï¼ˆåœ¨é¦–æ¬¡éœ€è¦æ—¶åŠ è½½ï¼‰
            self._mapping_loaded = False
            
            logger.info("RAGæœåŠ¡åˆå§‹åŒ–æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"RAGæœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    async def process_document(self, content: str, filename: str, file_type: str) -> str:
        """
        å¤„ç†æ–‡æ¡£ï¼šåˆ†å—ã€å‘é‡åŒ–ã€å­˜å‚¨
        
        Args:
            content: æ–‡æ¡£å†…å®¹
            filename: æ–‡ä»¶å
            file_type: æ–‡ä»¶ç±»å‹
            
        Returns:
            document_id: æ–‡æ¡£å”¯ä¸€æ ‡è¯†
        """
        try:
            # ç”Ÿæˆæ–‡æ¡£ID
            doc_id = self._generate_doc_id(content, filename)
            
            # æ£€æŸ¥æ–‡æ¡£æ˜¯å¦å·²å­˜åœ¨
            existing_docs = self.collection.get(
                where={"doc_id": doc_id}
            )
            
            if existing_docs['ids']:
                logger.info(f"æ–‡æ¡£å·²å­˜åœ¨ï¼Œè·³è¿‡å¤„ç†: {filename}")
                return doc_id
            
            # æ–‡æ¡£åˆ†å—
            chunks = await self._split_document(content, filename, file_type)
            
            if not chunks:
                logger.warning(f"æ–‡æ¡£åˆ†å—ä¸ºç©º: {filename}")
                return doc_id
            
            # å‘é‡åŒ–å’Œå­˜å‚¨
            await self._vectorize_and_store(chunks, doc_id)
            
            logger.info(f"æ–‡æ¡£å¤„ç†å®Œæˆ: {filename}, åˆ†å—æ•°: {len(chunks)}")
            return doc_id
            
        except Exception as e:
            logger.error(f"æ–‡æ¡£å¤„ç†å¤±è´¥ {filename}: {e}")
            raise
    
    async def search_relevant_chunks(
        self, 
        query: str, 
        doc_ids: Optional[List[str]] = None,
        top_k: int = 5,
        min_similarity: float = 0.355,
        _retry_count: int = 0  # å†…éƒ¨é‡è¯•è®¡æ•°å™¨
    ) -> List[Dict]:
        """
        æ£€ç´¢ç›¸å…³æ–‡æ¡£å—
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            doc_ids: é™å®šæœç´¢çš„æ–‡æ¡£IDåˆ—è¡¨
            top_k: æœ€å¤§è¿”å›æ•°é‡ï¼ˆä¸ä¿è¯è¿”å›è¿™ä¹ˆå¤šï¼Œä¼šæ ¹æ®ç›¸ä¼¼åº¦ç­›é€‰ï¼‰
            min_similarity: æœ€å°ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œåªè¿”å›ç›¸ä¼¼åº¦è¾¾åˆ°æ­¤é˜ˆå€¼çš„ç»“æœ
            _retry_count: å†…éƒ¨é‡è¯•è®¡æ•°å™¨ï¼Œé˜²æ­¢æ— é™é€’å½’
            
        Returns:
            ç›¸å…³æ–‡æ¡£å—åˆ—è¡¨ï¼ˆæŒ‰ç›¸ä¼¼åº¦é™åºæ’åˆ—ï¼Œæ•°é‡å¯èƒ½å°‘äºtop_kï¼‰
        """
        try:
            if not query.strip():
                logger.warning("æŸ¥è¯¢å­—ç¬¦ä¸²ä¸ºç©º")
                return []
            
            # é™åˆ¶æœ€å¤§é‡è¯•æ¬¡æ•°
            max_retries = 2
            if _retry_count > max_retries:
                logger.warning(f"å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° {max_retries}ï¼Œåœæ­¢æœç´¢")
                return []
            
            retry_info = f", é‡è¯•ç¬¬{_retry_count}æ¬¡" if _retry_count > 0 else ""
            logger.info(f"å¼€å§‹RAGæ£€ç´¢{retry_info}: query='{query}', doc_ids={doc_ids}, max_results={top_k}, min_similarity={min_similarity}")
            
            # ç”ŸæˆæŸ¥è¯¢å‘é‡
            query_embedding = await self._generate_embedding(query)
            logger.info(f"æŸ¥è¯¢å‘é‡ç”ŸæˆæˆåŠŸï¼Œç»´åº¦: {query_embedding.shape}")
            
            # æ„å»ºæŸ¥è¯¢æ¡ä»¶
            where_clause = None
            if doc_ids:
                where_clause = {"doc_id": {"$in": doc_ids}}
                logger.info(f"é™å®šæœç´¢æ–‡æ¡£: {doc_ids}")
            
            # æ£€æŸ¥æ•°æ®åº“ä¸­çš„æ–‡æ¡£æ•°é‡
            total_docs = len(self.collection.get()['ids'])
            logger.info(f"æ•°æ®åº“ä¸­å…±æœ‰ {total_docs} ä¸ªæ–‡æ¡£å—")
            
            if total_docs == 0:
                logger.warning("æ•°æ®åº“ä¸­æ²¡æœ‰æ–‡æ¡£å—ï¼Œè¯·å…ˆä¸Šä¼ å¹¶å¤„ç†æ–‡æ¡£")
                return []
            
            # å¢åŠ æ£€ç´¢æ•°é‡ä»¥è·å¾—æ›´å¤šå€™é€‰ç»“æœè¿›è¡Œç­›é€‰
            search_count = min(max(top_k * 2, 20), total_docs)
            
            # å‘é‡æ£€ç´¢
            results = self.collection.query(
                query_embeddings=[query_embedding.tolist()],
                n_results=search_count,
                where=where_clause
            )
            
            logger.info(f"ChromaDBè¿”å› {len(results['ids'][0]) if results['ids'][0] else 0} ä¸ªå€™é€‰ç»“æœ")
            
            # å¤„ç†ç»“æœ
            relevant_chunks = []
            
            if results['ids'][0]:  # å¦‚æœæœ‰ç»“æœ
                logger.info("å¤„ç†æ£€ç´¢ç»“æœ...")
                for i, chunk_id in enumerate(results['ids'][0]):
                    distance = results['distances'][0][i]
                    
                    # ä¿®å¤ç›¸ä¼¼åº¦è®¡ç®—ï¼šæ­£ç¡®å¤„ç†ChromaDBçš„L2è·ç¦»
                    # ChromaDBä½¿ç”¨å¹³æ–¹L2è·ç¦»ï¼Œå¯¹äºå½’ä¸€åŒ–å‘é‡çš„æ­£ç¡®è½¬æ¢å¦‚ä¸‹ï¼š
                    if distance >= 0:
                        similarity = max(0.0, 1.0 - distance)
                    else:
                        similarity = 1.0  # è·ç¦»ä¸ºè´Ÿæ•°æ—¶è®¾ä¸ºæœ€é«˜ç›¸ä¼¼åº¦
                    
                    # ç¡®ä¿ç›¸ä¼¼åº¦åœ¨åˆç†èŒƒå›´å†…
                    similarity = max(0.0, min(1.0, similarity))
                    
                    logger.debug(f"å€™é€‰ç»“æœ {i+1}: L2è·ç¦»={distance:.4f}, è®¡ç®—ç›¸ä¼¼åº¦={similarity:.4f}")
                    
                    # åªä¿ç•™è¾¾åˆ°ç›¸ä¼¼åº¦é˜ˆå€¼çš„ç»“æœ
                    if similarity >= min_similarity:
                        relevant_chunks.append({
                            'chunk_id': chunk_id,
                            'content': results['documents'][0][i],
                            'metadata': results['metadatas'][0][i],
                            'similarity': similarity
                        })
                        logger.debug(f"ç»“æœ {i+1} é€šè¿‡ç›¸ä¼¼åº¦é˜ˆå€¼")
                    else:
                        logger.debug(f"ç»“æœ {i+1} æœªé€šè¿‡ç›¸ä¼¼åº¦é˜ˆå€¼ ({similarity:.4f} < {min_similarity})")
                
                # æŒ‰ç›¸ä¼¼åº¦é™åºæ’åº
                relevant_chunks.sort(key=lambda x: x['similarity'], reverse=True)
                
                # åªè¿”å›top_kä¸ªæœ€ç›¸å…³çš„ç»“æœ
                if len(relevant_chunks) > top_k:
                    logger.info(f"ç­›é€‰å‡º {len(relevant_chunks)} ä¸ªç›¸å…³ç»“æœï¼Œä¿ç•™å‰ {top_k} ä¸ªæœ€ç›¸å…³çš„")
                    relevant_chunks = relevant_chunks[:top_k]
                
            else:
                logger.warning("ChromaDBæ²¡æœ‰è¿”å›ä»»ä½•ç»“æœ")
            
            logger.info(f"æœ€ç»ˆæ£€ç´¢åˆ° {len(relevant_chunks)} ä¸ªç›¸å…³æ–‡æ¡£å—")
            
            # æ™ºèƒ½é‡è¯•é€»è¾‘ï¼šåªåœ¨ç‰¹å®šæ¡ä»¶ä¸‹é‡è¯•ï¼Œä¸”æé«˜é˜ˆå€¼è¦æ±‚
            if not relevant_chunks and _retry_count < max_retries:
                # è·å–æœ€é«˜ç›¸ä¼¼åº¦ä»¥åˆ¤æ–­æ˜¯å¦éœ€è¦é‡è¯•
                if results['ids'][0]:
                    highest_similarity = max([
                        max(0.0, 1.0 - results['distances'][0][i])  # ä¿®æ­£ç›¸ä¼¼åº¦è®¡ç®—
                        for i in range(len(results['distances'][0]))
                    ])
                    
                    # åªæœ‰åœ¨æœ€é«˜ç›¸ä¼¼åº¦æ¥è¿‘é˜ˆå€¼æ—¶æ‰é‡è¯•ï¼Œä¸”ä¸é™å¾—å¤ªä½
                    if highest_similarity >= 0.25 and min_similarity > 0.5:
                        retry_threshold = max(0.5, min_similarity - 0.1)  # è°ƒæ•´é‡è¯•é˜ˆå€¼ä¸‹é™åˆ°0.5
                        logger.info(f"æœ€é«˜ç›¸ä¼¼åº¦ {highest_similarity:.3f} æ¥è¿‘é˜ˆå€¼ï¼Œå°è¯•é™ä½ç›¸ä¼¼åº¦é˜ˆå€¼ä» {min_similarity} åˆ° {retry_threshold}")
                        return await self.search_relevant_chunks(
                            query=query,
                            doc_ids=doc_ids,
                            top_k=top_k,
                            min_similarity=retry_threshold,
                            _retry_count=_retry_count + 1
                        )
                    elif highest_similarity < 0.25:
                        logger.info(f"æœ€é«˜ç›¸ä¼¼åº¦ {highest_similarity:.3f} è¿‡ä½ï¼Œç›´æ¥è¿”å›ç©ºç»“æœ")
                        return []
            
            return relevant_chunks
            
        except Exception as e:
            logger.error(f"æ–‡æ¡£æ£€ç´¢å¤±è´¥: {e}")
            return []
    
    async def get_document_info(self, doc_id: str) -> Optional[Dict]:
        """è·å–æ–‡æ¡£ä¿¡æ¯"""
        try:
            results = self.collection.get(
                where={"doc_id": doc_id},
                limit=1
            )
            
            if results['ids']:
                metadata = results['metadatas'][0]
                return {
                    'doc_id': doc_id,
                    'filename': metadata.get('filename'),
                    'file_type': metadata.get('file_type'),
                    'chunk_count': len(results['ids']),
                    'created_at': metadata.get('created_at')
                }
            
            return None
            
        except Exception as e:
            logger.error(f"è·å–æ–‡æ¡£ä¿¡æ¯å¤±è´¥ {doc_id}: {e}")
            return None
    
    async def get_all_documents(self) -> List[Dict]:
        """è·å–æ‰€æœ‰æ–‡æ¡£åˆ—è¡¨"""
        try:
            # è·å–æ‰€æœ‰æ–‡æ¡£å—
            results = self.collection.get()
            
            # æŒ‰doc_idåˆ†ç»„ç»Ÿè®¡
            doc_stats = {}
            
            if results['ids']:
                for i, chunk_id in enumerate(results['ids']):
                    metadata = results['metadatas'][i]
                    doc_id = metadata.get('doc_id')
                    
                    if doc_id not in doc_stats:
                        doc_stats[doc_id] = {
                            'doc_id': doc_id,
                            'filename': metadata.get('filename', 'Unknown'),
                            'file_type': metadata.get('file_type', 'Unknown'),
                            'created_at': metadata.get('created_at', ''),
                            'chunk_count': 0,
                            'total_length': 0
                        }
                    
                    doc_stats[doc_id]['chunk_count'] += 1
                    doc_stats[doc_id]['total_length'] += metadata.get('chunk_length', 0)
            
            # è½¬æ¢ä¸ºåˆ—è¡¨å¹¶æŒ‰åˆ›å»ºæ—¶é—´æ’åº
            documents = list(doc_stats.values())
            documents.sort(key=lambda x: x['created_at'], reverse=True)
            
            logger.info(f"è·å–åˆ° {len(documents)} ä¸ªæ–‡æ¡£")
            return documents
            
        except Exception as e:
            logger.error(f"è·å–æ–‡æ¡£åˆ—è¡¨å¤±è´¥: {e}")
            return []

    async def get_document_chunks(self, doc_id: str) -> List[Dict]:
        """è·å–æŒ‡å®šæ–‡æ¡£çš„æ‰€æœ‰åˆ†å—"""
        try:
            # è·å–æ‰€æœ‰ç›¸å…³åˆ†å—
            results = self.collection.get(
                where={"doc_id": doc_id}
            )
            
            chunks = []
            if results['ids']:
                for i, chunk_id in enumerate(results['ids']):
                    chunks.append({
                        'chunk_id': chunk_id,
                        'content': results['documents'][i],
                        'metadata': results['metadatas'][i],
                        'similarity': 1.0  # å®Œæ•´æ–‡æ¡£æ—¶ç›¸ä¼¼åº¦è®¾ä¸º1.0
                    })
                
                # æŒ‰chunk_indexæ’åº
                chunks.sort(key=lambda x: x['metadata'].get('chunk_index', 0))
            
            logger.info(f"è·å–æ–‡æ¡£ {doc_id} çš„ {len(chunks)} ä¸ªåˆ†å—")
            return chunks
            
        except Exception as e:
            logger.error(f"è·å–æ–‡æ¡£åˆ†å—å¤±è´¥ {doc_id}: {e}")
            return []

    async def delete_document(self, doc_id: str) -> bool:
        """åˆ é™¤æ–‡æ¡£åŠå…¶æ‰€æœ‰åˆ†å—"""
        try:
            # è·å–æ‰€æœ‰ç›¸å…³åˆ†å—
            results = self.collection.get(
                where={"doc_id": doc_id}
            )
            
            if results['ids']:
                # åˆ é™¤åˆ†å—
                self.collection.delete(ids=results['ids'])
                logger.info(f"åˆ é™¤æ–‡æ¡£æˆåŠŸ: {doc_id}, åˆ é™¤åˆ†å—æ•°: {len(results['ids'])}")
                return True
            
            return False
            
        except Exception as e:
            logger.error(f"åˆ é™¤æ–‡æ¡£å¤±è´¥ {doc_id}: {e}")
            return False
    
    async def _split_document(self, content: str, filename: str, file_type: str) -> List[DocumentChunk]:
        """åˆ†å‰²æ–‡æ¡£"""
        try:
            # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡Œæ–‡æœ¬åˆ†å‰²ï¼ˆCPUå¯†é›†å‹æ“ä½œï¼‰
            loop = asyncio.get_event_loop()
            chunks_text = await loop.run_in_executor(
                self.executor,
                self.text_splitter.split_text,
                content
            )
            
            chunks = []
            for i, chunk_text in enumerate(chunks_text):
                if chunk_text.strip():  # è·³è¿‡ç©ºåˆ†å—
                    metadata = {
                        'filename': filename,
                        'file_type': file_type,
                        'chunk_index': i,
                        'chunk_length': len(chunk_text),
                        'created_at': datetime.now().isoformat()
                    }
                    chunks.append(DocumentChunk(chunk_text, metadata))
            
            return chunks
            
        except Exception as e:
            logger.error(f"æ–‡æ¡£åˆ†å—å¤±è´¥: {e}")
            return []
    
    async def _vectorize_and_store(self, chunks: List[DocumentChunk], doc_id: str):
        """å‘é‡åŒ–å¹¶å­˜å‚¨æ–‡æ¡£å— - é’ˆå¯¹PDFå’ŒOCRæ–‡æ¡£ä¼˜åŒ–"""
        try:
            if not chunks:
                return
            # å¯¹chunksè¿›è¡Œè´¨é‡ä¼˜åŒ–å¤„ç†
            processed_chunks = await self._optimize_chunks_for_indexing(chunks)
            
            if not processed_chunks:
                logger.warning(f"æ–‡æ¡£å—è´¨é‡ä¼˜åŒ–åä¸ºç©º: {doc_id}")
                return
            
            # å‡†å¤‡æ•°æ®
            chunk_texts = [chunk.content for chunk in processed_chunks]
            chunk_ids = [chunk.chunk_id for chunk in processed_chunks]
            metadatas = []
            
            for chunk in processed_chunks:
                metadata = chunk.metadata.copy()
                metadata['doc_id'] = doc_id
                metadatas.append(metadata)
            
            # æ ¹æ®æ–‡æ¡£ç±»å‹é€‰æ‹©æœ€ä½³åµŒå…¥ç­–ç•¥
            embeddings = await self._generate_optimized_embeddings(chunk_texts, metadatas)
            
            # å­˜å‚¨åˆ°ChromaDB
            self.collection.add(
                ids=chunk_ids,
                embeddings=embeddings.tolist(),
                documents=chunk_texts,
                metadatas=metadatas
            )
            
            logger.info(f"æˆåŠŸå­˜å‚¨ {len(processed_chunks)} ä¸ªä¼˜åŒ–åçš„æ–‡æ¡£å—")
            
        except Exception as e:
            logger.error(f"å‘é‡åŒ–å­˜å‚¨å¤±è´¥: {e}")
            raise

    async def _optimize_chunks_for_indexing(self, chunks: List[DocumentChunk]) -> List[DocumentChunk]:
        """ä¼˜åŒ–æ–‡æ¡£å—ä»¥æé«˜ç´¢å¼•è´¨é‡"""
        optimized_chunks = []
        
        for chunk in chunks:
            try:
                # æ£€æµ‹æ–‡æ¡£ç±»å‹å’Œè´¨é‡
                doc_type = chunk.metadata.get('file_type', '').lower()
                is_pdf = chunk.metadata.get('is_pdf', False)
                is_text_pdf = chunk.metadata.get('is_text_pdf', False)
                is_ocr = chunk.metadata.get('is_ocr_processed', False)
                
                # åŸå§‹å†…å®¹
                original_content = chunk.content
                
                # åº”ç”¨åˆ†å±‚ä¼˜åŒ–ç­–ç•¥
                if is_ocr or (is_pdf and not is_text_pdf):
                    # OCRæ‰«æä»¶ï¼šéœ€è¦æ›´ä¸¥æ ¼çš„å¤„ç†
                    processed_content = await self._process_ocr_content(original_content)
                elif is_pdf and is_text_pdf:
                    # æ–‡æœ¬PDFï¼šä¸­ç­‰å¤„ç†
                    processed_content = await self._process_text_pdf_content(original_content)
                else:
                    # æ™®é€šæ–‡æ¡£ï¼šåŸºç¡€å¤„ç†
                    processed_content = await self._process_standard_content(original_content)
                
                # è´¨é‡æ£€æŸ¥
                quality_score = self._calculate_content_quality(processed_content)
                
                # åªä¿ç•™è´¨é‡è¾¾æ ‡çš„åˆ†å—
                if quality_score >= 0.3:  # è´¨é‡é˜ˆå€¼
                    # æ›´æ–°åˆ†å—å†…å®¹å’Œå…ƒæ•°æ®
                    optimized_chunk = DocumentChunk(processed_content, chunk.metadata.copy())
                    optimized_chunk.chunk_id = chunk.chunk_id
                    
                    # å¢å¼ºå…ƒæ•°æ®
                    optimized_chunk.metadata.update({
                        'quality_score': quality_score,
                        'content_length_original': len(original_content),
                        'content_length_processed': len(processed_content),
                        'processing_applied': self._get_processing_type(is_ocr, is_pdf, is_text_pdf),
                        'optimization_version': '2.0'
                    })
                    
                    optimized_chunks.append(optimized_chunk)
                else:
                    logger.debug(f"è·³è¿‡ä½è´¨é‡åˆ†å—: quality_score={quality_score:.3f}")
                    
            except Exception as e:
                logger.warning(f"åˆ†å—ä¼˜åŒ–å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹å†…å®¹: {e}")
                optimized_chunks.append(chunk)
        
        logger.info(f"åˆ†å—ä¼˜åŒ–å®Œæˆ: {len(chunks)} -> {len(optimized_chunks)}")
        return optimized_chunks

    async def _process_ocr_content(self, content: str) -> str:
        """å¤„ç†OCRæ‰«æå†…å®¹ - æœ€ä¸¥æ ¼çš„æ¸…ç†"""
        # åŸºç¡€æ¸…ç†
        text = await self._process_standard_content(content)
        
        # OCRç‰¹æ®Šé”™è¯¯æ¸…ç†
        import re
        
        # ä¿®æ­£å¸¸è§OCRé”™è¯¯
        ocr_corrections = {
            r'\b0\b': 'O',  # æ•°å­—0è¯¯è¯†åˆ«ä¸ºå­—æ¯O
            r'\bl\b': 'I',  # å°å†™lè¯¯è¯†åˆ«ä¸ºå¤§å†™I
            r'rn\b': 'm',   # rnè¯¯è¯†åˆ«ä¸ºm
            r'\s+': ' ',    # å¤šç©ºæ ¼åˆå¹¶
        }
        
        for pattern, replacement in ocr_corrections.items():
            text = re.sub(pattern, replacement, text)
        
        # ç§»é™¤å¯èƒ½çš„OCRå™ªéŸ³
        text = re.sub(r'[^\w\s\u4e00-\u9fff\u3000-\u303f\uff00-\uffef,.!?;:"\'()\-]', '', text)
        
        # ç§»é™¤è¿‡çŸ­çš„è¡Œï¼ˆå¯èƒ½æ˜¯å™ªéŸ³ï¼‰
        lines = text.split('\n')
        valid_lines = [line.strip() for line in lines if len(line.strip()) > 3]
        
        return '\n'.join(valid_lines)

    async def _process_text_pdf_content(self, content: str) -> str:
        """å¤„ç†æ–‡æœ¬PDFå†…å®¹ - ä¸­ç­‰æ¸…ç†"""
        # åŸºç¡€æ¸…ç†
        text = await self._process_standard_content(content)
        
        import re
        
        # PDFç‰¹æ®Šæ ¼å¼æ¸…ç†
        # ç§»é™¤é¡µçœ‰é¡µè„šæ¨¡å¼
        text = re.sub(r'^ç¬¬\s*\d+\s*é¡µ.*$', '', text, flags=re.MULTILINE)
        text = re.sub(r'^\d+\s*/\s*\d+.*$', '', text, flags=re.MULTILINE)
        
        # ç§»é™¤é‡å¤çš„åˆ†éš”ç¬¦
        text = re.sub(r'-{3,}', '---', text)
        text = re.sub(r'={3,}', '===', text)
        
        # åˆå¹¶ç ´ç¢çš„è¡Œ
        text = re.sub(r'(\w)\n(\w)', r'\1 \2', text)
        
        return text

    async def _process_standard_content(self, content: str) -> str:
        """æ ‡å‡†å†…å®¹å¤„ç† - åŸºç¡€æ¸…ç†"""
        import re
        
        # åŸºç¡€æ–‡æœ¬æ¸…ç†
        text = content.strip()
        
        # æ ‡å‡†åŒ–ç©ºç™½å­—ç¬¦
        text = re.sub(r'\r\n', '\n', text)  # æ ‡å‡†åŒ–æ¢è¡Œç¬¦
        text = re.sub(r'\r', '\n', text)
        text = re.sub(r'\t', ' ', text)     # åˆ¶è¡¨ç¬¦è½¬ç©ºæ ¼
        text = re.sub(r'[ ]{2,}', ' ', text)  # å¤šç©ºæ ¼åˆå¹¶
        
        # ç§»é™¤æ§åˆ¶å­—ç¬¦
        text = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f-\x84\x86-\x9f]', '', text)
        
        # ç§»é™¤ç©ºè¡Œè¿‡å¤šçš„æƒ…å†µ
        text = re.sub(r'\n{3,}', '\n\n', text)
        
        return text.strip()

    def _calculate_content_quality(self, content: str) -> float:
        """è®¡ç®—å†…å®¹è´¨é‡åˆ†æ•°"""
        if not content or len(content.strip()) < 10:
            return 0.0
        
        import re
        
        score = 1.0
        
        # é•¿åº¦æ£€æŸ¥
        length = len(content)
        if length < 20:
            score *= 0.5
        elif length < 50:
            score *= 0.8
        
        # å­—ç¬¦ç»„æˆæ£€æŸ¥
        total_chars = len(content)
        if total_chars > 0:
            # ä¸­æ–‡å­—ç¬¦æ¯”ä¾‹
            chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', content))
            chinese_ratio = chinese_chars / total_chars
            
            # è‹±æ–‡å­—æ¯æ¯”ä¾‹
            alpha_chars = len(re.findall(r'[a-zA-Z]', content))
            alpha_ratio = alpha_chars / total_chars
            
            # æ•°å­—æ¯”ä¾‹
            digit_chars = len(re.findall(r'\d', content))
            digit_ratio = digit_chars / total_chars
            
            # æœ‰æ„ä¹‰å­—ç¬¦æ¯”ä¾‹
            meaningful_ratio = chinese_ratio + alpha_ratio + digit_ratio
            
            if meaningful_ratio < 0.6:
                score *= 0.6
            elif meaningful_ratio < 0.8:
                score *= 0.8
        
        # ç‰¹æ®Šå­—ç¬¦è¿‡å¤šæ£€æŸ¥
        special_chars = len(re.findall(r'[^\w\s\u4e00-\u9fff]', content))
        if special_chars / total_chars > 0.3:
            score *= 0.7
        
        # é‡å¤å†…å®¹æ£€æŸ¥
        lines = content.split('\n')
        unique_lines = set(line.strip() for line in lines if line.strip())
        if len(lines) > 0 and len(unique_lines) / len(lines) < 0.7:
            score *= 0.8
        
        return max(0.0, min(1.0, score))

    def _get_processing_type(self, is_ocr: bool, is_pdf: bool, is_text_pdf: bool) -> str:
        """è·å–å¤„ç†ç±»å‹æ ‡è¯†"""
        if is_ocr or (is_pdf and not is_text_pdf):
            return 'ocr_enhanced'
        elif is_pdf and is_text_pdf:
            return 'pdf_optimized'
        else:
            return 'standard'

    async def _generate_optimized_embeddings(self, texts: List[str], metadatas: List[Dict]) -> np.ndarray:
        """æ ¹æ®æ–‡æ¡£ç±»å‹ç”Ÿæˆä¼˜åŒ–çš„åµŒå…¥å‘é‡"""
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰ç‰¹æ®Šç±»å‹çš„æ–‡æ¡£éœ€è¦å¢å¼ºå¤„ç†
            has_ocr_docs = any(meta.get('processing_applied') == 'ocr_enhanced' for meta in metadatas)
            
            if has_ocr_docs:
                # å¯¹OCRæ–‡æ¡£ä½¿ç”¨æ›´robustçš„åµŒå…¥è®¾ç½®
                loop = asyncio.get_event_loop()
                embeddings = await loop.run_in_executor(
                    self.executor,
                    lambda: self.embedding_model.encode(
                        texts, 
                        normalize_embeddings=True,
                        batch_size=16,  # å‡å°æ‰¹æ¬¡å¤§å°æé«˜ç¨³å®šæ€§
                        show_progress_bar=False
                    )
                )
            else:
                # æ ‡å‡†åµŒå…¥å¤„ç†
                embeddings = await self._generate_embeddings_batch(texts)
            
            # ç¡®ä¿å‘é‡è´¨é‡
            norms = np.linalg.norm(embeddings, axis=1, keepdims=True)
            norms[norms == 0] = 1  # é¿å…é™¤é›¶
            embeddings = embeddings / norms
            
            return embeddings
            
        except Exception as e:
            logger.error(f"ä¼˜åŒ–åµŒå…¥ç”Ÿæˆå¤±è´¥ï¼Œå›é€€åˆ°æ ‡å‡†æ–¹æ³•: {e}")
            return await self._generate_embeddings_batch(texts)
    
    async def _generate_embedding(self, text: str) -> np.ndarray:
        """ç”Ÿæˆå•ä¸ªæ–‡æœ¬çš„åµŒå…¥å‘é‡"""
        loop = asyncio.get_event_loop()
        embedding = await loop.run_in_executor(
            self.executor,
            lambda: self.embedding_model.encode(text, normalize_embeddings=True)
        )
        # ç¡®ä¿å‘é‡è¢«å½’ä¸€åŒ–
        norm = np.linalg.norm(embedding)
        if norm > 0:
            embedding = embedding / norm
        return embedding
    
    async def _generate_embeddings_batch(self, texts: List[str]) -> np.ndarray:
        """æ‰¹é‡ç”ŸæˆåµŒå…¥å‘é‡"""
        loop = asyncio.get_event_loop()
        embeddings = await loop.run_in_executor(
            self.executor,
            lambda: self.embedding_model.encode(texts, normalize_embeddings=True)
        )
        # ç¡®ä¿æ‰€æœ‰å‘é‡è¢«å½’ä¸€åŒ–
        norms = np.linalg.norm(embeddings, axis=1, keepdims=True)
        norms[norms == 0] = 1  # é¿å…é™¤é›¶
        embeddings = embeddings / norms
        return embeddings
    
    def _generate_doc_id(self, content: str, filename: str) -> str:
        """ç”Ÿæˆæ–‡æ¡£å”¯ä¸€æ ‡è¯†"""
        # ä½¿ç”¨å†…å®¹å’Œæ–‡ä»¶åçš„å“ˆå¸Œä½œä¸ºæ–‡æ¡£ID
        content_hash = hashlib.md5(
            (content + filename).encode('utf-8')
        ).hexdigest()
        return f"doc_{content_hash[:16]}"
    
    async def _load_document_kb_mapping_from_db(self):
        """ä»æ•°æ®åº“åŠ è½½æ–‡æ¡£-çŸ¥è¯†åº“å…³è”å…³ç³»"""
        try:
            if self._mapping_loaded:
                return
                
            self.document_kb_mapping.clear()
            
            async with self.db.get_connection() as db:
                cursor = await db.execute("""
                    SELECT doc_id, knowledge_base_id 
                    FROM knowledge_base_documents
                    ORDER BY doc_id, added_at
                """)
                
                rows = await cursor.fetchall()
                
                for row in rows:
                    doc_id, kb_id = row
                    
                    if doc_id not in self.document_kb_mapping:
                        self.document_kb_mapping[doc_id] = []
                    
                    if kb_id not in self.document_kb_mapping[doc_id]:
                        self.document_kb_mapping[doc_id].append(kb_id)
                
                self._mapping_loaded = True
                logger.info(f"ä»æ•°æ®åº“åŠ è½½äº† {len(self.document_kb_mapping)} ä¸ªæ–‡æ¡£çš„çŸ¥è¯†åº“å…³è”å…³ç³»")
                
        except Exception as e:
            logger.error(f"ä»æ•°æ®åº“åŠ è½½æ–‡æ¡£-çŸ¥è¯†åº“å…³è”å…³ç³»å¤±è´¥: {e}")
            # ç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªç©ºçš„æ˜ å°„ï¼Œé¿å…åç»­æ“ä½œå¤±è´¥
            self.document_kb_mapping = {}
            self._mapping_loaded = True
    
    async def _ensure_mapping_loaded(self):
        """ç¡®ä¿æ˜ å°„å…³ç³»å·²åŠ è½½"""
        if not self._mapping_loaded:
            await self._load_document_kb_mapping_from_db()
    
    # çŸ¥è¯†åº“ç®¡ç†æ–¹æ³• - ä½¿ç”¨æ•°æ®åº“å­˜å‚¨ï¼ˆä¸ç»‘å®šç”¨æˆ·ï¼‰
    async def create_knowledge_base(self, name: str, description: str = None, color: str = "#3B82F6") -> Dict:
        """åˆ›å»ºçŸ¥è¯†åº“"""
        kb_id = str(uuid.uuid4())
        now = datetime.now()
        
        try:
            async with self.db.get_connection() as db:
                # ä½¿ç”¨é»˜è®¤ç”¨æˆ·IDï¼ˆ0è¡¨ç¤ºå…¨å±€ï¼‰
                await db.execute("""
                    INSERT INTO knowledge_bases (id, name, description, color, document_count, created_at, updated_at)
                    VALUES (?, ?, ?, ?, ?, ?, ?)
                """, (kb_id, name, description, color, 0, now, now))
                await db.commit()
            
            knowledge_base = {
                "id": kb_id,
                "name": name,
                "description": description,
                "color": color,
                "document_count": 0,
                "created_at": now,
                "updated_at": now
            }
            
            logger.info(f"åˆ›å»ºçŸ¥è¯†åº“æˆåŠŸ: {name} (ID: {kb_id})")
            return knowledge_base
            
        except Exception as e:
            logger.error(f"åˆ›å»ºçŸ¥è¯†åº“å¤±è´¥: {e}")
            raise
        
    async def get_all_knowledge_bases(self) -> List[Dict]:
        """è·å–æ‰€æœ‰çŸ¥è¯†åº“"""
        try:
            async with self.db.get_connection() as db:
                cursor = await db.execute("""
                    SELECT id, name, description, color, document_count, created_at, updated_at
                    FROM knowledge_bases
                    ORDER BY created_at DESC
                """)
                
                rows = await cursor.fetchall()
                
                knowledge_bases = []
                for row in rows:
                    kb = {
                        "id": row[0],
                        "name": row[1],
                        "description": row[2],
                        "color": row[3],
                        "document_count": row[4],
                        "created_at": row[5],
                        "updated_at": row[6]
                    }
                    knowledge_bases.append(kb)
                
                return knowledge_bases
                
        except Exception as e:
            logger.error(f"è·å–çŸ¥è¯†åº“åˆ—è¡¨å¤±è´¥: {e}")
            return []
        
    async def get_knowledge_base(self, kb_id: str) -> Optional[Dict]:
        """è·å–å•ä¸ªçŸ¥è¯†åº“"""
        try:
            async with self.db.get_connection() as db:
                cursor = await db.execute("""
                    SELECT id, name, description, color, document_count, created_at, updated_at
                    FROM knowledge_bases
                    WHERE id = ? 
                """, (kb_id,))
                row = await cursor.fetchone()
                
                if row:
                    return {
                        "id": row[0],
                        "name": row[1],
                        "description": row[2],
                        "color": row[3],
                        "document_count": row[4],
                        "created_at": row[5],
                        "updated_at": row[6]
                    }
                return None
                
        except Exception as e:
            logger.error(f"è·å–çŸ¥è¯†åº“å¤±è´¥: {e}")
            return None
        
    async def update_knowledge_base(self, kb_id: str, name: str = None, description: str = None, color: str = None) -> bool:
        """æ›´æ–°çŸ¥è¯†åº“"""
        try:
            # æ„å»ºæ›´æ–°å­—æ®µ
            update_fields = []
            update_values = []
            
            if name is not None:
                update_fields.append("name = ?")
                update_values.append(name)
            if description is not None:
                update_fields.append("description = ?")
                update_values.append(description)
            if color is not None:
                update_fields.append("color = ?")
                update_values.append(color)
            
            if not update_fields:
                return True  # æ²¡æœ‰éœ€è¦æ›´æ–°çš„å­—æ®µ
            
            update_fields.append("updated_at = ?")
            update_values.append(datetime.now())
            update_values.append(kb_id)
            
            async with self.db.get_connection() as db:
                cursor = await db.execute(f"""
                    UPDATE knowledge_bases 
                    SET {', '.join(update_fields)}
                    WHERE id = ?
                """, update_values)
                
                await db.commit()
                
                if cursor.rowcount > 0:
                    logger.info(f"æ›´æ–°çŸ¥è¯†åº“æˆåŠŸ: ID={kb_id}")
                    return True
                else:
                    logger.warning(f"çŸ¥è¯†åº“ä¸å­˜åœ¨: ID={kb_id}")
                    return False
                    
        except Exception as e:
            logger.error(f"æ›´æ–°çŸ¥è¯†åº“å¤±è´¥: {e}")
            return False
        
    async def delete_knowledge_base(self, kb_id: str) -> bool:
        """åˆ é™¤çŸ¥è¯†åº“"""
        try:
            async with self.db.get_connection() as db:
                # é¦–å…ˆè·å–çŸ¥è¯†åº“åç§°ç”¨äºæ—¥å¿—
                cursor = await db.execute("""
                    SELECT name FROM knowledge_bases 
                    WHERE id = ?
                """, (kb_id,))
                
                row = await cursor.fetchone()
                kb_name = row[0] if row else "æœªçŸ¥"
                
                # åˆ é™¤çŸ¥è¯†åº“
                cursor = await db.execute("""
                    DELETE FROM knowledge_bases 
                    WHERE id = ?
                """, (kb_id,))
                
                await db.commit()
                
                if cursor.rowcount > 0:
                    # ä»æ•°æ®åº“åˆ é™¤å…³è”å…³ç³»
                    await db.execute("""
                        DELETE FROM knowledge_base_documents 
                        WHERE knowledge_base_id = ?
                    """, (kb_id,))
                    
                    # æ¸…ç†å†…å­˜ä¸­çš„æ–‡æ¡£å…³è”å…³ç³»
                    if hasattr(self, '_mapping_loaded') and self._mapping_loaded:
                        for doc_id in list(self.document_kb_mapping.keys()):
                            if kb_id in self.document_kb_mapping[doc_id]:
                                self.document_kb_mapping[doc_id].remove(kb_id)
                                if not self.document_kb_mapping[doc_id]:
                                    del self.document_kb_mapping[doc_id]
                    
                    logger.info(f"åˆ é™¤çŸ¥è¯†åº“æˆåŠŸ: {kb_name} (ID: {kb_id})")
                    return True
                else:
                    logger.warning(f"çŸ¥è¯†åº“ä¸å­˜åœ¨: ID={kb_id}")
                    return False
                    
        except Exception as e:
            logger.error(f"åˆ é™¤çŸ¥è¯†åº“å¤±è´¥: {e}")
            return False
        
    async def add_documents_to_knowledge_base(self, kb_id: str, doc_ids: List[str]) -> bool:
        """å°†æ–‡æ¡£æ·»åŠ åˆ°çŸ¥è¯†åº“"""
        try:
            # ç¡®ä¿æ˜ å°„å…³ç³»å·²åŠ è½½
            await self._ensure_mapping_loaded()
            
            # éªŒè¯çŸ¥è¯†åº“æ˜¯å¦å­˜åœ¨
            kb = await self.get_knowledge_base(kb_id)
            if not kb:
                logger.warning(f"çŸ¥è¯†åº“ä¸å­˜åœ¨: ID={kb_id}")
                return False
            
            # æ£€æŸ¥æ–‡æ¡£æ˜¯å¦å­˜åœ¨
            existing_docs = await self.get_all_documents()
            existing_doc_ids = {doc["doc_id"] for doc in existing_docs}
            
            added_count = 0
            async with self.db.get_connection() as db:
                for doc_id in doc_ids:
                    if doc_id in existing_doc_ids:
                        # æ£€æŸ¥æ˜¯å¦å·²ç»å­˜åœ¨å…³è”å…³ç³»
                        if doc_id not in self.document_kb_mapping:
                            self.document_kb_mapping[doc_id] = []
                        
                        if kb_id not in self.document_kb_mapping[doc_id]:
                            try:
                                # æ’å…¥åˆ°æ•°æ®åº“
                                await db.execute("""
                                    INSERT INTO knowledge_base_documents (knowledge_base_id, doc_id, added_at)
                                    VALUES (?, ?, ?)
                                """, (kb_id, doc_id, datetime.now()))
                                
                                # æ›´æ–°å†…å­˜ç¼“å­˜
                                self.document_kb_mapping[doc_id].append(kb_id)
                                added_count += 1
                                
                            except Exception as e:
                                # å¯èƒ½æ˜¯é‡å¤æ’å…¥ï¼Œå¿½ç•¥
                                if "UNIQUE constraint failed" not in str(e):
                                    logger.warning(f"æ’å…¥å…³è”å…³ç³»å¤±è´¥ {doc_id}->{kb_id}: {e}")
                
                # æ›´æ–°çŸ¥è¯†åº“çš„æ–‡æ¡£è®¡æ•°
                if added_count > 0:
                    await db.execute("""
                        UPDATE knowledge_bases 
                        SET document_count = document_count + ?, updated_at = ?
                        WHERE id = ?
                    """, (added_count, datetime.now(), kb_id))
                
                await db.commit()
            
            kb_name = kb["name"]
            logger.info(f"å‘çŸ¥è¯†åº“ '{kb_name}' æ·»åŠ äº† {added_count} ä¸ªæ–‡æ¡£")
            return True
            
        except Exception as e:
            logger.error(f"æ·»åŠ æ–‡æ¡£åˆ°çŸ¥è¯†åº“å¤±è´¥: {e}")
            return False
        
    async def remove_documents_from_knowledge_base(self, kb_id: str, doc_ids: List[str]) -> bool:
        """ä»çŸ¥è¯†åº“ä¸­ç§»é™¤æ–‡æ¡£"""
        try:
            # ç¡®ä¿æ˜ å°„å…³ç³»å·²åŠ è½½
            await self._ensure_mapping_loaded()
            
            # éªŒè¯çŸ¥è¯†åº“æ˜¯å¦å­˜åœ¨
            kb = await self.get_knowledge_base(kb_id)
            if not kb:
                logger.warning(f"çŸ¥è¯†åº“ä¸å­˜åœ¨: ID={kb_id}")
                return False
            
            removed_count = 0
            async with self.db.get_connection() as db:
                for doc_id in doc_ids:
                    if doc_id in self.document_kb_mapping and kb_id in self.document_kb_mapping[doc_id]:
                        try:
                            # ä»æ•°æ®åº“åˆ é™¤å…³è”å…³ç³»
                            cursor = await db.execute("""
                                DELETE FROM knowledge_base_documents 
                                WHERE knowledge_base_id = ? AND doc_id = ?
                            """, (kb_id, doc_id))
                            
                            if cursor.rowcount > 0:
                                # æ›´æ–°å†…å­˜ç¼“å­˜
                                self.document_kb_mapping[doc_id].remove(kb_id)
                                if not self.document_kb_mapping[doc_id]:
                                    del self.document_kb_mapping[doc_id]
                                removed_count += 1
                                
                        except Exception as e:
                            logger.warning(f"åˆ é™¤å…³è”å…³ç³»å¤±è´¥ {doc_id}->{kb_id}: {e}")
                
                # æ›´æ–°çŸ¥è¯†åº“çš„æ–‡æ¡£è®¡æ•°
                if removed_count > 0:
                    await db.execute("""
                        UPDATE knowledge_bases 
                        SET document_count = GREATEST(0, document_count - ?), updated_at = ?
                        WHERE id = ?
                    """, (removed_count, datetime.now(), kb_id))
                
                await db.commit()
            
            kb_name = kb["name"]
            logger.info(f"ä»çŸ¥è¯†åº“ '{kb_name}' ç§»é™¤äº† {removed_count} ä¸ªæ–‡æ¡£")
            return True
            
        except Exception as e:
            logger.error(f"ä»çŸ¥è¯†åº“ç§»é™¤æ–‡æ¡£å¤±è´¥: {e}")
            return False
        
    async def get_knowledge_base_documents(self, kb_id: str) -> List[str]:
        """è·å–çŸ¥è¯†åº“çš„æ‰€æœ‰æ–‡æ¡£ID"""
        try:
            # ç¡®ä¿æ˜ å°„å…³ç³»å·²åŠ è½½
            await self._ensure_mapping_loaded()
            
            # éªŒè¯çŸ¥è¯†åº“æ˜¯å¦å­˜åœ¨
            kb = await self.get_knowledge_base(kb_id)
            if not kb:
                logger.warning(f"çŸ¥è¯†åº“ä¸å­˜åœ¨: ID={kb_id}")
                return []
            
            # ä»å†…å­˜ç¼“å­˜è·å–ï¼ˆå·²ä»æ•°æ®åº“åŠ è½½ï¼‰
            return [doc_id for doc_id, kb_ids in self.document_kb_mapping.items() if kb_id in kb_ids]
            
        except Exception as e:
            logger.error(f"è·å–çŸ¥è¯†åº“æ–‡æ¡£å¤±è´¥: {e}")
            return []

    async def analyze_document_for_archive(
        self, 
        file_content: bytes, 
        filename: str, 
        file_type: str,
        analysis_prompt: str,
        custom_analysis: bool = False
    ) -> Dict:
        """
        åˆ†ææ–‡æ¡£å†…å®¹å¹¶ä¿å­˜åˆ°å‘é‡æ•°æ®åº“ï¼Œé¢„è§ˆå½’æ¡£å»ºè®®
        
        Args:
            file_content: æ–‡ä»¶å†…å®¹ï¼ˆå­—èŠ‚ï¼‰
            filename: æ–‡ä»¶å
            file_type: æ–‡ä»¶ç±»å‹
            analysis_prompt: åˆ†ææç¤ºè¯
            custom_analysis: æ˜¯å¦ä½¿ç”¨è‡ªå®šä¹‰åˆ†æ
            
        Returns:
            åˆ†æç»“æœä¿¡æ¯ï¼ˆåŒ…å«å®é™…çš„docIdï¼‰
        """
        try:
            # æå–æ–‡æ¡£å†…å®¹
            text_content = await self._extract_text_from_file(file_content, filename, file_type)
            
            # ğŸš€ ä¼˜åŒ–ï¼šåœ¨åˆ†æé˜¶æ®µå°±ä¿å­˜æ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“
            doc_id = await self.process_document(text_content, filename, file_type)
            logger.info(f"æ–‡æ¡£å·²ä¿å­˜åˆ°å‘é‡æ•°æ®åº“: {filename} (doc_id: {doc_id})")
            
            # ä½¿ç”¨AIåˆ†ææ–‡æ¡£å†…å®¹å¹¶åŒ¹é…çŸ¥è¯†åº“
            analysis_result = await self._analyze_document_content(
                content=text_content,
                filename=filename,
                analysis_prompt=analysis_prompt,
                custom_analysis=custom_analysis
            )
            
            knowledge_base_name = analysis_result['knowledge_base_name']
            is_new_kb = analysis_result['is_new_knowledge_base']
            reason = analysis_result.get('reason', '')
            
            # å¦‚æœä¸æ˜¯æ–°å»ºçŸ¥è¯†åº“ï¼Œæ£€æŸ¥ç°æœ‰çŸ¥è¯†åº“æ˜¯å¦å­˜åœ¨
            kb_id = None
            if not is_new_kb:
                all_kbs = await self.get_all_knowledge_bases()
                for kb in all_kbs:
                    if kb['name'] == knowledge_base_name:
                        kb_id = kb['id']
                        break
                
                # å¦‚æœæ²¡æ‰¾åˆ°åŒ¹é…çš„çŸ¥è¯†åº“ï¼Œæ ‡è®°ä¸ºæ–°å»º
                if not kb_id:
                    is_new_kb = True
            
            result = {
                "fileName": filename,
                "knowledgeBaseName": knowledge_base_name,
                "isNewKnowledgeBase": is_new_kb,
                "reason": reason,
                "knowledgeBaseId": kb_id,
                "documentType": analysis_result.get('document_type', 'æœªçŸ¥'),
                "textContent": text_content[:500] + "..." if len(text_content) > 500 else text_content,  # é¢„è§ˆå†…å®¹
                "docId": doc_id,  # âœ¨ æ–°å¢ï¼šè¿”å›æ–‡æ¡£ID
                "analysisTime": datetime.now().timestamp()
            }
            
            logger.info(f"æ–‡æ¡£åˆ†æå’Œä¿å­˜å®Œæˆ: {filename} -> {knowledge_base_name} ({'æ–°å»º' if is_new_kb else 'ç°æœ‰'}), doc_id: {doc_id}")
            return result
            
        except Exception as e:
            logger.error(f"æ–‡æ¡£åˆ†æå¤±è´¥ {filename}: {e}")
            raise

    async def confirm_archive_document(
        self,
        file_content: str,  # å·²å¼ƒç”¨ï¼Œä¿ç•™ç”¨äºå…¼å®¹æ€§
        filename: str,
        file_type: str,
        analysis_result: Dict
    ) -> Dict:
        """
        ç¡®è®¤å½’æ¡£æ–‡æ¡£ï¼Œä»…æ‰§è¡Œæ–‡æ¡£ä¸çŸ¥è¯†åº“çš„å…³è”æ“ä½œ
        ï¼ˆæ–‡æ¡£åœ¨analyzeé˜¶æ®µå·²ä¿å­˜åˆ°å‘é‡æ•°æ®åº“ï¼‰
        
        Args:
            file_content: å·²å¼ƒç”¨ï¼Œæ–‡æ¡£å·²åœ¨åˆ†æé˜¶æ®µä¿å­˜
            filename: æ–‡ä»¶å
            file_type: æ–‡ä»¶ç±»å‹  
            analysis_result: åˆ†æç»“æœï¼ˆåŒ…å«doc_idï¼‰
            
        Returns:
            å½’æ¡£ç»“æœä¿¡æ¯
        """
        try:
            # ğŸš€ ä¼˜åŒ–ï¼šä»åˆ†æç»“æœç›´æ¥è·å–doc_idï¼Œä¸å†é‡å¤å¤„ç†æ–‡æ¡£
            doc_id = analysis_result.get('docId')
            if not doc_id:
                raise ValueError("åˆ†æç»“æœä¸­ç¼ºå°‘æ–‡æ¡£IDï¼Œè¯·é‡æ–°åˆ†ææ–‡æ¡£")
            
            knowledge_base_name = analysis_result['knowledgeBaseName']
            is_new_kb = analysis_result['isNewKnowledgeBase']
            reason = analysis_result.get('reason', '')
            kb_id = analysis_result.get('knowledgeBaseId')
            
            # è·å–æˆ–åˆ›å»ºçŸ¥è¯†åº“
            if is_new_kb or not kb_id:
                kb = await self.create_knowledge_base(
                    name=knowledge_base_name,
                    description=f"ç”±AIæ™ºèƒ½åˆ†æåˆ›å»ºçš„çŸ¥è¯†åº“ï¼Œç”¨äºå­˜å‚¨{analysis_result.get('documentType', 'ç›¸å…³')}ç±»å‹çš„æ–‡æ¡£",
                    color=self._get_random_color()
                )
                kb_id = kb['id']
                logger.info(f"åˆ›å»ºæ–°çŸ¥è¯†åº“: {knowledge_base_name} (ID: {kb_id})")
            else:
                # éªŒè¯çŸ¥è¯†åº“æ˜¯å¦ä»ç„¶å­˜åœ¨
                try:
                    all_kbs = await self.get_all_knowledge_bases()
                    kb_exists = any(kb['id'] == kb_id for kb in all_kbs)
                    if not kb_exists:
                        # çŸ¥è¯†åº“ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°çš„
                        kb = await self.create_knowledge_base(
                            name=knowledge_base_name,
                            description=f"æ™ºèƒ½å½’æ¡£åˆ›å»ºçš„çŸ¥è¯†åº“",
                            color=self._get_random_color()
                        )
                        kb_id = kb['id']
                        is_new_kb = True
                        logger.info(f"åŸçŸ¥è¯†åº“ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°çŸ¥è¯†åº“: {knowledge_base_name}")
                except Exception as e:
                    logger.warning(f"éªŒè¯çŸ¥è¯†åº“æ—¶å‡ºé”™: {e}ï¼Œåˆ›å»ºæ–°çŸ¥è¯†åº“")
                    kb = await self.create_knowledge_base(
                        name=knowledge_base_name,
                        description=f"æ™ºèƒ½å½’æ¡£åˆ›å»ºçš„çŸ¥è¯†åº“",
                        color=self._get_random_color()
                    )
                    kb_id = kb['id']
                    is_new_kb = True
            
            # âš¡ æ ¸å¿ƒï¼šå°†å·²ä¿å­˜çš„æ–‡æ¡£å…³è”åˆ°çŸ¥è¯†åº“
            await self.add_documents_to_knowledge_base(kb_id, [doc_id])
            
            result = {
                "fileName": filename,
                "knowledgeBaseName": knowledge_base_name,
                "isNewKnowledgeBase": is_new_kb,
                "reason": reason,
                "docId": doc_id,
                "knowledgeBaseId": kb_id
            }
            
            logger.info(f"ç¡®è®¤å½’æ¡£å®Œæˆï¼ˆä»…å…³è”ï¼‰: {filename} -> {knowledge_base_name} ({'æ–°å»º' if is_new_kb else 'ç°æœ‰'}), doc_id: {doc_id}")
            return result
            
        except Exception as e:
            logger.error(f"ç¡®è®¤å½’æ¡£å¤±è´¥ {filename}: {e}")
            raise

    
    async def _extract_text_from_file(self, file_content: bytes, filename: str, file_type: str) -> str:
        """ä»æ–‡ä»¶ä¸­æå–æ–‡æœ¬å†…å®¹"""
        try:
            from app.services.file_extraction_service import file_extraction_service
            
            # ä½¿ç”¨ç»Ÿä¸€çš„æ–‡ä»¶æå–æœåŠ¡
            text, metadata = await file_extraction_service.extract_text_from_file(
                file_content, filename, file_type
            )
            
            # è®°å½•æå–å…ƒæ•°æ®åˆ°æ—¥å¿—
            extraction_method = metadata.get('extraction_method', 'unknown')
            processing_time = metadata.get('extraction_time', 0)
            confidence = metadata.get('confidence', None)
            
            log_msg = f"æ–‡ä»¶æå–å®Œæˆ: {filename}, æ–¹æ³•: {extraction_method}, è€—æ—¶: {processing_time:.2f}ç§’"
            if confidence is not None:
                log_msg += f", ç½®ä¿¡åº¦: {confidence:.2f}"
            
            logger.info(log_msg)
            
            return text
                
        except Exception as e:
            logger.error(f"æ–‡æœ¬æå–å¤±è´¥ {filename}: {e}")
            return f"æ–‡æ¡£: {filename}\nå†…å®¹æå–å¤±è´¥: {str(e)}"
    
    async def _analyze_document_content(
        self, 
        content: str, 
        filename: str, 
        analysis_prompt: str,
        custom_analysis: bool = False
    ) -> Dict:
        """
        ä½¿ç”¨çœŸæ­£çš„LLMåˆ†ææ–‡æ¡£å†…å®¹å¹¶å†³å®šå½’æ¡£ä½ç½®
        
        å¯¹äºç‰¹å¤§æ–‡æ¡£é‡‡ç”¨æ™ºèƒ½å¤„ç†ç­–ç•¥ï¼š
        1. å°æ–‡æ¡£(<5000å­—ç¬¦)ï¼šç›´æ¥å…¨æ–‡åˆ†æ
        2. ä¸­ç­‰æ–‡æ¡£(5000-20000å­—ç¬¦)ï¼šæå–å…³é”®æ®µè½åˆ†æ
        3. å¤§æ–‡æ¡£(20000-100000å­—ç¬¦)ï¼šåˆ†æ®µæ‘˜è¦åˆ†æ
        4. ç‰¹å¤§æ–‡æ¡£(>100000å­—ç¬¦)ï¼šæ™ºèƒ½é‡‡æ ·+å…³é”®ä¿¡æ¯æå–
        """
        try:
            # æ£€æŸ¥ç¼“å­˜
            content_hash = hashlib.md5((content + filename + analysis_prompt).encode('utf-8')).hexdigest()
            if content_hash in self.analysis_cache:
                logger.info(f"ä½¿ç”¨ç¼“å­˜çš„åˆ†æç»“æœ: {filename}")
                return self.analysis_cache[content_hash]
            
            content_length = len(content)
            logger.info(f"å¼€å§‹åˆ†ææ–‡æ¡£: {filename}, å¤§å°: {content_length} å­—ç¬¦")
            
            # æ ¹æ®æ–‡æ¡£å¤§å°é€‰æ‹©å¤„ç†ç­–ç•¥
            if content_length < 5000:
                # å°æ–‡æ¡£ï¼šç›´æ¥å…¨æ–‡åˆ†æ
                analysis_content = content
                processing_strategy = "direct_analysis"
            elif content_length < 20000:
                # ä¸­ç­‰æ–‡æ¡£ï¼šæå–å…³é”®æ®µè½
                analysis_content = await self._extract_key_paragraphs(content, filename)
                processing_strategy = "key_paragraphs"
            elif content_length < 100000:
                # å¤§æ–‡æ¡£ï¼šåˆ†æ®µæ‘˜è¦
                analysis_content = await self._create_document_summary(content, filename)
                processing_strategy = "segment_summary"
            else:
                # ç‰¹å¤§æ–‡æ¡£ï¼šæ™ºèƒ½é‡‡æ ·
                analysis_content = await self._intelligent_sampling(content, filename)
                processing_strategy = "intelligent_sampling"
            
            logger.info(f"æ–‡æ¡£å¤„ç†ç­–ç•¥: {processing_strategy}, åˆ†æå†…å®¹é•¿åº¦: {len(analysis_content)}")
            
            # è°ƒç”¨LLMè¿›è¡Œåˆ†æ
            analysis_result = await self._call_llm_for_analysis(
                content=analysis_content,
                filename=filename,
                analysis_prompt=analysis_prompt,
                custom_analysis=custom_analysis,
                processing_strategy=processing_strategy
            )
            
            # ç¼“å­˜ç»“æœ
            self.analysis_cache[content_hash] = analysis_result
            
            # é™åˆ¶ç¼“å­˜å¤§å°
            if len(self.analysis_cache) > 100:
                # ç§»é™¤æœ€æ—§çš„ç¼“å­˜é¡¹
                oldest_key = next(iter(self.analysis_cache))
                del self.analysis_cache[oldest_key]
            
            logger.info(f"æ–‡æ¡£åˆ†æå®Œæˆ: {filename} -> {analysis_result['knowledge_base_name']}")
            return analysis_result
            
        except Exception as e:
            logger.error(f"æ–‡æ¡£åˆ†æå¤±è´¥: {e}")
    
    async def _extract_key_paragraphs(self, content: str, filename: str) -> str:
        """æå–å…³é”®æ®µè½ï¼ˆç”¨äºä¸­ç­‰æ–‡æ¡£ï¼‰"""
        try:
            lines = content.split('\n')
            
            # è·å–æ–‡æ¡£å¼€å¤´ï¼ˆå‰20%æˆ–æœ€å¤š500è¡Œï¼‰
            start_lines = int(min(len(lines) * 0.2, 500))
            beginning = '\n'.join(lines[:start_lines])
            
            # è·å–æ–‡æ¡£ç»“å°¾ï¼ˆå10%æˆ–æœ€å¤š200è¡Œï¼‰
            end_lines = int(min(len(lines) * 0.1, 200))
            ending = '\n'.join(lines[-end_lines:]) if end_lines > 0 else ""
            
            # æŸ¥æ‰¾åŒ…å«å…³é”®è¯çš„æ®µè½
            keywords = ['æ‘˜è¦', 'æ€»ç»“', 'æ¦‚è¿°', 'ç®€ä»‹', 'ç›®å½•', 'abstract', 'summary', 'introduction']
            key_paragraphs = []
            
            for i, line in enumerate(lines):
                line_lower = line.lower()
                if any(keyword in line_lower for keyword in keywords):
                    # åŒ…å«å…³é”®è¯çš„æ®µè½åŠå…¶å‘¨å›´å†…å®¹
                    start_idx = max(0, i - 2)
                    end_idx = min(len(lines), i + 5)
                    key_paragraphs.append('\n'.join(lines[start_idx:end_idx]))
            
            # ç»„åˆå†…å®¹
            extracted_content = f"æ–‡æ¡£å¼€å¤´ï¼š\n{beginning}\n\n"
            
            if key_paragraphs:
                extracted_content += f"å…³é”®æ®µè½ï¼š\n" + '\n\n'.join(key_paragraphs) + "\n\n"
            
            if ending:
                extracted_content += f"æ–‡æ¡£ç»“å°¾ï¼š\n{ending}"
            
            # å¦‚æœæå–çš„å†…å®¹å¤ªé•¿ï¼Œæˆªæ–­
            if len(extracted_content) > 8000:
                extracted_content = extracted_content[:8000] + "...\n[å†…å®¹å·²æˆªæ–­]"
            
            return extracted_content
            
        except Exception as e:
            logger.warning(f"æå–å…³é”®æ®µè½å¤±è´¥: {e}")
            return content[:5000] + "...\n[æå–å¤±è´¥ï¼Œä½¿ç”¨å¼€å¤´å†…å®¹]"
    
    async def _create_document_summary(self, content: str, filename: str) -> str:
        """åˆ›å»ºæ–‡æ¡£æ‘˜è¦ï¼ˆç”¨äºå¤§æ–‡æ¡£ï¼‰"""
        try:
            # å°†æ–‡æ¡£åˆ†æˆå¤šä¸ªæ®µè½
            paragraphs = content.split('\n\n')
            
            # æ¯æ®µæœ€å¤š1000å­—ç¬¦
            segments = []
            current_segment = ""
            
            for paragraph in paragraphs:
                if len(current_segment) + len(paragraph) < 1000:
                    current_segment += paragraph + '\n\n'
                else:
                    if current_segment:
                        segments.append(current_segment.strip())
                    current_segment = paragraph + '\n\n'
            
            if current_segment:
                segments.append(current_segment.strip())
            
            # é€‰æ‹©å…³é”®æ®µè½è¿›è¡Œæ‘˜è¦
            key_segments = []
            
            # æ€»æ˜¯åŒ…å«å¼€å¤´å’Œç»“å°¾
            if segments:
                key_segments.append(segments[0])  # å¼€å¤´
                if len(segments) > 1:
                    key_segments.append(segments[-1])  # ç»“å°¾
            
            # æ·»åŠ ä¸­é—´çš„å…³é”®æ®µè½
            for segment in segments[1:-1]:
                segment_lower = segment.lower()
                # æŸ¥æ‰¾åŒ…å«é‡è¦ä¿¡æ¯çš„æ®µè½
                if any(keyword in segment_lower for keyword in [
                    'æ‘˜è¦', 'æ€»ç»“', 'ç»“è®º', 'æ¦‚è¿°', 'ç›®çš„', 'ç›®æ ‡', 'æ–¹æ³•', 'ç»“æœ',
                    'abstract', 'summary', 'conclusion', 'objective', 'purpose', 'method', 'result'
                ]):
                    key_segments.append(segment)
                    if len(key_segments) >= 5:  # é™åˆ¶æ®µè½æ•°é‡
                        break
            
            # å¦‚æœå…³é”®æ®µè½ä¸å¤Ÿï¼Œéšæœºé€‰æ‹©ä¸€äº›æ®µè½
            if len(key_segments) < 3 and len(segments) > 2:
                import random
                remaining_segments = [s for s in segments[1:-1] if s not in key_segments]
                additional_count = min(3 - len(key_segments), len(remaining_segments))
                key_segments.extend(random.sample(remaining_segments, additional_count))
            
            # ç»„åˆæ‘˜è¦
            summary = f"æ–‡æ¡£æ‘˜è¦ ({len(segments)}ä¸ªæ®µè½ä¸­çš„{len(key_segments)}ä¸ªå…³é”®æ®µè½)ï¼š\n\n"
            summary += '\n\n---æ®µè½åˆ†éš”---\n\n'.join(key_segments)
            
            # é™åˆ¶æ€»é•¿åº¦
            if len(summary) > 6000:
                summary = summary[:6000] + "...\n[æ‘˜è¦å·²æˆªæ–­]"
            
            return summary
            
        except Exception as e:
            logger.warning(f"åˆ›å»ºæ–‡æ¡£æ‘˜è¦å¤±è´¥: {e}")
            return content[:3000] + "...\n[æ‘˜è¦å¤±è´¥ï¼Œä½¿ç”¨å¼€å¤´å†…å®¹]"
    
    async def _intelligent_sampling(self, content: str, filename: str) -> str:
        """æ™ºèƒ½é‡‡æ ·ï¼ˆç”¨äºç‰¹å¤§æ–‡æ¡£ï¼‰"""
        try:
            total_length = len(content)
            
            # æå–å¼€å¤´ã€ä¸­é—´ã€ç»“å°¾çš„æ ·æœ¬
            start_sample = content[:2000]  # å¼€å¤´2000å­—ç¬¦
            end_sample = content[-1000:]   # ç»“å°¾1000å­—ç¬¦
            
            # ä¸­é—´é‡‡æ ·ï¼šæ¯éš”ä¸€å®šé—´è·é‡‡æ ·
            middle_samples = []
            sample_interval = max(1000, total_length // 20)  # æœ€å¤šé‡‡æ ·20ä¸ªç‰‡æ®µ
            
            for i in range(2000, total_length - 1000, sample_interval):
                sample = content[i:i+500]  # æ¯æ¬¡é‡‡æ ·500å­—ç¬¦
                middle_samples.append(sample)
                if len(middle_samples) >= 10:  # æœ€å¤š10ä¸ªä¸­é—´æ ·æœ¬
                    break
            
            # æŸ¥æ‰¾ç»“æ„åŒ–ä¿¡æ¯ï¼ˆæ ‡é¢˜ã€ç›®å½•ç­‰ï¼‰
            lines = content.split('\n')
            structured_info = []
            
            for line in lines:
                line_stripped = line.strip()
                if line_stripped and (
                    line_stripped.startswith('#') or  # Markdownæ ‡é¢˜
                    line_stripped.endswith(':') or   # å†’å·ç»“å°¾ï¼ˆå¯èƒ½æ˜¯æ ‡é¢˜ï¼‰
                    len(line_stripped) < 100 and (   # çŸ­è¡Œä¸”åŒ…å«å…³é”®è¯
                        any(keyword in line_stripped.lower() for keyword in [
                            'ç¬¬', 'ç« ', 'èŠ‚', 'éƒ¨åˆ†', 'chapter', 'section', 'part'
                        ])
                    )
                ):
                    structured_info.append(line_stripped)
                    if len(structured_info) >= 20:  # æœ€å¤š20ä¸ªç»“æ„åŒ–ä¿¡æ¯
                        break
            
            # ç»„åˆé‡‡æ ·ç»“æœ
            sampled_content = f"è¶…å¤§æ–‡æ¡£æ™ºèƒ½é‡‡æ · (æ€»é•¿åº¦: {total_length} å­—ç¬¦)ï¼š\n\n"
            sampled_content += f"æ–‡æ¡£å¼€å¤´ï¼š\n{start_sample}\n\n"
            
            if structured_info:
                sampled_content += f"æ–‡æ¡£ç»“æ„ï¼š\n" + '\n'.join(structured_info) + "\n\n"
            
            if middle_samples:
                sampled_content += f"ä¸­é—´é‡‡æ ·ï¼š\n" + '\n\n---é‡‡æ ·åˆ†éš”---\n\n'.join(middle_samples) + "\n\n"
            
            sampled_content += f"æ–‡æ¡£ç»“å°¾ï¼š\n{end_sample}"
            
            # é™åˆ¶æ€»é•¿åº¦
            if len(sampled_content) > 8000:
                sampled_content = sampled_content[:8000] + "...\n[é‡‡æ ·å·²æˆªæ–­]"
            
            return sampled_content
            
        except Exception as e:
            logger.warning(f"æ™ºèƒ½é‡‡æ ·å¤±è´¥: {e}")
            return content[:3000] + "...\n[é‡‡æ ·å¤±è´¥ï¼Œä½¿ç”¨å¼€å¤´å†…å®¹]"
    
    async def _call_llm_for_analysis(
        self,
        content: str,
        filename: str,
        analysis_prompt: str,
        custom_analysis: bool,
        processing_strategy: str
    ) -> Dict:
        """è°ƒç”¨LLMè¿›è¡Œæ–‡æ¡£å†…å®¹åˆ†æ"""
        try:
            # æ„å»ºç³»ç»Ÿæç¤ºè¯
            system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£åˆ†æä¸“å®¶ï¼Œè´Ÿè´£åˆ†ææ–‡æ¡£å†…å®¹å¹¶å†³å®šæœ€åˆé€‚çš„çŸ¥è¯†åº“å½’æ¡£ä½ç½®ã€‚

            ä½ çš„ä»»åŠ¡æ˜¯ï¼š
                1. åˆ†ææ–‡æ¡£çš„ä¸»è¦å†…å®¹å’Œç±»å‹
                2. ä»ä»¥ä¸‹é¢„è®¾çŸ¥è¯†åº“ä¸­é€‰æ‹©æœ€åˆé€‚çš„å½’æ¡£ä½ç½®ï¼š
                - ä¸ªäººç®€å†ï¼šç®€å†ã€CVã€ä¸ªäººèµ„æ–™ã€æ±‚èŒç›¸å…³
                - åˆåŒæ–‡æ¡£ï¼šåˆåŒã€åè®®ã€æ³•å¾‹æ–‡ä»¶
                - æ•™è‚²åŸ¹è®­ï¼šåŸ¹è®­ææ–™ã€è¯¾ç¨‹ã€æ•™è‚²å†…å®¹
                - æŠ€æœ¯æ–‡æ¡£ï¼šAPIæ–‡æ¡£ã€æŠ€æœ¯è§„èŒƒã€å¼€å‘èµ„æ–™
                - å•†åŠ¡æ–‡æ¡£ï¼šå•†ä¸šè®¡åˆ’ã€å¸‚åœºåˆ†æã€å•†åŠ¡èµ„æ–™
                - æ“ä½œæ‰‹å†Œï¼šç”¨æˆ·æ‰‹å†Œã€æ“ä½œæŒ‡å—ã€è¯´æ˜ä¹¦
                - åŒ»ç–—å¥åº·ï¼šåŒ»ç–—æŠ¥å‘Šã€å¥åº·èµ„æ–™ã€åŒ»å­¦æ–‡çŒ®
                - æ”¿ç­–æ³•è§„ï¼šæ”¿ç­–æ–‡ä»¶ã€æ³•è§„æ¡ä¾‹ã€è§„ç« åˆ¶åº¦

                3. å¦‚æœæ–‡æ¡£ä¸é€‚åˆä»»ä½•é¢„è®¾çŸ¥è¯†åº“ï¼Œå»ºè®®åˆ›å»ºæ–°çš„çŸ¥è¯†åº“

                è¯·è¿”å›JSONæ ¼å¼çš„åˆ†æç»“æœï¼š
                {
                    "knowledge_base_name": "çŸ¥è¯†åº“åç§°",
                    "is_new_knowledge_base": false,
                    "document_type": "æ–‡æ¡£ç±»å‹",
                    "reason": "é€‰æ‹©ç†ç”±",
                    "confidence": 0.85
                }"""

            # æ„å»ºç”¨æˆ·æç¤ºè¯
            user_prompt = f"""è¯·åˆ†æä»¥ä¸‹æ–‡æ¡£å¹¶å†³å®šæœ€åˆé€‚çš„çŸ¥è¯†åº“å½’æ¡£ä½ç½®ï¼š

                æ–‡ä»¶åï¼š{filename}
                ç”¨æˆ·æç¤ºï¼š{analysis_prompt}
                å¤„ç†ç­–ç•¥ï¼š{processing_strategy}

                æ–‡æ¡£å†…å®¹ï¼š
                {content}

                è¯·ä»”ç»†åˆ†ææ–‡æ¡£çš„ä¸»è¦å†…å®¹ã€ç±»å‹å’Œç”¨é€”ï¼Œç„¶åé€‰æ‹©æœ€åˆé€‚çš„çŸ¥è¯†åº“è¿›è¡Œå½’æ¡£ã€‚
                å¦‚æœç”¨æˆ·æä¾›äº†ç‰¹å®šçš„åˆ†ç±»å»ºè®®ï¼Œè¯·ä¼˜å…ˆè€ƒè™‘ã€‚"""

            # è°ƒç”¨LLM
            response = await self._make_llm_request(system_prompt, user_prompt)
            
            # è§£æLLMå“åº”
            analysis_result = await self._parse_llm_response(response)
            
            # éªŒè¯å’Œä¿®æ­£ç»“æœ
            return await self._validate_analysis_result(analysis_result, filename)
            
        except Exception as e:
            logger.error(f"LLMåˆ†æè°ƒç”¨å¤±è´¥: {e}")
    
    async def _make_llm_request(self, system_prompt: str, user_prompt: str) -> str:
        """å‘LM Studioå‘é€è¯·æ±‚"""
        try:
            async with httpx.AsyncClient(timeout=60.0) as client:
                payload = {
                    "model": settings.lm_studio_model,
                    "messages": [
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    "temperature": 0.3,  # è¾ƒä½çš„æ¸©åº¦ä»¥è·å¾—æ›´ä¸€è‡´çš„åˆ†æç»“æœ
                    "max_tokens": 1000,
                    "stream": False
                }
                
                response = await client.post(
                    f"{settings.lm_studio_base_url}/chat/completions",
                    json=payload,
                    headers={
                        "Content-Type": "application/json",
                        "Authorization": f"Bearer {settings.lm_studio_api_key}"
                    }
                )
                
                if response.status_code == 200:
                    result = response.json()
                    return result["choices"][0]["message"]["content"]
                else:
                    logger.error(f"LLMè¯·æ±‚å¤±è´¥: {response.status_code} - {response.text}")
                    raise Exception(f"LLMè¯·æ±‚å¤±è´¥: {response.status_code}")
                    
        except Exception as e:
            logger.error(f"LLMè¯·æ±‚å¼‚å¸¸: {e}")
            raise
    
    async def _parse_llm_response(self, response: str) -> Dict:
        """è§£æLLMå“åº”ï¼Œæå–JSONç»“æœ"""
        try:
            # å°è¯•ç›´æ¥è§£æJSON
            try:
                return json.loads(response)
            except json.JSONDecodeError:
                pass
            
            # å¦‚æœç›´æ¥è§£æå¤±è´¥ï¼Œå°è¯•æå–JSONéƒ¨åˆ†
            import re
            json_match = re.search(r'\{[^{}]*\}', response, re.DOTALL)
            if json_match:
                return json.loads(json_match.group())
            
            # å¦‚æœä»ç„¶å¤±è´¥ï¼Œä»æ–‡æœ¬ä¸­æå–ä¿¡æ¯
            knowledge_base_match = re.search(r'knowledge_base_name["\s]*:["\s]*([^"]*)', response)
            is_new_match = re.search(r'is_new_knowledge_base["\s]*:["\s]*(true|false)', response)
            doc_type_match = re.search(r'document_type["\s]*:["\s]*([^"]*)', response)
            reason_match = re.search(r'reason["\s]*:["\s]*([^"]*)', response)
            
            return {
                "knowledge_base_name": knowledge_base_match.group(1) if knowledge_base_match else "é€šç”¨æ–‡æ¡£",
                "is_new_knowledge_base": is_new_match.group(1).lower() == 'true' if is_new_match else False,
                "document_type": doc_type_match.group(1) if doc_type_match else "æœªçŸ¥",
                "reason": reason_match.group(1) if reason_match else "AIåˆ†æç»“æœ",
                "confidence": 0.5
            }
            
        except Exception as e:
            logger.warning(f"è§£æLLMå“åº”å¤±è´¥: {e}")
            return {
                "knowledge_base_name": "é€šç”¨æ–‡æ¡£",
                "is_new_knowledge_base": False,
                "document_type": "æœªçŸ¥",
                "reason": "è§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤åˆ†ç±»",
                "confidence": 0.3
            }
    
    async def _validate_analysis_result(self, result: Dict, filename: str) -> Dict:
        """éªŒè¯å’Œä¿®æ­£åˆ†æç»“æœ"""
        try:
            # ç¡®ä¿å¿…è¦å­—æ®µå­˜åœ¨
            if "knowledge_base_name" not in result:
                result["knowledge_base_name"] = "é€šç”¨æ–‡æ¡£"
            
            if "is_new_knowledge_base" not in result:
                result["is_new_knowledge_base"] = False
            
            if "document_type" not in result:
                result["document_type"] = "æœªçŸ¥"
            
            if "reason" not in result:
                result["reason"] = "AIæ™ºèƒ½åˆ†æç»“æœ"
            
            # æ£€æŸ¥çŸ¥è¯†åº“åç§°æ˜¯å¦åœ¨é¢„è®¾åˆ—è¡¨ä¸­
            preset_knowledge_bases = {
                "ä¸ªäººç®€å†", "åˆåŒæ–‡æ¡£", "æ•™è‚²åŸ¹è®­", "æŠ€æœ¯æ–‡æ¡£", 
                "å•†åŠ¡æ–‡æ¡£", "æ“ä½œæ‰‹å†Œ", "åŒ»ç–—å¥åº·", "æ”¿ç­–æ³•è§„"
            }
            
            kb_name = result["knowledge_base_name"]
            if kb_name not in preset_knowledge_bases:
                # å¦‚æœä¸åœ¨é¢„è®¾åˆ—è¡¨ä¸­ï¼Œæ ‡è®°ä¸ºæ–°çŸ¥è¯†åº“
                result["is_new_knowledge_base"] = True
                
                # ä½†å¦‚æœåç§°å¾ˆç›¸ä¼¼ï¼Œä¿®æ­£ä¸ºé¢„è®¾åç§°
                for preset in preset_knowledge_bases:
                    if any(word in kb_name for word in preset.split()) or any(word in preset for word in kb_name.split()):
                        result["knowledge_base_name"] = preset
                        result["is_new_knowledge_base"] = False
                        result["reason"] += f"ï¼ˆå·²ä¿®æ­£ä¸ºé¢„è®¾çŸ¥è¯†åº“ï¼š{preset}ï¼‰"
                        break
            
            return result
            
        except Exception as e:
            logger.warning(f"éªŒè¯åˆ†æç»“æœå¤±è´¥: {e}")
            return {
                "knowledge_base_name": "é€šç”¨æ–‡æ¡£",
                "is_new_knowledge_base": False,
                "document_type": "æœªçŸ¥",
                "reason": "éªŒè¯å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤åˆ†ç±»",
                "confidence": 0.2
            }
    
    def _extract_document_type(self, filename: str, content: str) -> str:
        """ä»æ–‡ä»¶åå’Œå†…å®¹ä¸­æå–æ–‡æ¡£ç±»å‹"""
        filename_lower = filename.lower()
        
        # ä»æ–‡ä»¶åæ¨æ–­ç±»å‹
        if 'report' in filename_lower or 'æŠ¥å‘Š' in filename_lower:
            return 'æŠ¥å‘Š'
        elif 'plan' in filename_lower or 'è®¡åˆ’' in filename_lower:
            return 'è®¡åˆ’'
        elif 'spec' in filename_lower or 'è§„èŒƒ' in filename_lower:
            return 'è§„èŒƒ'
        elif 'note' in filename_lower or 'ç¬”è®°' in filename_lower:
            return 'ç¬”è®°'
        elif 'summary' in filename_lower or 'æ€»ç»“' in filename_lower:
            return 'æ€»ç»“'
        else:
            return 'é€šç”¨'
    
    def _get_random_color(self) -> str:
        """è·å–éšæœºé¢œè‰²"""
        colors = [
            "#3B82F6", "#10B981", "#F59E0B", "#EF4444", 
            "#8B5CF6", "#06B6D4", "#84CC16", "#F97316",
            "#EC4899", "#6366F1"
        ]
        import random
        return random.choice(colors)

    async def analyze_existing_document_for_archive(
        self, 
        doc_id: str,
        filename: str, 
        file_type: str,
        text_content: str,
        analysis_prompt: str,
        custom_analysis: bool = False
    ) -> Dict:
        """
        åˆ†æå·²æœ‰æ–‡æ¡£è¿›è¡Œæ™ºèƒ½å½’æ¡£å»ºè®®
        
        Args:
            doc_id: æ–‡æ¡£ID
            filename: æ–‡ä»¶å
            file_type: æ–‡ä»¶ç±»å‹
            text_content: æ–‡æ¡£å†…å®¹
            analysis_prompt: åˆ†ææç¤ºè¯
            custom_analysis: æ˜¯å¦ä½¿ç”¨è‡ªå®šä¹‰åˆ†æ
            
        Returns:
            åˆ†æç»“æœä¿¡æ¯
        """
        try:
            # ä½¿ç”¨AIåˆ†ææ–‡æ¡£å†…å®¹å¹¶åŒ¹é…çŸ¥è¯†åº“
            analysis_result = await self._analyze_document_content(
                content=text_content,
                filename=filename,
                analysis_prompt=analysis_prompt,
                custom_analysis=custom_analysis
            )
            
            knowledge_base_name = analysis_result['knowledge_base_name']
            is_new_kb = analysis_result['is_new_knowledge_base']
            reason = analysis_result.get('reason', '')
            
            # å¦‚æœä¸æ˜¯æ–°å»ºçŸ¥è¯†åº“ï¼Œæ£€æŸ¥ç°æœ‰çŸ¥è¯†åº“æ˜¯å¦å­˜åœ¨
            kb_id = None
            if not is_new_kb:
                all_kbs = await self.get_all_knowledge_bases()
                for kb in all_kbs:
                    if kb['name'] == knowledge_base_name:
                        kb_id = kb['id']
                        break
                
                # å¦‚æœæ²¡æ‰¾åˆ°åŒ¹é…çš„çŸ¥è¯†åº“ï¼Œæ ‡è®°ä¸ºæ–°å»º
                if not kb_id:
                    is_new_kb = True
            
            result = {
                "filename": filename,
                "knowledgeBaseName": knowledge_base_name,
                "isNewKnowledgeBase": is_new_kb,
                "reason": reason,
                "knowledgeBaseId": kb_id,
                "documentType": analysis_result.get('document_type', 'æœªçŸ¥'),
                "textContent": text_content[:500] + "..." if len(text_content) > 500 else text_content,  # é¢„è§ˆå†…å®¹
                "docId": doc_id,  # æ–‡æ¡£IDï¼ˆå·²å­˜åœ¨ï¼‰
                "analysisTime": datetime.now().timestamp()
            }
            
            logger.info(f"å·²æœ‰æ–‡æ¡£åˆ†æå®Œæˆ: {filename} -> {knowledge_base_name} ({'æ–°å»º' if is_new_kb else 'ç°æœ‰'}), doc_id: {doc_id}")
            return result
            
        except Exception as e:
            logger.error(f"å·²æœ‰æ–‡æ¡£åˆ†æå¤±è´¥ {filename}: {e}")
            raise

    async def confirm_existing_document_archive(
        self,
        doc_id: str,
        analysis_result: Dict
    ) -> Dict:
        """
        ç¡®è®¤å·²æœ‰æ–‡æ¡£çš„å½’æ¡£æ“ä½œï¼Œæ‰§è¡Œæ–‡æ¡£ä¸çŸ¥è¯†åº“çš„å…³è”
        
        Args:
            doc_id: æ–‡æ¡£ID
            analysis_result: åˆ†æç»“æœ
            
        Returns:
            å½’æ¡£ç»“æœä¿¡æ¯
        """
        try:
            knowledge_base_name = analysis_result['knowledgeBaseName']
            is_new_kb = analysis_result['isNewKnowledgeBase']
            reason = analysis_result.get('reason', '')
            kb_id = analysis_result.get('knowledgeBaseId')
            filename = analysis_result.get('filename', 'æœªçŸ¥æ–‡æ¡£')
            
            # è·å–æˆ–åˆ›å»ºçŸ¥è¯†åº“
            if is_new_kb or not kb_id:
                kb = await self.create_knowledge_base(
                    name=knowledge_base_name,
                    description=f"ç”±AIæ™ºèƒ½åˆ†æåˆ›å»ºçš„çŸ¥è¯†åº“ï¼Œç”¨äºå­˜å‚¨{analysis_result.get('documentType', 'ç›¸å…³')}ç±»å‹çš„æ–‡æ¡£",
                    color=self._get_random_color()
                )
                kb_id = kb['id']
                logger.info(f"åˆ›å»ºæ–°çŸ¥è¯†åº“: {knowledge_base_name} (ID: {kb_id})")
            else:
                # éªŒè¯çŸ¥è¯†åº“æ˜¯å¦ä»ç„¶å­˜åœ¨
                try:
                    all_kbs = await self.get_all_knowledge_bases()
                    kb_exists = any(kb['id'] == kb_id for kb in all_kbs)
                    if not kb_exists:
                        # çŸ¥è¯†åº“ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°çš„
                        kb = await self.create_knowledge_base(
                            name=knowledge_base_name,
                            description=f"æ™ºèƒ½å½’æ¡£åˆ›å»ºçš„çŸ¥è¯†åº“",
                            color=self._get_random_color()
                        )
                        kb_id = kb['id']
                        is_new_kb = True
                        logger.info(f"åŸçŸ¥è¯†åº“ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°çŸ¥è¯†åº“: {knowledge_base_name}")
                except Exception as e:
                    logger.warning(f"éªŒè¯çŸ¥è¯†åº“æ—¶å‡ºé”™: {e}ï¼Œåˆ›å»ºæ–°çŸ¥è¯†åº“")
                    kb = await self.create_knowledge_base(
                        name=knowledge_base_name,
                        description=f"æ™ºèƒ½å½’æ¡£åˆ›å»ºçš„çŸ¥è¯†åº“",
                        color=self._get_random_color()
                    )
                    kb_id = kb['id']
                    is_new_kb = True
            
            # å°†æ–‡æ¡£å…³è”åˆ°çŸ¥è¯†åº“
            await self.add_documents_to_knowledge_base(kb_id, [doc_id])
            
            result = {
                "filename": filename,
                "knowledgeBaseName": knowledge_base_name,
                "isNewKnowledgeBase": is_new_kb,
                "reason": reason,
                "docId": doc_id,
                "knowledgeBaseId": kb_id
            }
            
            logger.info(f"å·²æœ‰æ–‡æ¡£å½’æ¡£å®Œæˆ: {filename} -> {knowledge_base_name} ({'æ–°å»º' if is_new_kb else 'ç°æœ‰'}), doc_id: {doc_id}")
            return result
            
        except Exception as e:
            logger.error(f"å·²æœ‰æ–‡æ¡£å½’æ¡£å¤±è´¥: {e}")
            raise

    def __del__(self):
        """æ¸…ç†èµ„æº"""
        if hasattr(self, 'executor'):
            self.executor.shutdown(wait=False)

# å…¨å±€RAGæœåŠ¡å®ä¾‹
rag_service = RAGService()
