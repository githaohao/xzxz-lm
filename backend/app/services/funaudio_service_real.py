"""
çœŸå®çš„ FunAudioLLM æœåŠ¡å®ç°
é›†æˆé˜¿é‡Œå·´å·´ SenseVoice æ¨¡å‹è¿›è¡Œé«˜æ€§èƒ½è¯­éŸ³è¯†åˆ«
"""

import logging
import torch
import numpy as np
import asyncio
import json
from typing import Optional, Dict, Any, List
from io import BytesIO
import soundfile as sf

# FunASR ç›¸å…³å¯¼å…¥
try:
    from funasr import AutoModel
    from funasr.utils.postprocess_utils import rich_transcription_postprocess
    FUNASR_AVAILABLE = True
except ImportError:
    FUNASR_AVAILABLE = False

# å¯¼å…¥å·¥å…·æ¨¡å—
from app.utils import (
    DeviceManager, AudioProcessor, EmotionAnalyzer, 
    MessageProcessor, get_timestamp
)

logger = logging.getLogger(__name__)

class FunAudioLLMService:
    """
    åŸºäºé˜¿é‡ŒFunAudioLLMçš„è¯­éŸ³æœåŠ¡
    é›†æˆSenseVoiceè¿›è¡Œé«˜æ€§èƒ½è¯­éŸ³è¯†åˆ«ã€æƒ…æ„Ÿåˆ†æå’Œå£°å­¦äº‹ä»¶æ£€æµ‹
    """
    
    def __init__(self):
        self.model = None
        self.vad_model = None
        
        # ä½¿ç”¨DeviceManagerè·å–æœ€ä¼˜è®¾å¤‡
        self.device = DeviceManager.get_optimal_device()
        
        self.conversation_history: Dict[str, List[Dict[str, Any]]] = {}
        self.max_history_length = 20
        self.model_name = "iic/SenseVoiceSmall"
        self.is_initialized = False
        
        logger.info(f"ğŸ¤ åˆå§‹åŒ–FunAudioLLMæœåŠ¡ï¼Œè®¾å¤‡: {self.device}")
        
        # è®¾å¤‡ä¼˜åŒ–é…ç½®
        optimization_result = DeviceManager.setup_device_optimization(self.device)
        if optimization_result["success"]:
            logger.info(f"âœ… è®¾å¤‡ä¼˜åŒ–å·²å¯ç”¨: {optimization_result['optimizations']}")
        else:
            logger.warning(f"âš ï¸ è®¾å¤‡ä¼˜åŒ–é…ç½®å¤±è´¥: {optimization_result.get('error', 'æœªçŸ¥é”™è¯¯')}")
        
        if not FUNASR_AVAILABLE:
            logger.warning("âš ï¸ FunASR æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install funasr")
        
    async def initialize(self):
        """åˆå§‹åŒ–SenseVoiceæ¨¡å‹"""
        if self.is_initialized:
            return True
            
        if not FUNASR_AVAILABLE:
            logger.error("âŒ FunASR æœªå®‰è£…ï¼Œæ— æ³•åˆå§‹åŒ–æ¨¡å‹")
            return False
            
        try:
            logger.info("ğŸ“¥ åŠ è½½SenseVoiceæ¨¡å‹...")
            
            # è®¾ç½®æ¨¡å‹ç¼“å­˜ç›®å½•
            cache_dir = DeviceManager.get_cache_dir("FUNAUDIO_CACHE_DIR", "./models/cache")
            
            # åŠ è½½SenseVoiceæ¨¡å‹ - Apple Silicon ä¼˜åŒ–
            model_kwargs = {
                "model": self.model_name,
                "trust_remote_code": True,
                "vad_model": "fsmn-vad",
                "vad_kwargs": {"max_single_segment_time": 30000},
                "cache_dir": cache_dir
            }
            
            # æ ¹æ®è®¾å¤‡ç±»å‹ä¼˜åŒ–é…ç½®
            device_config = DeviceManager.get_model_device_config(self.device, "funasr")
            model_kwargs["device"] = device_config["device"]
            
            if device_config.get("fallback_reason"):
                logger.info(f"ğŸ”„ è®¾å¤‡å›é€€: {device_config['fallback_reason']}")
            
            self.model = AutoModel(**model_kwargs)
            
            self.is_initialized = True
            logger.info("âœ… FunAudioLLM SenseVoiceæ¨¡å‹åŠ è½½æˆåŠŸ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ FunAudioLLMæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            self.is_initialized = False
            return False
    
    async def voice_recognition(self, audio_data: bytes, language: str = "auto") -> Dict[str, Any]:
        """
        é«˜æ€§èƒ½è¯­éŸ³è¯†åˆ«ï¼Œæ”¯æŒæƒ…æ„Ÿåˆ†æå’Œå£°å­¦äº‹ä»¶æ£€æµ‹
        """
        try:
            if not self.is_initialized:
                success = await self.initialize()
                if not success:
                    return {
                        "success": False,
                        "error": "FunAudioLLMæ¨¡å‹æœªåˆå§‹åŒ–",
                        "engine": "FunAudioLLM-SenseVoice",
                        "recognized_text": ""
                    }
            
            logger.info("ğŸ¯ å¼€å§‹FunAudioLLMè¯­éŸ³è¯†åˆ«...")
            
            # é¢„å¤„ç†éŸ³é¢‘æ•°æ®
            try:
                processed_audio_path = await AudioProcessor.preprocess_audio(audio_data)
            except ValueError as ve:
                # éŸ³é¢‘é¢„å¤„ç†å¤±è´¥ï¼Œè¿”å›ç‰¹å®šé”™è¯¯
                logger.error(f"âŒ éŸ³é¢‘é¢„å¤„ç†å¤±è´¥: {ve}")
                return {
                    "success": False,
                    "error": "æœªè¯†åˆ«åˆ°æœ‰æ•ˆè¯­éŸ³å†…å®¹",
                    "engine": "FunAudioLLM-SenseVoice",
                    "recognized_text": ""
                }
            
            try:
                # ä½¿ç”¨SenseVoiceè¿›è¡Œè¯†åˆ«
                result = self.model.generate(
                    input=processed_audio_path,
                    cache={},
                    language=language,  # "auto", "zh", "en", "yue", "ja", "ko"
                    use_itn=True,  # å¯ç”¨é€†æ–‡æœ¬æ ‡å‡†åŒ–
                    batch_size_s=60,
                    merge_vad=True,  # åˆå¹¶VADç»“æœ
                    merge_length_s=15,
                )
                
                if not result or len(result) == 0:
                    return {
                        "success": False,
                        "error": "è¯­éŸ³è¯†åˆ«è¿”å›ç©ºç»“æœ",
                        "engine": "FunAudioLLM-SenseVoice",
                        "recognized_text": ""
                    }
                
                # å¤„ç†è¯†åˆ«ç»“æœ
                raw_text = result[0]["text"]
                processed_text = rich_transcription_postprocess(raw_text)
                
                # è§£ææƒ…æ„Ÿå’Œäº‹ä»¶ä¿¡æ¯
                emotion_info = self._extract_emotion_info(processed_text)
                event_info = self._extract_event_info(processed_text)
                clean_text = self._clean_text(processed_text)
                
                # è·å–ç½®ä¿¡åº¦
                confidence = result[0].get("confidence", 1.0)
                
                logger.info(f"âœ… è¯­éŸ³è¯†åˆ«æˆåŠŸ: {clean_text[:50]}...")
                
                return {
                    "success": True,
                    "recognized_text": clean_text,
                    "raw_text": raw_text,
                    "processed_text": processed_text,
                    "emotion": emotion_info,
                    "events": event_info,
                    "language": language,
                    "engine": "FunAudioLLM-SenseVoice",
                    "confidence": confidence,
                    "model_name": self.model_name,
                    "device": self.device
                }
                
            finally:
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                AudioProcessor.cleanup_temp_file(processed_audio_path)
            
        except Exception as e:
            logger.error(f"âŒ FunAudioLLMè¯­éŸ³è¯†åˆ«å¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "engine": "FunAudioLLM-SenseVoice",
                "recognized_text": ""
            }
    

    
    def _extract_emotion_info(self, processed_text: str) -> Dict[str, Any]:
        """ä»å¤„ç†åçš„æ–‡æœ¬ä¸­æå–æƒ…æ„Ÿä¿¡æ¯"""
        return EmotionAnalyzer.extract_sensevoice_emotion_info(processed_text)
    
    def _extract_event_info(self, processed_text: str) -> List[str]:
        """ä»å¤„ç†åçš„æ–‡æœ¬ä¸­æå–å£°å­¦äº‹ä»¶ä¿¡æ¯"""
        return EmotionAnalyzer.extract_sensevoice_event_info(processed_text)
    
    def _clean_text(self, processed_text: str) -> str:
        """æ¸…ç†æ–‡æœ¬ï¼Œç§»é™¤ç‰¹æ®Šæ ‡è®°"""
        return EmotionAnalyzer.clean_sensevoice_text(processed_text)
    
    async def get_health_status(self) -> Dict[str, Any]:
        """è·å–æœåŠ¡å¥åº·çŠ¶æ€"""
        try:
            # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²åˆå§‹åŒ–
            if not self.is_initialized:
                await self.initialize()
            
            # è·å–è®¾å¤‡ä¿¡æ¯
            device_info = DeviceManager.get_device_info()
            
            # æ£€æŸ¥æ¨¡å‹çŠ¶æ€
            model_loaded = self.model is not None
            
            return {
                "available": model_loaded and FUNASR_AVAILABLE,
                "model_name": f"FunAudioLLM-{self.model_name}",
                "device": self.device,
                "audio_model": {
                    "name": "SenseVoice",
                    "available": model_loaded,
                    "device": self.device,
                    "model_path": self.model_name
                },
                "chat_model": {
                    "name": "LM Studio",
                    "available": True,  # å‡è®¾ LM Studio å¯ç”¨
                    "endpoint": "http://localhost:1234"
                },
                "features": [
                    "é«˜æ€§èƒ½è¯­éŸ³è¯†åˆ« (æ¯”Whisperå¿«15å€)",
                    "æƒ…æ„Ÿè¯†åˆ«",
                    "å£°å­¦äº‹ä»¶æ£€æµ‹", 
                    "50+è¯­è¨€æ”¯æŒ",
                    "å®æ—¶VADè¯­éŸ³æ´»åŠ¨æ£€æµ‹",
                    "é€†æ–‡æœ¬æ ‡å‡†åŒ–(ITN)"
                ],
                "system_info": {
                    "funasr_available": FUNASR_AVAILABLE,
                    **device_info,
                    "torch_version": torch.__version__
                },
                "message": "FunAudioLLM SenseVoice æœåŠ¡è¿è¡Œæ­£å¸¸" if model_loaded else "æ¨¡å‹æœªåŠ è½½",
                "lm_studio_available": True  # è¿™é‡Œå¯ä»¥æ·»åŠ å®é™…çš„ LM Studio æ£€æŸ¥
            }
            
        except Exception as e:
            logger.error(f"âŒ è·å–å¥åº·çŠ¶æ€å¤±è´¥: {e}")
            return {
                "available": False,
                "model_name": "FunAudioLLM-Error",
                "device": "unknown",
                "audio_model": {"name": "SenseVoice", "available": False},
                "chat_model": {"name": "LM Studio", "available": False},
                "features": [],
                "message": f"æœåŠ¡å¼‚å¸¸: {str(e)}",
                "lm_studio_available": False
            }
    
    async def voice_chat(self, audio_data: bytes, session_id: str = "default", language: str = "auto") -> Dict[str, Any]:
        """å®Œæ•´çš„è¯­éŸ³èŠå¤©æµç¨‹ï¼šè¯­éŸ³è¯†åˆ« + AIå¯¹è¯"""
        try:
            # 1. è¯­éŸ³è¯†åˆ«
            recognition_result = await self.voice_recognition(audio_data, language)
            
            if not recognition_result["success"]:
                return {
                    "success": False,
                    "error": f"è¯­éŸ³è¯†åˆ«å¤±è´¥: {recognition_result['error']}",
                    "recognized_text": "",
                    "response": ""
                }
            
            recognized_text = recognition_result["recognized_text"]
            
            if not recognized_text.strip():
                return {
                    "success": False,
                    "error": "æœªè¯†åˆ«åˆ°æœ‰æ•ˆè¯­éŸ³å†…å®¹",
                    "recognized_text": "",
                    "response": ""
                }
            
            # 2. è°ƒç”¨ LM Studio è¿›è¡Œå¯¹è¯
            try:
                from app.services.lm_studio_service import lm_studio_service
                from app.models.schemas import ChatRequest, ChatMessage, MessageType
                
                logger.info(f"ğŸ¤– å¼€å§‹AIå¯¹è¯å¤„ç†ï¼Œè¯†åˆ«æ–‡æœ¬: {recognized_text}")
                
                # è·å–å¯¹è¯å†å²
                history = self.conversation_history.get(session_id, [])
                logger.info(f"ğŸ“š å¯¹è¯å†å²é•¿åº¦: {len(history)}")
                
                # è½¬æ¢å†å²è®°å½•æ ¼å¼
                chat_history = []
                for msg in history[-self.max_history_length:]:
                    chat_history.append(ChatMessage(
                        content=msg["content"],
                        message_type=MessageType.TEXT,
                        is_user=(msg["role"] == "user")
                    ))
                
                # æ„å»ºèŠå¤©è¯·æ±‚
                chat_request = ChatRequest(
                    message=recognized_text,
                    history=chat_history,
                    temperature=0.7,
                    max_tokens=500
                )
                
                logger.info("ğŸ¤– è°ƒç”¨LM Studioè¿›è¡ŒAIå¯¹è¯...")
                logger.info(f"ğŸ“ è¯·æ±‚æ¶ˆæ¯: {recognized_text}")
                logger.info(f"ğŸ“Š å†å²æ¶ˆæ¯æ•°: {len(chat_history)}")
                
                # ç›´æ¥è°ƒç”¨LM StudioæœåŠ¡
                ai_response = await lm_studio_service.chat_completion(chat_request)
                
                logger.info(f"âœ… LM Studioå“åº”: {ai_response[:100]}...")
                
                # æ›´æ–°å¯¹è¯å†å²
                if session_id not in self.conversation_history:
                    self.conversation_history[session_id] = []
                
                self.conversation_history[session_id].extend([
                    {"role": "user", "content": recognized_text},
                    {"role": "assistant", "content": ai_response}
                ])
                
                # é™åˆ¶å†å²é•¿åº¦
                if len(self.conversation_history[session_id]) > self.max_history_length * 2:
                    self.conversation_history[session_id] = self.conversation_history[session_id][-self.max_history_length * 2:]
                
                logger.info(f"âœ… AIå¯¹è¯æˆåŠŸ: {ai_response[:50]}...")
                
                return {
                    "success": True,
                    "recognized_text": recognized_text,
                    "response": ai_response,
                    "emotion": recognition_result.get("emotion"),
                    "events": recognition_result.get("events"),
                    "confidence": recognition_result.get("confidence"),
                    "engine": "FunAudioLLM-SenseVoice",
                    "session_id": session_id
                }
                
            except Exception as e:
                logger.error(f"âŒ AIå¯¹è¯å¤±è´¥: {e}")
                logger.error(f"âŒ é”™è¯¯ç±»å‹: {type(e).__name__}")
                logger.error(f"âŒ é”™è¯¯è¯¦æƒ…: {str(e)}")
                import traceback
                logger.error(f"âŒ é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
                return {
                    "success": True,  # è¯­éŸ³è¯†åˆ«æˆåŠŸ
                    "recognized_text": recognized_text,
                    "response": f"è¯­éŸ³è¯†åˆ«æˆåŠŸï¼Œä½†AIå¯¹è¯æœåŠ¡å¼‚å¸¸: {str(e)}",
                    "emotion": recognition_result.get("emotion"),
                    "events": recognition_result.get("events"),
                    "confidence": recognition_result.get("confidence"),
                    "engine": "FunAudioLLM-SenseVoice"
                }
            
        except Exception as e:
            logger.error(f"âŒ è¯­éŸ³èŠå¤©å¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "recognized_text": "",
                "response": ""
            }
    
    async def health_check(self) -> Dict[str, Any]:
        """å¥åº·æ£€æŸ¥æ–¹æ³•"""
        try:
            await self.initialize()
            
            # æ£€æŸ¥LM StudioçŠ¶æ€
            lm_studio_available = False
            try:
                from app.services.lm_studio_service import lm_studio_service
                lm_studio_available = await lm_studio_service.health_check()
            except Exception as e:
                logger.warning(f"LM Studioå¥åº·æ£€æŸ¥å¤±è´¥: {e}")
            
            return {
                "available": True,
                "model_name": self.model_name,
                "device": self.device,
                **DeviceManager.get_device_info(),
                "lm_studio_available": lm_studio_available,
                "audio_model": {
                    "name": "SenseVoice",
                    "available": self.is_initialized,
                    "type": "speech_recognition"
                },
                "chat_model": {
                    "name": "LM Studio Chat Model", 
                    "available": lm_studio_available,
                    "type": "chat_completion"
                },
                "features": [
                    "é«˜æ€§èƒ½è¯­éŸ³è¯†åˆ«", 
                    "æƒ…æ„Ÿè¯†åˆ«", 
                    "å£°å­¦äº‹ä»¶æ£€æµ‹",
                    "å¤šè¯­è¨€æ”¯æŒ",
                    "è¿ç»­å¯¹è¯",
                    "å¤šä¼šè¯ç®¡ç†"
                ],
                "supported_languages": ["auto", "zh", "en", "yue", "ja", "ko", "fr", "es", "de"],
                "performance": {
                    "speed": "æ¯”Whisperå¿«15å€",
                    "accuracy": "é«˜ç²¾åº¦è¯†åˆ«"
                },
                "message": f"FunAudioLLMæœåŠ¡æ­£å¸¸ (è®¾å¤‡: {self.device}, LM Studio: {'å¯ç”¨' if lm_studio_available else 'ä¸å¯ç”¨'})"
            }
        except Exception as e:
            logger.error(f"âŒ å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
            return {
                "available": False,
                "error": str(e),
                "message": "FunAudioLLMæœåŠ¡å¼‚å¸¸"
            }
    
    async def get_conversation_summary(self, session_id: str) -> Dict[str, Any]:
        """è·å–ä¼šè¯æ‘˜è¦"""
        if session_id not in self.conversation_history:
            return {
                "session_id": session_id,
                "total_messages": 0,
                "messages": []
            }
        
        history = self.conversation_history[session_id]
        return {
            "session_id": session_id,
            "total_messages": len(history),
            "messages": history[-10:]  # è¿”å›æœ€è¿‘10æ¡æ¶ˆæ¯
        }
    
    async def clear_conversation_history(self, session_id: str) -> bool:
        """æ¸…é™¤æŒ‡å®šä¼šè¯çš„å¯¹è¯å†å²"""
        try:
            if session_id in self.conversation_history:
                del self.conversation_history[session_id]
                logger.info(f"âœ… å·²æ¸…é™¤ä¼šè¯ {session_id} çš„å¯¹è¯å†å²")
            else:
                logger.info(f"â„¹ï¸ ä¼šè¯ {session_id} ä¸å­˜åœ¨å¯¹è¯å†å²")
            return True
        except Exception as e:
            logger.error(f"âŒ æ¸…é™¤ä¼šè¯å†å²å¤±è´¥: {e}")
            return False
    
    async def wake_word_detection(self, audio_data: bytes, wake_words: List[str] = None) -> Dict[str, Any]:
        """
        å”¤é†’è¯æ£€æµ‹åŠŸèƒ½
        æ£€æµ‹éŸ³é¢‘ä¸­æ˜¯å¦åŒ…å«æŒ‡å®šçš„å”¤é†’è¯ï¼ˆå¦‚"å°æ™ºå°æ™º"ï¼‰
        """
        if wake_words is None:
            wake_words = ["å°æ™ºå°æ™º", "å°æ™º", "æ™ºèƒ½åŠ©æ‰‹", "hey xiaozhi"]
        
        try:
            if not self.is_initialized:
                success = await self.initialize()
                if not success:
                    return {
                        "success": False,
                        "wake_word_detected": False,
                        "error": "FunAudioLLMæ¨¡å‹æœªåˆå§‹åŒ–",
                        "confidence": 0.0
                    }
            
            logger.info("ğŸ¯ å¼€å§‹å”¤é†’è¯æ£€æµ‹...")
            
            # ä½¿ç”¨å¿«é€Ÿè¯­éŸ³è¯†åˆ«æ£€æµ‹å”¤é†’è¯
            recognition_result = await self.voice_recognition(audio_data, language="zh")
            
            if not recognition_result["success"]:
                return {
                    "success": False,
                    "wake_word_detected": False,
                    "error": recognition_result["error"],
                    "confidence": 0.0
                }
            
            recognized_text = recognition_result["recognized_text"].lower()
            confidence = recognition_result.get("confidence", 0.0)
            
            # æ£€æµ‹å”¤é†’è¯
            wake_word_detected = False
            detected_word = ""
            
            for wake_word in wake_words:
                if wake_word.lower() in recognized_text:
                    wake_word_detected = True
                    detected_word = wake_word
                    logger.info(f"âœ… æ£€æµ‹åˆ°å”¤é†’è¯: {wake_word}")
                    break
            
            # æé«˜å”¤é†’è¯æ£€æµ‹çš„å‡†ç¡®æ€§
            if not wake_word_detected:
                # æ¨¡ç³ŠåŒ¹é…ï¼Œå¤„ç†è¯­éŸ³è¯†åˆ«çš„ä¸å‡†ç¡®æ€§
                for wake_word in wake_words:
                    # æ£€æŸ¥ç›¸ä¼¼åº¦
                    if self._fuzzy_match(wake_word.lower(), recognized_text):
                        wake_word_detected = True
                        detected_word = wake_word
                        logger.info(f"âœ… æ¨¡ç³ŠåŒ¹é…æ£€æµ‹åˆ°å”¤é†’è¯: {wake_word}")
                        break
            
            return {
                "success": True,
                "wake_word_detected": wake_word_detected,
                "detected_word": detected_word,
                "recognized_text": recognition_result["recognized_text"],
                "confidence": confidence,
                "engine": "FunAudioLLM-SenseVoice",
                "emotion": recognition_result.get("emotion"),
                "events": recognition_result.get("events")
            }
            
        except Exception as e:
            logger.error(f"âŒ å”¤é†’è¯æ£€æµ‹å¤±è´¥: {e}")
            return {
                "success": False,
                "wake_word_detected": False,
                "error": str(e),
                "confidence": 0.0
            }
    
    def _fuzzy_match(self, wake_word: str, recognized_text: str) -> bool:
        """
        æ¨¡ç³ŠåŒ¹é…å”¤é†’è¯ï¼Œå¤„ç†è¯­éŸ³è¯†åˆ«çš„ä¸å‡†ç¡®æ€§
        """
        return EmotionAnalyzer.fuzzy_match_wake_word(wake_word, recognized_text)

# åˆ›å»ºå…¨å±€æœåŠ¡å®ä¾‹
funaudio_service = FunAudioLLMService() 