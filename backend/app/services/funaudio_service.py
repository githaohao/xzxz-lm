import logging
import torch
import numpy as np
from typing import Optional, Dict, Any, List
from io import BytesIO
import tempfile
import os
import asyncio
from funasr import AutoModel
from funasr.utils.postprocess_utils import rich_transcription_postprocess

logger = logging.getLogger(__name__)

class FunAudioLLMService:
    """
    åŸºäºé˜¿é‡ŒFunAudioLLMçš„è¯­éŸ³æœåŠ¡
    é›†æˆSenseVoiceè¿›è¡Œé«˜æ€§èƒ½è¯­éŸ³è¯†åˆ«ã€æƒ…æ„Ÿåˆ†æå’Œå£°å­¦äº‹ä»¶æ£€æµ‹
    å®˜æ–¹ä»“åº“ï¼šhttps://github.com/FunAudioLLM
    """
    
    def __init__(self):
        self.model = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        # å¯¹è¯å†å²ç®¡ç†
        self.conversation_history: Dict[str, List[Dict[str, Any]]] = {}
        self.max_history_length = 20
        
        logger.info(f"ğŸ¤ åˆå§‹åŒ–FunAudioLLMæœåŠ¡ï¼Œè®¾å¤‡: {self.device}")
        
    async def initialize(self):
        """åˆå§‹åŒ–SenseVoiceæ¨¡å‹"""
        try:
            logger.info("ğŸ“¥ åŠ è½½SenseVoiceæ¨¡å‹...")
            
            # åŠ è½½SenseVoiceæ¨¡å‹
            self.model = AutoModel(
                model="iic/SenseVoiceSmall",
                trust_remote_code=True,
                vad_model="fsmn-vad",
                vad_kwargs={"max_single_segment_time": 30000},
                device=self.device,
            )
            
            logger.info("âœ… FunAudioLLM SenseVoiceæ¨¡å‹åŠ è½½æˆåŠŸ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ FunAudioLLMæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return False
    
    def _save_audio_temp(self, audio_data: bytes) -> str:
        """ä¿å­˜éŸ³é¢‘æ•°æ®åˆ°ä¸´æ—¶æ–‡ä»¶"""
        try:
            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.wav')
            temp_file.write(audio_data)
            temp_file.close()
            return temp_file.name
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜ä¸´æ—¶éŸ³é¢‘æ–‡ä»¶å¤±è´¥: {e}")
            raise
    
    async def voice_recognition(self, audio_data: bytes, language: str = "auto") -> Dict[str, Any]:
        """
        é«˜æ€§èƒ½è¯­éŸ³è¯†åˆ«ï¼Œæ”¯æŒæƒ…æ„Ÿåˆ†æå’Œå£°å­¦äº‹ä»¶æ£€æµ‹
        """
        try:
            if not self.model:
                await self.initialize()
            
            logger.info("ğŸ¯ å¼€å§‹FunAudioLLMè¯­éŸ³è¯†åˆ«...")
            
            # ä¿å­˜éŸ³é¢‘åˆ°ä¸´æ—¶æ–‡ä»¶
            temp_audio_path = self._save_audio_temp(audio_data)
            
            try:
                # ä½¿ç”¨SenseVoiceè¿›è¡Œè¯†åˆ«
                result = self.model.generate(
                    input=temp_audio_path,
                    cache={},
                    language=language,  # "auto", "zh", "en", "yue", "ja", "ko", "nospeech"
                    use_itn=True,
                    batch_size_s=60,
                    merge_vad=True,
                    merge_length_s=15,
                )
                
                # å¤„ç†è¯†åˆ«ç»“æœï¼Œæå–æ–‡æœ¬å’Œæƒ…æ„Ÿä¿¡æ¯
                raw_text = result[0]["text"]
                processed_text = rich_transcription_postprocess(raw_text)
                
                # è§£ææƒ…æ„Ÿå’Œäº‹ä»¶ä¿¡æ¯
                emotion_info = self._extract_emotion_info(processed_text)
                event_info = self._extract_event_info(processed_text)
                clean_text = self._clean_text(processed_text)
                
                logger.info(f"âœ… è¯­éŸ³è¯†åˆ«æˆåŠŸ: {clean_text[:50]}...")
                
                return {
                    "success": True,
                    "recognized_text": clean_text,
                    "raw_text": raw_text,
                    "processed_text": processed_text,
                    "emotion": emotion_info,
                    "events": event_info,
                    "language": language,
                    "engine": "FunAudioLLM-SenseVoice",
                    "confidence": result[0].get("confidence", 1.0)
                }
                
            finally:
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                if os.path.exists(temp_audio_path):
                    os.unlink(temp_audio_path)
            
        except Exception as e:
            logger.error(f"âŒ FunAudioLLMè¯­éŸ³è¯†åˆ«å¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "engine": "FunAudioLLM-SenseVoice",
                "recognized_text": ""
            }
    
    async def voice_chat(self, audio_data: bytes, session_id: str = "default", language: str = "auto") -> Dict[str, Any]:
        """
        è¯­éŸ³èŠå¤©æ¨¡å¼ - ç»“åˆè¯­éŸ³è¯†åˆ«å’Œå¯¹è¯ç”Ÿæˆ
        """
        try:
            logger.info(f"ğŸ—£ï¸ å¼€å§‹FunAudioLLMè¯­éŸ³èŠå¤©ï¼Œä¼šè¯ID: {session_id}")
            
            # é¦–å…ˆè¿›è¡Œè¯­éŸ³è¯†åˆ«
            recognition_result = await self.voice_recognition(audio_data, language)
            
            if not recognition_result["success"]:
                return recognition_result
            
            recognized_text = recognition_result["recognized_text"]
            emotion_info = recognition_result["emotion"]
            
            # è·å–æˆ–åˆå§‹åŒ–å¯¹è¯å†å²
            if session_id not in self.conversation_history:
                self.conversation_history[session_id] = []
            
            # æ„å»ºåŒ…å«æƒ…æ„Ÿä¿¡æ¯çš„ç”¨æˆ·æ¶ˆæ¯
            user_message = {
                "role": "user",
                "content": recognized_text,
                "emotion": emotion_info,
                "timestamp": self._get_timestamp()
            }
            
            # æ·»åŠ åˆ°å†å²
            self.conversation_history[session_id].append(user_message)
            
            # è¿™é‡Œå¯ä»¥é›†æˆå…¶ä»–LLMè¿›è¡Œå¯¹è¯ç”Ÿæˆ
            # æš‚æ—¶è¿”å›è¯†åˆ«ç»“æœå’Œç®€å•å›å¤
            response_text = self._generate_response(recognized_text, emotion_info)
            
            # æ·»åŠ AIå›å¤åˆ°å†å²
            ai_message = {
                "role": "assistant", 
                "content": response_text,
                "timestamp": self._get_timestamp()
            }
            self.conversation_history[session_id].append(ai_message)
            
            # é™åˆ¶å†å²é•¿åº¦
            if len(self.conversation_history[session_id]) > self.max_history_length * 2:
                self.conversation_history[session_id] = self.conversation_history[session_id][-self.max_history_length * 2:]
            
            logger.info(f"âœ… FunAudioLLMè¯­éŸ³èŠå¤©æˆåŠŸ")
            
            return {
                "success": True,
                "recognized_text": recognized_text,
                "response": response_text,
                "emotion": emotion_info,
                "events": recognition_result["events"],
                "session_id": session_id,
                "history_length": len(self.conversation_history[session_id]) // 2,
                "engine": "FunAudioLLM-SenseVoice",
                "language": language
            }
            
        except Exception as e:
            logger.error(f"âŒ FunAudioLLMè¯­éŸ³èŠå¤©å¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "engine": "FunAudioLLM-SenseVoice",
                "response": "æŠ±æ­‰ï¼Œè¯­éŸ³å¤„ç†å‡ºç°é—®é¢˜ã€‚è¯·ç¨åé‡è¯•ã€‚"
            }
    
    def _extract_emotion_info(self, text: str) -> Dict[str, Any]:
        """æå–æƒ…æ„Ÿä¿¡æ¯"""
        emotions = {
            "ğŸ˜Š": "happy",
            "ğŸ˜¡": "angry", 
            "ğŸ˜”": "sad",
            "ğŸ˜€": "laugh",
            "ğŸ˜­": "cry"
        }
        
        detected_emotions = []
        for emoji, emotion in emotions.items():
            if emoji in text:
                detected_emotions.append(emotion)
        
        return {
            "detected": detected_emotions,
            "primary": detected_emotions[0] if detected_emotions else "neutral"
        }
    
    def _extract_event_info(self, text: str) -> List[str]:
        """æå–å£°å­¦äº‹ä»¶ä¿¡æ¯"""
        events = {
            "ğŸ¼": "music",
            "ğŸ‘": "applause", 
            "ğŸ¤§": "cough_sneeze",
            "ğŸ˜­": "crying",
            "ğŸ˜€": "laughter"
        }
        
        detected_events = []
        for emoji, event in events.items():
            if emoji in text:
                detected_events.append(event)
        
        return detected_events
    
    def _clean_text(self, text: str) -> str:
        """æ¸…ç†æ–‡æœ¬ï¼Œç§»é™¤æƒ…æ„Ÿå’Œäº‹ä»¶æ ‡è®°"""
        import re
        # ç§»é™¤emoji
        emoji_pattern = re.compile("["
                                 u"\U0001F600-\U0001F64F"  # emoticons
                                 u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                                 u"\U0001F680-\U0001F6FF"  # transport & map symbols
                                 u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                                 "]+", flags=re.UNICODE)
        return emoji_pattern.sub('', text).strip()
    
    def _generate_response(self, user_text: str, emotion_info: Dict[str, Any]) -> str:
        """ç”Ÿæˆç®€å•çš„å›å¤ï¼ˆå¯æ‰©å±•æ¥å…¥å…¶ä»–LLMï¼‰"""
        primary_emotion = emotion_info.get("primary", "neutral")
        
        if primary_emotion == "happy":
            return f"å¾ˆé«˜å…´å¬åˆ°ä½ å¼€å¿ƒçš„è¯è¯­ï¼ä½ è¯´ï¼šã€Œ{user_text}ã€"
        elif primary_emotion == "sad":
            return f"æˆ‘èƒ½æ„Ÿå—åˆ°ä½ çš„æƒ…ç»ªï¼Œè®©æˆ‘æ¥å¸®åŠ©ä½ ã€‚ä½ è¯´ï¼šã€Œ{user_text}ã€"
        elif primary_emotion == "angry":
            return f"æˆ‘ç†è§£ä½ çš„æ„Ÿå—ï¼Œè®©æˆ‘ä»¬å†·é™åœ°è®¨è®ºä¸€ä¸‹ã€‚ä½ è¯´ï¼šã€Œ{user_text}ã€" 
        else:
            return f"æˆ‘å¬åˆ°ä½ è¯´ï¼šã€Œ{user_text}ã€ã€‚æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ"
    
    def _get_timestamp(self) -> float:
        """è·å–å½“å‰æ—¶é—´æˆ³"""
        import time
        return time.time()
    
    async def clear_conversation_history(self, session_id: str = "default") -> bool:
        """æ¸…é™¤æŒ‡å®šä¼šè¯çš„å¯¹è¯å†å²"""
        try:
            if session_id in self.conversation_history:
                del self.conversation_history[session_id]
                logger.info(f"âœ… å·²æ¸…é™¤FunAudioLLMä¼šè¯ {session_id} çš„å¯¹è¯å†å²")
            return True
        except Exception as e:
            logger.error(f"âŒ æ¸…é™¤å¯¹è¯å†å²å¤±è´¥: {e}")
            return False
    
    async def get_conversation_summary(self, session_id: str = "default") -> Dict[str, Any]:
        """è·å–å¯¹è¯æ‘˜è¦ä¿¡æ¯"""
        try:
            history = self.conversation_history.get(session_id, [])
            user_messages = [msg for msg in history if msg["role"] == "user"]
            assistant_messages = [msg for msg in history if msg["role"] == "assistant"]
            
            # ç»Ÿè®¡æƒ…æ„Ÿåˆ†å¸ƒ
            emotions = [msg.get("emotion", {}).get("primary", "neutral") for msg in user_messages if "emotion" in msg]
            emotion_stats = {emotion: emotions.count(emotion) for emotion in set(emotions)} if emotions else {}
            
            return {
                "session_id": session_id,
                "total_messages": len(history),
                "user_messages": len(user_messages),
                "assistant_messages": len(assistant_messages),
                "conversation_rounds": len(user_messages),
                "emotion_stats": emotion_stats,
                "has_history": len(history) > 0,
                "last_user_message": user_messages[-1]["content"] if user_messages else None,
                "last_assistant_message": assistant_messages[-1]["content"] if assistant_messages else None,
                "engine": "FunAudioLLM-SenseVoice"
            }
        except Exception as e:
            logger.error(f"âŒ è·å–å¯¹è¯æ‘˜è¦å¤±è´¥: {e}")
            return {"error": str(e)}
    
    async def health_check(self) -> Dict[str, Any]:
        """æ£€æŸ¥æœåŠ¡å¥åº·çŠ¶æ€"""
        try:
            if not self.model:
                await self.initialize()
            
            return {
                "available": self.model is not None,
                "model_name": "FunAudioLLM-SenseVoice",
                "device": self.device,
                "cuda_available": torch.cuda.is_available(),
                "features": [
                    "é«˜æ€§èƒ½è¯­éŸ³è¯†åˆ« (æ¯”Whisperå¿«15å€)",
                    "æƒ…æ„Ÿè¯†åˆ«",
                    "å£°å­¦äº‹ä»¶æ£€æµ‹", 
                    "50+è¯­è¨€æ”¯æŒ",
                    "è¿ç»­å¯¹è¯",
                    "å¤šä¼šè¯ç®¡ç†"
                ],
                "message": "FunAudioLLMæœåŠ¡æ­£å¸¸" if self.model else "æ¨¡å‹æœªåŠ è½½",
                "supported_languages": ["auto", "zh", "en", "yue", "ja", "ko", "fr", "es", "de"],
                "performance": {
                    "speed": "15x faster than Whisper-Large",
                    "accuracy": "SOTA performance on multiple benchmarks"
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ FunAudioLLMå¥åº·æ£€æŸ¥å¤±è´¥: {e}")
            return {
                "available": False,
                "error": str(e),
                "message": "FunAudioLLMæœåŠ¡å¼‚å¸¸"
            }

# åˆ›å»ºå…¨å±€æœåŠ¡å®ä¾‹
funaudio_service = FunAudioLLMService() 