import httpx
import asyncio
import json
from typing import List, Dict, Any, AsyncGenerator
from app.config import settings
from app.models.schemas import ChatMessage, ChatRequest
import logging

# å¯¼å…¥å·¥å…·æ¨¡å—
from app.utils import MessageProcessor, get_timestamp, safe_str_convert

logger = logging.getLogger(__name__)

class LMStudioService:
    def __init__(self):
        self.base_url = settings.lm_studio_base_url
        self.model = settings.lm_studio_model
        self.api_key = settings.lm_studio_api_key
        
    async def health_check(self) -> bool:
        """æ£€æŸ¥LM StudioæœåŠ¡çŠ¶æ€"""
        try:
            async with httpx.AsyncClient() as client:
                response = await client.get(
                    f"{self.base_url}/models",
                    timeout=10.0
                )
                return response.status_code == 200
        except Exception as e:
            logger.error(f"LM Studioå¥åº·æ£€æŸ¥å¤±è´¥: {e}")
            return False
    
    def _prepare_messages(self, request: ChatRequest) -> List[Dict[str, str]]:
        """å‡†å¤‡æ¶ˆæ¯æ ¼å¼ä¾›LM Studioä½¿ç”¨"""
        return MessageProcessor.prepare_lm_studio_messages(request)
    
    async def chat_completion(self, request: ChatRequest) -> str:
        """å‘é€èŠå¤©è¯·æ±‚åˆ°LM Studio"""
        try:
            logger.info(f"ğŸš€ å¼€å§‹å¤„ç†èŠå¤©è¯·æ±‚: {request.message[:50]}...")
            logger.info(f"ğŸ“Š å†å²æ¶ˆæ¯æ•°é‡: {len(request.history)}")
            
            messages = self._prepare_messages(request)
            logger.info(f"ğŸ“ å‡†å¤‡çš„æ¶ˆæ¯æ•°é‡: {len(messages)}")
            
            payload = {
                "model": self.model,
                "messages": messages,
                "temperature": request.temperature,
                "max_tokens": request.max_tokens,
                "stream": False
            }
            
            logger.info(f"ğŸ“¤ å‘é€åˆ°LM Studioçš„payload: {json.dumps(payload, ensure_ascii=False, indent=2)}")
            
            async with httpx.AsyncClient() as client:
                logger.info(f"ğŸ”— è¿æ¥åˆ°LM Studio: {self.base_url}/chat/completions")
                response = await client.post(
                    f"{self.base_url}/chat/completions",
                    json=payload,
                    headers={
                        "Content-Type": "application/json",
                        "Authorization": f"Bearer {self.api_key}"
                    },
                    timeout=120.0
                )
                
                logger.info(f"ğŸ“¡ LM Studioå“åº”çŠ¶æ€: {response.status_code}")
                
                if response.status_code == 200:
                    result = response.json()
                    ai_response = result["choices"][0]["message"]["content"]
                    logger.info(f"âœ… LM Studioå“åº”æˆåŠŸ: {ai_response[:100]}...")
                    return ai_response
                else:
                    error_text = response.text
                    logger.error(f"âŒ LM Studio APIé”™è¯¯: {response.status_code} - {error_text}")
                    return "æŠ±æ­‰ï¼ŒæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åå†è¯•ã€‚"
                    
        except Exception as e:
            logger.error(f"âŒ èŠå¤©å®Œæˆè¯·æ±‚å¤±è´¥: {e}")
            logger.error(f"âŒ é”™è¯¯ç±»å‹: {type(e).__name__}")
            import traceback
            logger.error(f"âŒ é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
            return "æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‡ºç°é”™è¯¯ï¼Œè¯·ç¨åå†è¯•ã€‚"
    
    async def chat_completion_stream(self, request: ChatRequest) -> AsyncGenerator[str, None]:
        """æµå¼èŠå¤©å®Œæˆ"""
        try:
            messages = self._prepare_messages(request)
            
            payload = {
                "model": self.model,
                "messages": messages,
                "temperature": request.temperature,
                "max_tokens": request.max_tokens,
                "stream": True
            }
            
            async with httpx.AsyncClient() as client:
                async with client.stream(
                    "POST",
                    f"{self.base_url}/chat/completions",
                    json=payload,
                    headers={
                        "Content-Type": "application/json",
                        "Authorization": f"Bearer {self.api_key}"
                    },
                    timeout=120.0
                ) as response:
                    if response.status_code == 200:
                        async for line in response.aiter_lines():
                            if line.startswith("data: "):
                                data = line[6:]  # ç§»é™¤ "data: " å‰ç¼€
                                if data == "[DONE]":
                                    break
                                try:
                                    json_data = json.loads(data)
                                    delta = json_data["choices"][0]["delta"]
                                    if "content" in delta:
                                        yield delta["content"]
                                except json.JSONDecodeError:
                                    continue
                    else:
                        yield "æŠ±æ­‰ï¼ŒæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åå†è¯•ã€‚"
                        
        except Exception as e:
            logger.error(f"æµå¼èŠå¤©å®Œæˆè¯·æ±‚å¤±è´¥: {e}")
            yield "æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‡ºç°é”™è¯¯ï¼Œè¯·ç¨åå†è¯•ã€‚"

# åˆ›å»ºå…¨å±€æœåŠ¡å®ä¾‹
lm_studio_service = LMStudioService() 